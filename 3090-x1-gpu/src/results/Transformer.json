export const model =
[{"name": "Transformer Training Iteration", "type": "root", "ops": [{"name": "Load Data", "type": "training loop", "instances": 27, "resources": {"cpu2": {"time": {"ts": 1678058059199273000, "dur": 2028000, "relative_dur": 0.0027701928343798962, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2.03 ms"}}, "gpu3": {"time": {"ts": 1678058059199381000, "dur": 525000, "relative_dur": 0.0007171357189592926, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "525 us"}}}, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 345, in train_once\n    with hotline.annotate('Load Data'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/submission_runner.py", "source_file_num": 345, "ops": [{"name": "aten::lift_fresh", "type": "generated", "generated_depth": 1, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1678058059199273000, "dur": 2000, "relative_dur": 0.0009861932938856016, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 us"}, "res_name": "aten::lift_fresh"}}, "id": "o1GuTmoDLVKyngzb", "pretty_name": "aten::lift_fresh", "trace_file": "/results/Transformer/Transformer.3.pt.trace.json", "trace_disk_size": "99 Bytes", "runtime": 2000, "runtime_str": "2 us", "start_timestamp": "23:14:19.199.199273", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 1}, {"name": "aten::to", "type": "generated", "generated_depth": 1, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1678058059199286000, "dur": 77000, "relative_dur": 0.03796844181459566, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "77 us"}}, "gpu3": {"time": {"ts": 1678058059199381000, "dur": 5000, "relative_dur": 0.002465483234714004, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "5 us"}}}, "ops": [{"name": "aten::empty_strided", "type": "generated", "generated_depth": 2, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1678058059199291000, "dur": 15000, "relative_dur": 0.19480519480519481, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "15 us"}, "res_name": "aten::empty_strided"}}, "id": "1uVtrIsg6eEQKLcg", "pretty_name": "aten::empty_strided", "trace_file": "/results/Transformer/Transformer.5.pt.trace.json", "trace_disk_size": "103 Bytes", "runtime": 15000, "runtime_str": "15 us", "start_timestamp": "23:14:19.199.199291", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 1}, {"name": "aten::copy_", "type": "generated", "generated_depth": 2, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1678058059199308000, "dur": 54000, "relative_dur": 0.7012987012987013, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "54 us"}}, "gpu3": {"time": {"ts": 1678058059199381000, "dur": 5000, "relative_dur": 0.06493506493506493, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "5 us"}}}, "ops": [{"name": "aten::to", "type": "generated", "generated_depth": 3, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1678058059199314000, "dur": 17000, "relative_dur": 0.3148148148148148, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "17 us"}, "res_name": "aten::to"}}, "id": "udeqRvl9kbzacYek", "pretty_name": "aten::to", "trace_file": "/results/Transformer/Transformer.7.pt.trace.json", "trace_disk_size": "384 Bytes", "runtime": 17000, "runtime_str": "17 us", "start_timestamp": "23:14:19.199.199314", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 4}, {"name": "aten::expand_as", "type": "generated", "generated_depth": 3, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1678058059199334000, "dur": 14000, "relative_dur": 0.25925925925925924, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "14 us"}, "res_name": "aten::expand_as"}}, "id": "fk7OppSvFnqE4n0A", "pretty_name": "aten::expand_as", "trace_file": "/results/Transformer/Transformer.8.pt.trace.json", "trace_disk_size": "291 Bytes", "runtime": 14000, "runtime_str": "14 us", "start_timestamp": "23:14:19.199.199334", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 3}], "id": "8AteBU62ExsqekcR", "pretty_name": "aten::copy_", "trace_file": "/results/Transformer/Transformer.6.pt.trace.json", "trace_disk_size": "1.2 kB", "runtime": 54000, "runtime_str": "54 us", "start_timestamp": "23:14:19.199.199308", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 12}], "id": "DPZoFvFdsWr7zcfN", "pretty_name": "aten::to", "trace_file": "/results/Transformer/Transformer.4.pt.trace.json", "trace_disk_size": "1.5 kB", "runtime": 77000, "runtime_str": "77 us", "start_timestamp": "23:14:19.199.199286", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 15}, {"name": "aten::to(92%) and 4 others\u2026", "type": "generated", "generated_depth": 1, "instances": 19, "resources": {"cpu2": {"time": {"ts": 1678058059199409000, "dur": 1925000, "relative_dur": 0.9492110453648915, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.93 ms"}, "res_name": "aten::to(92%) and 4 others\u2026"}, "gpu3": {"time": {"ts": 1678058059199475000, "dur": 431000, "relative_dur": 0.21252465483234714, "relative_gap_to_previous": 0.04388560157790927, "parent_is_longest": true, "runtime_str": "431 us"}, "res_name": "Memcpy HtoD Pageable -> Device"}}, "id": "gzwGclmfjslT8r0h", "pretty_name": "aten::to(92%) and 4 others\u2026", "trace_file": "/results/Transformer/Transformer.9.pt.trace.json", "trace_disk_size": "9.1 kB", "runtime": 1925000, "runtime_str": "1.93 ms", "start_timestamp": "23:14:19.199.199409", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 93}], "idx": 1, "id": "c1tgzNRMJbnpLTGW", "pretty_name": "Load Data", "trace_file": "/results/Transformer/Transformer.2.pt.trace.json", "trace_disk_size": "10.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/submission_runner.py", "runtime": 2028000, "runtime_str": "2.03 ms", "start_timestamp": "23:14:19.199.199273", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 109}, {"name": "Forward", "type": "training loop", "instances": 7, "resources": {"cpu2": {"time": {"ts": 1678058059201437000, "dur": 74025000, "relative_dur": 0.10111613637326027, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "74 ms"}}, "gpu3": {"time": {"ts": 1678058059202729000, "dur": 238148000, "relative_dur": 0.3253036899023193, "relative_gap_to_previous": 0.003856141208803968, "parent_is_longest": true, "runtime_str": "238 ms"}}}, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 137, in update_params\n    with hotline.annotate('Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "source_file_num": 137, "ops": [{"idx": 4, "name": "Encoder", "type": "Encoder", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 205, in forward\n    with hotline.annotate('Encoder'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 205, "ops": [{"idx": 5, "name": "Embedding", "type": "Embedding", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 254, in forward\n    with hotline.annotate('Embedding'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 254, "resources": {"cpu2": {"time": {"ts": 1678058059201438000, "dur": 2199000, "relative_dur": 0.025861764809653177, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2.20 ms"}}, "gpu3": {"time": {"ts": 1678058059202729000, "dur": 1730000, "relative_dur": 0.020345999600136423, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.73 ms"}}}, "id": "ggB9dDv11vfq0Byo", "pretty_name": "Embedding", "trace_file": "/results/Transformer/Transformer.12.pt.trace.json", "trace_disk_size": "26.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 2199000, "runtime_str": "2.20 ms", "start_timestamp": "23:14:19.201.201438", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 177}, {"idx": 6, "name": "PositionalEncoding", "type": "PositionalEncoding", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 258, in forward\n    with hotline.annotate('PositionalEncoding'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 258, "ops": [{"idx": 7, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 259, in forward\n    src = self.pos_encoder(src, inputs_positions)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 372, in forward\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 372, "resources": {"cpu2": {"time": {"ts": 1678058059203823000, "dur": 329000, "relative_dur": 1.0, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "329 us"}}, "gpu3": {"time": {"ts": 1678058059204460000, "dur": 277000, "relative_dur": 0.8419452887537994, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "277 us"}}}, "id": "prnfgwHR3oTUOO2G", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.14.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 329000, "runtime_str": "329 us", "start_timestamp": "23:14:19.203.203823", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059203823000, "dur": 329000, "relative_dur": 0.0038692681320490656, "relative_gap_to_previous": 0.0006821202178080419, "parent_is_longest": true, "runtime_str": "329 us"}}, "gpu3": {"time": {"ts": 1678058059204460000, "dur": 277000, "relative_dur": 0.0032577120747039248, "relative_gap_to_previous": 1.1760693410483483e-05, "parent_is_longest": true, "runtime_str": "277 us"}}}, "id": "dmY0XVNi23LSa9i0", "pretty_name": "PositionalEncoding", "trace_file": "/results/Transformer/Transformer.13.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 329000, "runtime_str": "329 us", "start_timestamp": "23:14:19.203.203823", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 30}, {"idx": 8, "name": "TransformerEncoder", "type": "TransformerEncoder", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 260, in forward\n    with hotline.annotate('TransformerEncoder'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 260, "ops": [{"idx": 9, "name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"idx": 10, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu2": {"time": {"ts": 1678058059204313000, "dur": 380000, "relative_dur": 0.028646814926498305, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "380 us"}}, "gpu3": {"time": {"ts": 1678058059204738000, "dur": 109000, "relative_dur": 0.008217112702600829, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "109 us"}}}, "id": "tQofBv3fDZfGHGNz", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.17.pt.trace.json", "trace_disk_size": "2.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 380000, "runtime_str": "380 us", "start_timestamp": "23:14:19.204.204313", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 21}, {"idx": 11, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"idx": 12, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059204824000, "dur": 379000, "relative_dur": 0.06843625857710364, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "379 us"}}, "gpu3": {"time": {"ts": 1678058059205086000, "dur": 903000, "relative_dur": 0.1630552546045504, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "903 us"}}}, "id": "6NbUyNanhKBf8XPV", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.19.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 903000, "runtime_str": "903 us", "start_timestamp": "23:14:19.204.204824", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 35}, {"idx": 13, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059205356000, "dur": 152000, "relative_dur": 0.027446731672083786, "relative_gap_to_previous": 0.02473817262549657, "parent_is_longest": true, "runtime_str": "152 us"}}, "gpu3": {"time": {"ts": 1678058059205990000, "dur": 884000, "relative_dur": 0.1596244131455399, "relative_gap_to_previous": 0.00018057060310581438, "parent_is_longest": true, "runtime_str": "884 us"}}}, "id": "LK5AwdhjI8B09Z0g", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.20.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 884000, "runtime_str": "884 us", "start_timestamp": "23:14:19.205.205356", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 14, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059205661000, "dur": 166000, "relative_dur": 0.029974720115565186, "relative_gap_to_previous": 0.02473817262549657, "parent_is_longest": true, "runtime_str": "166 us"}}, "gpu3": {"time": {"ts": 1678058059206876000, "dur": 885000, "relative_dur": 0.1598049837486457, "relative_gap_to_previous": 0.00036114120621162876, "parent_is_longest": true, "runtime_str": "885 us"}}}, "id": "ZR4lOlfci7hidJzi", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.21.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 885000, "runtime_str": "885 us", "start_timestamp": "23:14:19.205.205661", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 15, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059205849000, "dur": 303000, "relative_dur": 0.05471289274106175, "relative_gap_to_previous": 0.0010834236186348862, "parent_is_longest": true, "runtime_str": "303 us"}}, "gpu3": {"time": {"ts": 1678058059207763000, "dur": 82000, "relative_dur": 0.014806789454676778, "relative_gap_to_previous": 0.00036114120621162876, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "meEw7kAcf0YDrOB4", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.22.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 303000, "runtime_str": "303 us", "start_timestamp": "23:14:19.205.205849", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 16, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059206297000, "dur": 89000, "relative_dur": 0.01607078367641748, "relative_gap_to_previous": 0.023293607800650054, "parent_is_longest": true, "runtime_str": "89 us"}}, "gpu3": {"time": {"ts": 1678058059207846000, "dur": 776000, "relative_dur": 0.14012278801011196, "relative_gap_to_previous": 0.00018057060310581438, "parent_is_longest": true, "runtime_str": "776 us"}}}, "id": "OUMLXD0sdd74TIra", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.23.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 776000, "runtime_str": "776 us", "start_timestamp": "23:14:19.206.206297", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 17, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059206570000, "dur": 69000, "relative_dur": 0.012459371614301192, "relative_gap_to_previous": 0.030335861321776816, "parent_is_longest": true, "runtime_str": "69 us"}}, "gpu3": {"time": {"ts": 1678058059208624000, "dur": 322000, "relative_dur": 0.058143734200072225, "relative_gap_to_previous": 0.00036114120621162876, "parent_is_longest": true, "runtime_str": "322 us"}}}, "id": "Qk9BW5x80w1avrXW", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.24.pt.trace.json", "trace_disk_size": "902 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 322000, "runtime_str": "322 us", "start_timestamp": "23:14:19.206.206570", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 18, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059206794000, "dur": 90000, "relative_dur": 0.016251354279523293, "relative_gap_to_previous": 0.025099313831708197, "parent_is_longest": true, "runtime_str": "90 us"}}, "gpu3": {"time": {"ts": 1678058059208947000, "dur": 381000, "relative_dur": 0.06879739978331528, "relative_gap_to_previous": 0.00018057060310581438, "parent_is_longest": true, "runtime_str": "381 us"}}}, "id": "vRoCJ874W1fn63Wm", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.25.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 381000, "runtime_str": "381 us", "start_timestamp": "23:14:19.206.206794", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 19, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059207063000, "dur": 88000, "relative_dur": 0.015890213073311666, "relative_gap_to_previous": 0.029433008306247743, "parent_is_longest": true, "runtime_str": "88 us"}}, "gpu3": {"time": {"ts": 1678058059209329000, "dur": 417000, "relative_dur": 0.07529794149512459, "relative_gap_to_previous": 0.00018057060310581438, "parent_is_longest": true, "runtime_str": "417 us"}}}, "id": "C1i0qlFKE3Y3zQAh", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.26.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 417000, "runtime_str": "417 us", "start_timestamp": "23:14:19.207.207063", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 20, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059207336000, "dur": 164000, "relative_dur": 0.029613578909353556, "relative_gap_to_previous": 0.03051643192488263, "parent_is_longest": true, "runtime_str": "164 us"}}, "gpu3": {"time": {"ts": 1678058059209748000, "dur": 876000, "relative_dur": 0.1581798483206934, "relative_gap_to_previous": 0.00036114120621162876, "parent_is_longest": true, "runtime_str": "876 us"}}}, "id": "faQ8S20pV5PHpoF2", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.27.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 876000, "runtime_str": "876 us", "start_timestamp": "23:14:19.207.207336", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059204824000, "dur": 2660000, "relative_dur": 0.20052770448548812, "relative_gap_to_previous": 0.008669430833019224, "parent_is_longest": true, "runtime_str": "2.66 ms"}}, "gpu3": {"time": {"ts": 1678058059205086000, "dur": 5538000, "relative_dur": 0.4174896343761779, "relative_gap_to_previous": 0.018017338861666038, "parent_is_longest": true, "runtime_str": "5.54 ms"}}}, "id": "Y4WZBF3pcj49jgiU", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.18.pt.trace.json", "trace_disk_size": "23.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5538000, "runtime_str": "5.54 ms", "start_timestamp": "23:14:19.204.204824", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 191}, {"idx": 21, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu2": {"time": {"ts": 1678058059207507000, "dur": 273000, "relative_dur": 0.02058047493403694, "relative_gap_to_previous": 0.0005277044854881266, "parent_is_longest": true, "runtime_str": "273 us"}}, "gpu3": {"time": {"ts": 1678058059210625000, "dur": 97000, "relative_dur": 0.00731247644176404, "relative_gap_to_previous": 7.538635506973238e-05, "parent_is_longest": true, "runtime_str": "97 us"}}}, "id": "wliJZngWKqXhx5a5", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.28.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 273000, "runtime_str": "273 us", "start_timestamp": "23:14:19.207.207507", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 22, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu2": {"time": {"ts": 1678058059207796000, "dur": 276000, "relative_dur": 0.020806633999246135, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "276 us"}}, "gpu3": {"time": {"ts": 1678058059210723000, "dur": 232000, "relative_dur": 0.017489634376177912, "relative_gap_to_previous": 7.538635506973238e-05, "parent_is_longest": true, "runtime_str": "232 us"}}}, "id": "oap8hNAm8Ykxh3gU", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.29.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 276000, "runtime_str": "276 us", "start_timestamp": "23:14:19.207.207796", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 23, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"idx": 24, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu2": {"time": {"ts": 1678058059208236000, "dur": 291000, "relative_dur": 0.04129416773094934, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "291 us"}}, "gpu3": {"time": {"ts": 1678058059210956000, "dur": 3170000, "relative_dur": 0.4498368099900667, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3.17 ms"}}}, "id": "yzfugXonduESTVKZ", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.31.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3170000, "runtime_str": "3.17 ms", "start_timestamp": "23:14:19.208.208236", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 25, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu2": {"time": {"ts": 1678058059208685000, "dur": 81000, "relative_dur": 0.011494252873563218, "relative_gap_to_previous": 0.022420888321271464, "parent_is_longest": true, "runtime_str": "81 us"}}, "gpu3": {"time": {"ts": 1678058059214127000, "dur": 319000, "relative_dur": 0.04526748971193416, "relative_gap_to_previous": 0.00014190435646374344, "parent_is_longest": true, "runtime_str": "319 us"}}}, "id": "o736MiSWHWeCGkGB", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.32.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 319000, "runtime_str": "319 us", "start_timestamp": "23:14:19.208.208685", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 26, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu2": {"time": {"ts": 1678058059208941000, "dur": 93000, "relative_dur": 0.01319710515112814, "relative_gap_to_previous": 0.024833262381155102, "parent_is_longest": true, "runtime_str": "93 us"}}, "gpu3": {"time": {"ts": 1678058059214448000, "dur": 377000, "relative_dur": 0.053497942386831275, "relative_gap_to_previous": 0.0002838087129274869, "parent_is_longest": true, "runtime_str": "377 us"}}}, "id": "JFNEIWsPztiSAfKq", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.33.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 377000, "runtime_str": "377 us", "start_timestamp": "23:14:19.208.208941", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 27, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu2": {"time": {"ts": 1678058059209215000, "dur": 128000, "relative_dur": 0.01816375762735916, "relative_gap_to_previous": 0.025684688519937562, "parent_is_longest": true, "runtime_str": "128 us"}}, "gpu3": {"time": {"ts": 1678058059214827000, "dur": 3078000, "relative_dur": 0.4367816091954023, "relative_gap_to_previous": 0.0002838087129274869, "parent_is_longest": true, "runtime_str": "3.08 ms"}}}, "id": "C9ADeR66pU8hWMiY", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.34.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3078000, "runtime_str": "3.08 ms", "start_timestamp": "23:14:19.209.209215", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 28, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu2": {"time": {"ts": 1678058059209516000, "dur": 87000, "relative_dur": 0.012345679012345678, "relative_gap_to_previous": 0.024549453668227614, "parent_is_longest": true, "runtime_str": "87 us"}}, "gpu3": {"time": {"ts": 1678058059217906000, "dur": 97000, "relative_dur": 0.013764722576983114, "relative_gap_to_previous": 0.00014190435646374344, "parent_is_longest": true, "runtime_str": "97 us"}}}, "id": "uDEzkuw1x85Amasc", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.35.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 97000, "runtime_str": "97 us", "start_timestamp": "23:14:19.209.209516", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1678058059208236000, "dur": 1367000, "relative_dur": 0.10305314738032416, "relative_gap_to_previous": 0.011157180550320393, "parent_is_longest": true, "runtime_str": "1.37 ms"}}, "gpu3": {"time": {"ts": 1678058059210956000, "dur": 7047000, "relative_dur": 0.531247644176404, "relative_gap_to_previous": 7.538635506973238e-05, "parent_is_longest": true, "runtime_str": "7.05 ms"}}}, "id": "mXiq41ZOoMSPtfCT", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.30.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7047000, "runtime_str": "7.05 ms", "start_timestamp": "23:14:19.208.208236", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1678058059204313000, "dur": 5274000, "relative_dur": 0.06352686099735004, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "5.27 ms"}}, "gpu3": {"time": {"ts": 1678058059204738000, "dur": 13265000, "relative_dur": 0.15978077571669477, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "13.3 ms"}}}, "id": "j75NfSm6rJpIFxW9", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.16.pt.trace.json", "trace_disk_size": "37.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 13265000, "runtime_str": "13.3 ms", "start_timestamp": "23:14:19.204.204313", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 315}, {"idx": 29, "name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"idx": 30, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu2": {"time": {"ts": 1678058059209625000, "dur": 446000, "relative_dur": 0.03403540903540903, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "446 us"}}, "gpu3": {"time": {"ts": 1678058059218004000, "dur": 226000, "relative_dur": 0.017246642246642248, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "226 us"}}}, "id": "1YejRgN6GHYBFPKe", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.37.pt.trace.json", "trace_disk_size": "2.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 446000, "runtime_str": "446 us", "start_timestamp": "23:14:19.209.209625", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 20}, {"idx": 31, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"idx": 32, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059210200000, "dur": 398000, "relative_dur": 0.07318867230599485, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "398 us"}}, "gpu3": {"time": {"ts": 1678058059218232000, "dur": 870000, "relative_dur": 0.1599852887090842, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "870 us"}}}, "id": "JNAY0lxu1breVQJB", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.39.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 870000, "runtime_str": "870 us", "start_timestamp": "23:14:19.210.210200", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 35}, {"idx": 33, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059210761000, "dur": 178000, "relative_dur": 0.03273262228760574, "relative_gap_to_previous": 0.027031997057741817, "parent_is_longest": true, "runtime_str": "178 us"}}, "gpu3": {"time": {"ts": 1678058059219104000, "dur": 870000, "relative_dur": 0.1599852887090842, "relative_gap_to_previous": 0.0003677822728944465, "parent_is_longest": true, "runtime_str": "870 us"}}}, "id": "4YbKVmr2PU2xs4Fr", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.40.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 870000, "runtime_str": "870 us", "start_timestamp": "23:14:19.210.210761", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 34, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059211138000, "dur": 143000, "relative_dur": 0.026296432511952923, "relative_gap_to_previous": 0.033652077969841855, "parent_is_longest": true, "runtime_str": "143 us"}}, "gpu3": {"time": {"ts": 1678058059219976000, "dur": 872000, "relative_dur": 0.16035307098197868, "relative_gap_to_previous": 0.0003677822728944465, "parent_is_longest": true, "runtime_str": "872 us"}}}, "id": "jbc7DWynUEKe8xjJ", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.41.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 872000, "runtime_str": "872 us", "start_timestamp": "23:14:19.211.211138", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 35, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059211308000, "dur": 307000, "relative_dur": 0.05645457888929754, "relative_gap_to_previous": 0.002022802500919456, "parent_is_longest": true, "runtime_str": "307 us"}}, "gpu3": {"time": {"ts": 1678058059220850000, "dur": 82000, "relative_dur": 0.015079073188672305, "relative_gap_to_previous": 0.0003677822728944465, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "89SzdLSFngbwWu8X", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.42.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 307000, "runtime_str": "307 us", "start_timestamp": "23:14:19.211.211308", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 36, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059211801000, "dur": 102000, "relative_dur": 0.01875689591761677, "relative_gap_to_previous": 0.03126149319602795, "parent_is_longest": true, "runtime_str": "102 us"}}, "gpu3": {"time": {"ts": 1678058059220933000, "dur": 774000, "relative_dur": 0.1423317396101508, "relative_gap_to_previous": 0.00018389113644722325, "parent_is_longest": true, "runtime_str": "774 us"}}}, "id": "2yLiLchxqYO3LZS5", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.43.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 774000, "runtime_str": "774 us", "start_timestamp": "23:14:19.211.211801", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 37, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059212100000, "dur": 57000, "relative_dur": 0.010481794777491725, "relative_gap_to_previous": 0.03328429569694741, "parent_is_longest": true, "runtime_str": "57 us"}}, "gpu3": {"time": {"ts": 1678058059221708000, "dur": 320000, "relative_dur": 0.05884516366311144, "relative_gap_to_previous": 0.00018389113644722325, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "Hhp6gpjL1TvRX7Rh", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.44.pt.trace.json", "trace_disk_size": "902 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "23:14:19.212.212100", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 38, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059212350000, "dur": 93000, "relative_dur": 0.01710187568959176, "relative_gap_to_previous": 0.032548731151158514, "parent_is_longest": true, "runtime_str": "93 us"}}, "gpu3": {"time": {"ts": 1678058059222030000, "dur": 379000, "relative_dur": 0.06969474071349761, "relative_gap_to_previous": 0.0003677822728944465, "parent_is_longest": true, "runtime_str": "379 us"}}}, "id": "itlgHh5j5ZRlyUXM", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.45.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 379000, "runtime_str": "379 us", "start_timestamp": "23:14:19.212.212350", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 39, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059212644000, "dur": 82000, "relative_dur": 0.015079073188672305, "relative_gap_to_previous": 0.0340198602427363, "parent_is_longest": true, "runtime_str": "82 us"}}, "gpu3": {"time": {"ts": 1678058059222410000, "dur": 392000, "relative_dur": 0.07208532548731152, "relative_gap_to_previous": 0.00018389113644722325, "parent_is_longest": true, "runtime_str": "392 us"}}}, "id": "4jEmS9RmXwvean7k", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.46.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 392000, "runtime_str": "392 us", "start_timestamp": "23:14:19.212.212644", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 40, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059212911000, "dur": 178000, "relative_dur": 0.03273262228760574, "relative_gap_to_previous": 0.03107760205958073, "parent_is_longest": true, "runtime_str": "178 us"}}, "gpu3": {"time": {"ts": 1678058059222803000, "dur": 867000, "relative_dur": 0.15943361529974256, "relative_gap_to_previous": 0.00018389113644722325, "parent_is_longest": true, "runtime_str": "867 us"}}}, "id": "oC3z8qJBS83NIGtM", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.47.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 867000, "runtime_str": "867 us", "start_timestamp": "23:14:19.212.212911", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059210200000, "dur": 2873000, "relative_dur": 0.21924603174603174, "relative_gap_to_previous": 0.008623321123321124, "parent_is_longest": true, "runtime_str": "2.87 ms"}}, "gpu3": {"time": {"ts": 1678058059218232000, "dur": 5438000, "relative_dur": 0.41498778998779, "relative_gap_to_previous": 0.00015262515262515263, "parent_is_longest": true, "runtime_str": "5.44 ms"}}}, "id": "IUT5jHCU1NLByTw0", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.38.pt.trace.json", "trace_disk_size": "23.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5438000, "runtime_str": "5.44 ms", "start_timestamp": "23:14:19.210.210200", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 191}, {"idx": 41, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu2": {"time": {"ts": 1678058059213097000, "dur": 320000, "relative_dur": 0.02442002442002442, "relative_gap_to_previous": 0.0006105006105006105, "parent_is_longest": true, "runtime_str": "320 us"}}, "gpu3": {"time": {"ts": 1678058059223671000, "dur": 97000, "relative_dur": 0.007402319902319902, "relative_gap_to_previous": 7.631257631257631e-05, "parent_is_longest": true, "runtime_str": "97 us"}}}, "id": "czB3v6twlHiUexBJ", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.48.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "23:14:19.213.213097", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 42, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu2": {"time": {"ts": 1678058059213435000, "dur": 325000, "relative_dur": 0.0248015873015873, "relative_gap_to_previous": 0.00015262515262515263, "parent_is_longest": true, "runtime_str": "325 us"}}, "gpu3": {"time": {"ts": 1678058059223769000, "dur": 222000, "relative_dur": 0.01694139194139194, "relative_gap_to_previous": 7.631257631257631e-05, "parent_is_longest": true, "runtime_str": "222 us"}}}, "id": "dHDMiIJ2KNQkO7ma", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.49.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 325000, "runtime_str": "325 us", "start_timestamp": "23:14:19.213.213435", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 43, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"idx": 44, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu2": {"time": {"ts": 1678058059213925000, "dur": 291000, "relative_dur": 0.04089950808151792, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "291 us"}}, "gpu3": {"time": {"ts": 1678058059223993000, "dur": 3173000, "relative_dur": 0.44595924104005624, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3.17 ms"}}}, "id": "1lwDQhXM8Uh9r3YJ", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.51.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3173000, "runtime_str": "3.17 ms", "start_timestamp": "23:14:19.213.213925", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 45, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu2": {"time": {"ts": 1678058059214384000, "dur": 65000, "relative_dur": 0.009135628952916374, "relative_gap_to_previous": 0.023612087139845396, "parent_is_longest": true, "runtime_str": "65 us"}}, "gpu3": {"time": {"ts": 1678058059227168000, "dur": 318000, "relative_dur": 0.04469430780042164, "relative_gap_to_previous": 0.00028109627547435, "parent_is_longest": true, "runtime_str": "318 us"}}}, "id": "Dy7IJy79z09aQjz7", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.52.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 318000, "runtime_str": "318 us", "start_timestamp": "23:14:19.214.214384", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 46, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu2": {"time": {"ts": 1678058059214605000, "dur": 73000, "relative_dur": 0.010260014054813773, "relative_gap_to_previous": 0.021925509486999298, "parent_is_longest": true, "runtime_str": "73 us"}}, "gpu3": {"time": {"ts": 1678058059227487000, "dur": 380000, "relative_dur": 0.05340829234012649, "relative_gap_to_previous": 0.000140548137737175, "parent_is_longest": true, "runtime_str": "380 us"}}}, "id": "TIus5vPKPa0AtrYD", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.53.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 380000, "runtime_str": "380 us", "start_timestamp": "23:14:19.214.214605", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 47, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu2": {"time": {"ts": 1678058059214806000, "dur": 131000, "relative_dur": 0.01841180604356992, "relative_gap_to_previous": 0.0179901616303584, "parent_is_longest": true, "runtime_str": "131 us"}}, "gpu3": {"time": {"ts": 1678058059227868000, "dur": 3141000, "relative_dur": 0.44146170063246665, "relative_gap_to_previous": 0.000140548137737175, "parent_is_longest": true, "runtime_str": "3.14 ms"}}}, "id": "lkZKIKICmahBXXxl", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.54.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3141000, "runtime_str": "3.14 ms", "start_timestamp": "23:14:19.214.214806", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 48, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu2": {"time": {"ts": 1678058059215090000, "dur": 80000, "relative_dur": 0.011243851018973999, "relative_gap_to_previous": 0.02150386507378777, "parent_is_longest": true, "runtime_str": "80 us"}}, "gpu3": {"time": {"ts": 1678058059231010000, "dur": 98000, "relative_dur": 0.013773717498243148, "relative_gap_to_previous": 0.000140548137737175, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "BqHPHWqgOlN4tg5r", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.55.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 98000, "runtime_str": "98 us", "start_timestamp": "23:14:19.215.215090", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1678058059213925000, "dur": 1245000, "relative_dur": 0.0950091575091575, "relative_gap_to_previous": 0.01137057387057387, "parent_is_longest": true, "runtime_str": "1.25 ms"}}, "gpu3": {"time": {"ts": 1678058059223993000, "dur": 7115000, "relative_dur": 0.5429639804639804, "relative_gap_to_previous": 0.00015262515262515263, "parent_is_longest": true, "runtime_str": "7.12 ms"}}}, "id": "wOtr8mD7N59ruUv1", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.50.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7115000, "runtime_str": "7.12 ms", "start_timestamp": "23:14:19.213.213925", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1678058059209625000, "dur": 5529000, "relative_dur": 0.06659841002168153, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "5.53 ms"}}, "gpu3": {"time": {"ts": 1678058059218004000, "dur": 13104000, "relative_dur": 0.1578414839797639, "relative_gap_to_previous": 1.2045290291496025e-05, "parent_is_longest": true, "runtime_str": "13.1 ms"}}}, "id": "yNabzKH97nHFYCeW", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.36.pt.trace.json", "trace_disk_size": "37.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 13104000, "runtime_str": "13.1 ms", "start_timestamp": "23:14:19.209.209625", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 314}, {"idx": 49, "name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"idx": 50, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu2": {"time": {"ts": 1678058059215186000, "dur": 379000, "relative_dur": 0.02711403634282444, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "379 us"}}, "gpu3": {"time": {"ts": 1678058059231110000, "dur": 224000, "relative_dur": 0.01602518242953212, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "224 us"}}}, "id": "Vgq1TAhPMc3pC5TF", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.57.pt.trace.json", "trace_disk_size": "2.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 379000, "runtime_str": "379 us", "start_timestamp": "23:14:19.215.215186", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 20}, {"idx": 51, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"idx": 52, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059215739000, "dur": 332000, "relative_dur": 0.0568104038329911, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "332 us"}}, "gpu3": {"time": {"ts": 1678058059231337000, "dur": 953000, "relative_dur": 0.1630732375085558, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "TRZmeA9iudixUQ56", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.59.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.215.215739", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 35}, {"idx": 53, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059216233000, "dur": 129000, "relative_dur": 0.022073921971252568, "relative_gap_to_previous": 0.025325119780971937, "parent_is_longest": true, "runtime_str": "129 us"}}, "gpu3": {"time": {"ts": 1678058059232291000, "dur": 953000, "relative_dur": 0.1630732375085558, "relative_gap_to_previous": 0.00017111567419575633, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "paUuVLUittOsJoXT", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.60.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.216.216233", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 54, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059216556000, "dur": 160000, "relative_dur": 0.02737850787132101, "relative_gap_to_previous": 0.030800821355236138, "parent_is_longest": true, "runtime_str": "160 us"}}, "gpu3": {"time": {"ts": 1678058059233246000, "dur": 952000, "relative_dur": 0.16290212183436004, "relative_gap_to_previous": 0.00034223134839151266, "parent_is_longest": true, "runtime_str": "952 us"}}}, "id": "PTg6yUkZXMATbidQ", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.61.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "23:14:19.216.216556", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 55, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059216738000, "dur": 242000, "relative_dur": 0.04140999315537303, "relative_gap_to_previous": 0.0013689253935660506, "parent_is_longest": true, "runtime_str": "242 us"}}, "gpu3": {"time": {"ts": 1678058059234200000, "dur": 82000, "relative_dur": 0.01403148528405202, "relative_gap_to_previous": 0.00034223134839151266, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "1zOm0Au6js1pywb3", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.62.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 242000, "runtime_str": "242 us", "start_timestamp": "23:14:19.216.216738", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 56, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059217119000, "dur": 72000, "relative_dur": 0.012320328542094456, "relative_gap_to_previous": 0.02138945927446954, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1678058059234283000, "dur": 780000, "relative_dur": 0.13347022587268995, "relative_gap_to_previous": 0.00017111567419575633, "parent_is_longest": true, "runtime_str": "780 us"}}}, "id": "NU2yMWREYAYjlE4S", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.63.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 780000, "runtime_str": "780 us", "start_timestamp": "23:14:19.217.217119", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 57, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059217344000, "dur": 49000, "relative_dur": 0.00838466803559206, "relative_gap_to_previous": 0.02378507871321013, "parent_is_longest": true, "runtime_str": "49 us"}}, "gpu3": {"time": {"ts": 1678058059235064000, "dur": 321000, "relative_dur": 0.054928131416837785, "relative_gap_to_previous": 0.00017111567419575633, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "i6VroXLgs6tV0eJs", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.64.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "23:14:19.217.217344", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 58, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059217527000, "dur": 62000, "relative_dur": 0.010609171800136893, "relative_gap_to_previous": 0.02053388090349076, "parent_is_longest": true, "runtime_str": "62 us"}}, "gpu3": {"time": {"ts": 1678058059235386000, "dur": 386000, "relative_dur": 0.06605065023956194, "relative_gap_to_previous": 0.00017111567419575633, "parent_is_longest": true, "runtime_str": "386 us"}}}, "id": "7X6gRA8bPqHrwgDM", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.65.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 386000, "runtime_str": "386 us", "start_timestamp": "23:14:19.217.217527", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 59, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059217724000, "dur": 56000, "relative_dur": 0.009582477754962354, "relative_gap_to_previous": 0.020704996577686516, "parent_is_longest": true, "runtime_str": "56 us"}}, "gpu3": {"time": {"ts": 1678058059235773000, "dur": 451000, "relative_dur": 0.0771731690622861, "relative_gap_to_previous": 0.00017111567419575633, "parent_is_longest": true, "runtime_str": "451 us"}}}, "id": "wd75k1KEboh2svty", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.66.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 451000, "runtime_str": "451 us", "start_timestamp": "23:14:19.217.217724", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 60, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059217912000, "dur": 136000, "relative_dur": 0.02327173169062286, "relative_gap_to_previous": 0.020191649555099247, "parent_is_longest": true, "runtime_str": "136 us"}}, "gpu3": {"time": {"ts": 1678058059236226000, "dur": 955000, "relative_dur": 0.1634154688569473, "relative_gap_to_previous": 0.00034223134839151266, "parent_is_longest": true, "runtime_str": "955 us"}}}, "id": "4IxKduumMxDwu0z7", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.67.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 955000, "runtime_str": "955 us", "start_timestamp": "23:14:19.217.217912", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059215739000, "dur": 2295000, "relative_dur": 0.16418657890971528, "relative_gap_to_previous": 0.01144655887823723, "parent_is_longest": true, "runtime_str": "2.29 ms"}}, "gpu3": {"time": {"ts": 1678058059231337000, "dur": 5844000, "relative_dur": 0.41808556302761485, "relative_gap_to_previous": 0.00021462297896694807, "parent_is_longest": true, "runtime_str": "5.84 ms"}}}, "id": "5sA009FFGT2CA504", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.58.pt.trace.json", "trace_disk_size": "23.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5844000, "runtime_str": "5.84 ms", "start_timestamp": "23:14:19.215.215739", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 191}, {"idx": 61, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu2": {"time": {"ts": 1678058059218053000, "dur": 224000, "relative_dur": 0.01602518242953212, "relative_gap_to_previous": 0.00035770496494491345, "parent_is_longest": true, "runtime_str": "224 us"}}, "gpu3": {"time": {"ts": 1678058059237182000, "dur": 99000, "relative_dur": 0.007082558305909286, "relative_gap_to_previous": 7.154099298898269e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "7Ukzs1J5P8PIexPH", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.68.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 224000, "runtime_str": "224 us", "start_timestamp": "23:14:19.218.218053", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 62, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu2": {"time": {"ts": 1678058059218289000, "dur": 218000, "relative_dur": 0.015595936471598225, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "218 us"}}, "gpu3": {"time": {"ts": 1678058059237282000, "dur": 229000, "relative_dur": 0.016382887394477037, "relative_gap_to_previous": 7.154099298898269e-05, "parent_is_longest": true, "runtime_str": "229 us"}}}, "id": "aczAjXVtpKlTRHje", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.69.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 229000, "runtime_str": "229 us", "start_timestamp": "23:14:19.218.218289", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 63, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"idx": 64, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu2": {"time": {"ts": 1678058059218634000, "dur": 213000, "relative_dur": 0.028115100316789862, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "213 us"}}, "gpu3": {"time": {"ts": 1678058059237512000, "dur": 3376000, "relative_dur": 0.44561774023231254, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3.38 ms"}}}, "id": "nyXEG5DDGUZfZ94h", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.71.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3376000, "runtime_str": "3.38 ms", "start_timestamp": "23:14:19.218.218634", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 65, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu2": {"time": {"ts": 1678058059218976000, "dur": 49000, "relative_dur": 0.00646779303062302, "relative_gap_to_previous": 0.017027455121436115, "parent_is_longest": true, "runtime_str": "49 us"}}, "gpu3": {"time": {"ts": 1678058059240889000, "dur": 320000, "relative_dur": 0.042238648363252376, "relative_gap_to_previous": 0.00013199577613516366, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "ylfPlaxRPMhypunq", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.72.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "23:14:19.218.218976", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 66, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu2": {"time": {"ts": 1678058059219150000, "dur": 65000, "relative_dur": 0.008579725448785639, "relative_gap_to_previous": 0.01649947201689546, "parent_is_longest": true, "runtime_str": "65 us"}}, "gpu3": {"time": {"ts": 1678058059241210000, "dur": 388000, "relative_dur": 0.051214361140443504, "relative_gap_to_previous": 0.00013199577613516366, "parent_is_longest": true, "runtime_str": "388 us"}}}, "id": "5ZcZqCBiKLylK0jP", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.73.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 388000, "runtime_str": "388 us", "start_timestamp": "23:14:19.219.219150", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 67, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu2": {"time": {"ts": 1678058059219339000, "dur": 108000, "relative_dur": 0.014255543822597676, "relative_gap_to_previous": 0.016367476240760296, "parent_is_longest": true, "runtime_str": "108 us"}}, "gpu3": {"time": {"ts": 1678058059241599000, "dur": 3391000, "relative_dur": 0.44759767687434004, "relative_gap_to_previous": 0.00013199577613516366, "parent_is_longest": true, "runtime_str": "3.39 ms"}}}, "id": "0ikna62ztR7NvxTI", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.74.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3391000, "runtime_str": "3.39 ms", "start_timestamp": "23:14:19.219.219339", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 68, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu2": {"time": {"ts": 1678058059219579000, "dur": 67000, "relative_dur": 0.008843717001055967, "relative_gap_to_previous": 0.017423442449841606, "parent_is_longest": true, "runtime_str": "67 us"}}, "gpu3": {"time": {"ts": 1678058059244991000, "dur": 97000, "relative_dur": 0.012803590285110876, "relative_gap_to_previous": 0.00013199577613516366, "parent_is_longest": true, "runtime_str": "97 us"}}}, "id": "Flm2yXyJ9b07vs34", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.75.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 97000, "runtime_str": "97 us", "start_timestamp": "23:14:19.219.219579", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1678058059218634000, "dur": 1012000, "relative_dur": 0.07239948490485047, "relative_gap_to_previous": 0.008084132207755043, "parent_is_longest": true, "runtime_str": "1.01 ms"}}, "gpu3": {"time": {"ts": 1678058059237512000, "dur": 7576000, "relative_dur": 0.5419945628845328, "relative_gap_to_previous": 7.154099298898269e-05, "parent_is_longest": true, "runtime_str": "7.58 ms"}}}, "id": "eiHvArCgBcHzrHFl", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.70.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7576000, "runtime_str": "7.58 ms", "start_timestamp": "23:14:19.218.218634", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1678058059215186000, "dur": 4446000, "relative_dur": 0.05355336063599133, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "4.45 ms"}}, "gpu3": {"time": {"ts": 1678058059231110000, "dur": 13978000, "relative_dur": 0.16836906769453144, "relative_gap_to_previous": 2.409058058299205e-05, "parent_is_longest": true, "runtime_str": "14 ms"}}}, "id": "75VK9j4pTWeZoKPB", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.56.pt.trace.json", "trace_disk_size": "37.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 13978000, "runtime_str": "14 ms", "start_timestamp": "23:14:19.215.215186", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 314}, {"idx": 69, "name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"idx": 70, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu2": {"time": {"ts": 1678058059219661000, "dur": 340000, "relative_dur": 0.023943661971830985, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "340 us"}}, "gpu3": {"time": {"ts": 1678058059245090000, "dur": 227000, "relative_dur": 0.015985915492957747, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "227 us"}}}, "id": "wxN1IKWbh9fThk3D", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.77.pt.trace.json", "trace_disk_size": "2.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 340000, "runtime_str": "340 us", "start_timestamp": "23:14:19.219.219661", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 20}, {"idx": 71, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"idx": 72, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059220126000, "dur": 298000, "relative_dur": 0.05087928973877412, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "298 us"}}, "gpu3": {"time": {"ts": 1678058059245319000, "dur": 952000, "relative_dur": 0.16254054976950658, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "952 us"}}}, "id": "QvzOMDngBFK8wmGw", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.79.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "23:14:19.220.220126", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 35}, {"idx": 73, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059220569000, "dur": 121000, "relative_dur": 0.02065904046440157, "relative_gap_to_previous": 0.022537135052074442, "parent_is_longest": true, "runtime_str": "121 us"}}, "gpu3": {"time": {"ts": 1678058059246273000, "dur": 953000, "relative_dur": 0.1627112856411132, "relative_gap_to_previous": 0.0003414717432132491, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "pWEsgIGWUCoPRFyV", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.80.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.220.220569", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 74, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059220833000, "dur": 125000, "relative_dur": 0.021341983950828067, "relative_gap_to_previous": 0.02219566330886119, "parent_is_longest": true, "runtime_str": "125 us"}}, "gpu3": {"time": {"ts": 1678058059247228000, "dur": 953000, "relative_dur": 0.1627112856411132, "relative_gap_to_previous": 0.0003414717432132491, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "CsszZaUVRIbOGfgf", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.81.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.220.220833", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 75, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059220976000, "dur": 223000, "relative_dur": 0.03807409936827728, "relative_gap_to_previous": 0.0008536793580331228, "parent_is_longest": true, "runtime_str": "223 us"}}, "gpu3": {"time": {"ts": 1678058059248182000, "dur": 82000, "relative_dur": 0.014000341471743213, "relative_gap_to_previous": 0.00017073587160662456, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "gGlSgqcfsFz9UmH7", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.82.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 223000, "runtime_str": "223 us", "start_timestamp": "23:14:19.220.220976", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 76, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059221335000, "dur": 72000, "relative_dur": 0.012292982755676968, "relative_gap_to_previous": 0.02100051220761482, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1678058059248266000, "dur": 776000, "relative_dur": 0.13249103636674064, "relative_gap_to_previous": 0.0003414717432132491, "parent_is_longest": true, "runtime_str": "776 us"}}}, "id": "hDn5mhLsAq4A8GHK", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.83.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 776000, "runtime_str": "776 us", "start_timestamp": "23:14:19.221.221335", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 77, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059221561000, "dur": 48000, "relative_dur": 0.00819532183711798, "relative_gap_to_previous": 0.024073757896534063, "parent_is_longest": true, "runtime_str": "48 us"}}, "gpu3": {"time": {"ts": 1678058059249044000, "dur": 321000, "relative_dur": 0.05480621478572648, "relative_gap_to_previous": 0.0003414717432132491, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "Ux9KQKxYIa1NDLv5", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.84.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "23:14:19.221.221561", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 78, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059221744000, "dur": 61000, "relative_dur": 0.010414888168004097, "relative_gap_to_previous": 0.020829776336008195, "parent_is_longest": true, "runtime_str": "61 us"}}, "gpu3": {"time": {"ts": 1678058059249366000, "dur": 385000, "relative_dur": 0.06573331056855045, "relative_gap_to_previous": 0.00017073587160662456, "parent_is_longest": true, "runtime_str": "385 us"}}}, "id": "WNvgyuslg9d8gH7f", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.85.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 385000, "runtime_str": "385 us", "start_timestamp": "23:14:19.221.221744", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 79, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059221940000, "dur": 56000, "relative_dur": 0.009561208809970976, "relative_gap_to_previous": 0.020829776336008195, "parent_is_longest": true, "runtime_str": "56 us"}}, "gpu3": {"time": {"ts": 1678058059249752000, "dur": 451000, "relative_dur": 0.07700187809458767, "relative_gap_to_previous": 0.00017073587160662456, "parent_is_longest": true, "runtime_str": "451 us"}}}, "id": "zftMUfLuO7Y5kE7a", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.86.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 451000, "runtime_str": "451 us", "start_timestamp": "23:14:19.221.221940", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 80, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059222130000, "dur": 135000, "relative_dur": 0.023049342666894315, "relative_gap_to_previous": 0.02065904046440157, "parent_is_longest": true, "runtime_str": "135 us"}}, "gpu3": {"time": {"ts": 1678058059250205000, "dur": 971000, "relative_dur": 0.16578453133003243, "relative_gap_to_previous": 0.0003414717432132491, "parent_is_longest": true, "runtime_str": "971 us"}}}, "id": "rSj2N3cdlgv8O4zP", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.87.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 971000, "runtime_str": "971 us", "start_timestamp": "23:14:19.222.222130", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059220126000, "dur": 2126000, "relative_dur": 0.14971830985915494, "relative_gap_to_previous": 0.007887323943661971, "parent_is_longest": true, "runtime_str": "2.13 ms"}}, "gpu3": {"time": {"ts": 1678058059245319000, "dur": 5857000, "relative_dur": 0.41246478873239434, "relative_gap_to_previous": 0.00014084507042253522, "parent_is_longest": true, "runtime_str": "5.86 ms"}}}, "id": "hV75MfX1rMRuvNWS", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.78.pt.trace.json", "trace_disk_size": "23.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5857000, "runtime_str": "5.86 ms", "start_timestamp": "23:14:19.220.220126", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 191}, {"idx": 81, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu2": {"time": {"ts": 1678058059222271000, "dur": 223000, "relative_dur": 0.015704225352112675, "relative_gap_to_previous": 0.00042253521126760566, "parent_is_longest": true, "runtime_str": "223 us"}}, "gpu3": {"time": {"ts": 1678058059251178000, "dur": 99000, "relative_dur": 0.006971830985915493, "relative_gap_to_previous": 0.00014084507042253522, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "M7k0iWsMh68G85k7", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.88.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 223000, "runtime_str": "223 us", "start_timestamp": "23:14:19.222.222271", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 82, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu2": {"time": {"ts": 1678058059222507000, "dur": 217000, "relative_dur": 0.01528169014084507, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "217 us"}}, "gpu3": {"time": {"ts": 1678058059251278000, "dur": 231000, "relative_dur": 0.016267605633802817, "relative_gap_to_previous": 7.042253521126761e-05, "parent_is_longest": true, "runtime_str": "231 us"}}}, "id": "34nra65RnB7cLAjH", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.89.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 231000, "runtime_str": "231 us", "start_timestamp": "23:14:19.222.222507", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 83, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"idx": 84, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu2": {"time": {"ts": 1678058059222858000, "dur": 219000, "relative_dur": 0.028149100257069407, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "219 us"}}, "gpu3": {"time": {"ts": 1678058059251510000, "dur": 3476000, "relative_dur": 0.4467866323907455, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3.48 ms"}}}, "id": "f9PZLQxLERsSjWtJ", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.91.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3476000, "runtime_str": "3.48 ms", "start_timestamp": "23:14:19.222.222858", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 85, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu2": {"time": {"ts": 1678058059223205000, "dur": 49000, "relative_dur": 0.006298200514138818, "relative_gap_to_previous": 0.016452442159383032, "parent_is_longest": true, "runtime_str": "49 us"}}, "gpu3": {"time": {"ts": 1678058059254987000, "dur": 320000, "relative_dur": 0.04113110539845758, "relative_gap_to_previous": 0.00012853470437017994, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "SAQk0s0lFl1TZLEf", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.92.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "23:14:19.223.223205", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 86, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu2": {"time": {"ts": 1678058059223377000, "dur": 64000, "relative_dur": 0.008226221079691516, "relative_gap_to_previous": 0.015809768637532133, "parent_is_longest": true, "runtime_str": "64 us"}}, "gpu3": {"time": {"ts": 1678058059255308000, "dur": 390000, "relative_dur": 0.05012853470437018, "relative_gap_to_previous": 0.00012853470437017994, "parent_is_longest": true, "runtime_str": "390 us"}}}, "id": "pniIjUfsv6ENfLUd", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.93.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 390000, "runtime_str": "390 us", "start_timestamp": "23:14:19.223.223377", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 87, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu2": {"time": {"ts": 1678058059223565000, "dur": 104000, "relative_dur": 0.013367609254498715, "relative_gap_to_previous": 0.015938303341902313, "parent_is_longest": true, "runtime_str": "104 us"}}, "gpu3": {"time": {"ts": 1678058059255699000, "dur": 3490000, "relative_dur": 0.448586118251928, "relative_gap_to_previous": 0.00012853470437017994, "parent_is_longest": true, "runtime_str": "3.49 ms"}}}, "id": "8A8YbklP7WH8Z0Gv", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.94.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3490000, "runtime_str": "3.49 ms", "start_timestamp": "23:14:19.223.223565", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 88, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu2": {"time": {"ts": 1678058059223797000, "dur": 67000, "relative_dur": 0.008611825192802057, "relative_gap_to_previous": 0.016452442159383032, "parent_is_longest": true, "runtime_str": "67 us"}}, "gpu3": {"time": {"ts": 1678058059259191000, "dur": 99000, "relative_dur": 0.012724935732647815, "relative_gap_to_previous": 0.0002570694087403599, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "AyNezmsp4lQ4qg9S", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.95.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "23:14:19.223.223797", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1678058059222858000, "dur": 1006000, "relative_dur": 0.07084507042253521, "relative_gap_to_previous": 0.00852112676056338, "parent_is_longest": true, "runtime_str": "1.01 ms"}}, "gpu3": {"time": {"ts": 1678058059251510000, "dur": 7780000, "relative_dur": 0.547887323943662, "relative_gap_to_previous": 7.042253521126761e-05, "parent_is_longest": true, "runtime_str": "7.78 ms"}}}, "id": "2DYTHjXrgxt0Wjpk", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.90.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7780000, "runtime_str": "7.78 ms", "start_timestamp": "23:14:19.222.222858", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1678058059219661000, "dur": 4190000, "relative_dur": 0.05046976632136835, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "4.19 ms"}}, "gpu3": {"time": {"ts": 1678058059245090000, "dur": 14200000, "relative_dur": 0.17104312213924355, "relative_gap_to_previous": 2.409058058299205e-05, "parent_is_longest": true, "runtime_str": "14.2 ms"}}}, "id": "Tk0FtzXsPl6pqZrH", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.76.pt.trace.json", "trace_disk_size": "37.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 14200000, "runtime_str": "14.2 ms", "start_timestamp": "23:14:19.219.219661", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 314}, {"idx": 89, "name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"idx": 90, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu2": {"time": {"ts": 1678058059223879000, "dur": 341000, "relative_dur": 0.023894611449793286, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "341 us"}}, "gpu3": {"time": {"ts": 1678058059259291000, "dur": 226000, "relative_dur": 0.015836311400742764, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "226 us"}}}, "id": "YO9uK1BIHxtN1kXH", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.97.pt.trace.json", "trace_disk_size": "2.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 341000, "runtime_str": "341 us", "start_timestamp": "23:14:19.223.223879", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 20}, {"idx": 91, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"idx": 92, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059224344000, "dur": 299000, "relative_dur": 0.05015936923335011, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "299 us"}}, "gpu3": {"time": {"ts": 1678058059259519000, "dur": 979000, "relative_dur": 0.1642341888944808, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "979 us"}}}, "id": "obpopxi88g6koGD7", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.99.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 979000, "runtime_str": "979 us", "start_timestamp": "23:14:19.224.224344", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 35}, {"idx": 93, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059224788000, "dur": 124000, "relative_dur": 0.020801878879382653, "relative_gap_to_previous": 0.02214393558127831, "parent_is_longest": true, "runtime_str": "124 us"}}, "gpu3": {"time": {"ts": 1678058059260499000, "dur": 978000, "relative_dur": 0.16406643180674382, "relative_gap_to_previous": 0.00016775708773695687, "parent_is_longest": true, "runtime_str": "978 us"}}}, "id": "j9lKhwNb1WtENNvJ", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.100.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 978000, "runtime_str": "978 us", "start_timestamp": "23:14:19.224.224788", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 94, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059225055000, "dur": 122000, "relative_dur": 0.02046636470390874, "relative_gap_to_previous": 0.021808421405804395, "parent_is_longest": true, "runtime_str": "122 us"}}, "gpu3": {"time": {"ts": 1678058059261479000, "dur": 978000, "relative_dur": 0.16406643180674382, "relative_gap_to_previous": 0.00033551417547391375, "parent_is_longest": true, "runtime_str": "978 us"}}}, "id": "G3t3yutMoN4aUAWe", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.101.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 978000, "runtime_str": "978 us", "start_timestamp": "23:14:19.225.225055", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 95, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059225195000, "dur": 225000, "relative_dur": 0.0377453447408153, "relative_gap_to_previous": 0.0008387854386847844, "parent_is_longest": true, "runtime_str": "225 us"}}, "gpu3": {"time": {"ts": 1678058059262459000, "dur": 82000, "relative_dur": 0.013756081194430465, "relative_gap_to_previous": 0.00033551417547391375, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "w3tU8IO8Ri2Xkm2K", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.102.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 225000, "runtime_str": "225 us", "start_timestamp": "23:14:19.225.225195", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 96, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059225555000, "dur": 73000, "relative_dur": 0.012246267404797853, "relative_gap_to_previous": 0.02046636470390874, "parent_is_longest": true, "runtime_str": "73 us"}}, "gpu3": {"time": {"ts": 1678058059262542000, "dur": 780000, "relative_dur": 0.13085052843482636, "relative_gap_to_previous": 0.00016775708773695687, "parent_is_longest": true, "runtime_str": "780 us"}}}, "id": "rOxThoMlkhzbDksP", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.103.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 780000, "runtime_str": "780 us", "start_timestamp": "23:14:19.225.225555", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 97, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059225780000, "dur": 52000, "relative_dur": 0.008723368562321757, "relative_gap_to_previous": 0.023318235195437007, "parent_is_longest": true, "runtime_str": "52 us"}}, "gpu3": {"time": {"ts": 1678058059263323000, "dur": 321000, "relative_dur": 0.05385002516356316, "relative_gap_to_previous": 0.00016775708773695687, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "tvscuVxbdmFwrCYp", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.104.pt.trace.json", "trace_disk_size": "902 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "23:14:19.225.225780", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 98, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059225967000, "dur": 62000, "relative_dur": 0.010400939439691326, "relative_gap_to_previous": 0.02046636470390874, "parent_is_longest": true, "runtime_str": "62 us"}}, "gpu3": {"time": {"ts": 1678058059263646000, "dur": 389000, "relative_dur": 0.06525750712967623, "relative_gap_to_previous": 0.00033551417547391375, "parent_is_longest": true, "runtime_str": "389 us"}}}, "id": "UOIvyshxvxEmxWDU", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.105.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 389000, "runtime_str": "389 us", "start_timestamp": "23:14:19.225.225967", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 99, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059226172000, "dur": 61000, "relative_dur": 0.01023318235195437, "relative_gap_to_previous": 0.021808421405804395, "parent_is_longest": true, "runtime_str": "61 us"}}, "gpu3": {"time": {"ts": 1678058059264036000, "dur": 463000, "relative_dur": 0.07767153162221103, "relative_gap_to_previous": 0.00016775708773695687, "parent_is_longest": true, "runtime_str": "463 us"}}}, "id": "9zRhK3VhtY8kspGK", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.106.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 463000, "runtime_str": "463 us", "start_timestamp": "23:14:19.226.226172", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 100, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059226366000, "dur": 136000, "relative_dur": 0.022814963932226137, "relative_gap_to_previous": 0.020130850528434826, "parent_is_longest": true, "runtime_str": "136 us"}}, "gpu3": {"time": {"ts": 1678058059264501000, "dur": 979000, "relative_dur": 0.1642341888944808, "relative_gap_to_previous": 0.00033551417547391375, "parent_is_longest": true, "runtime_str": "979 us"}}}, "id": "p5vXOTTPiPE97d5y", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.107.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 979000, "runtime_str": "979 us", "start_timestamp": "23:14:19.226.226366", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059224344000, "dur": 2145000, "relative_dur": 0.15030481395837714, "relative_gap_to_previous": 0.007778011351692243, "parent_is_longest": true, "runtime_str": "2.15 ms"}}, "gpu3": {"time": {"ts": 1678058059259519000, "dur": 5961000, "relative_dur": 0.41770023123817535, "relative_gap_to_previous": 0.00014014434867913952, "parent_is_longest": true, "runtime_str": "5.96 ms"}}}, "id": "wwTeM39CRj4U3tag", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.98.pt.trace.json", "trace_disk_size": "23.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5961000, "runtime_str": "5.96 ms", "start_timestamp": "23:14:19.224.224344", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 191}, {"idx": 101, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu2": {"time": {"ts": 1678058059226507000, "dur": 227000, "relative_dur": 0.015906383575082336, "relative_gap_to_previous": 0.0003503608716978488, "parent_is_longest": true, "runtime_str": "227 us"}}, "gpu3": {"time": {"ts": 1678058059265482000, "dur": 99000, "relative_dur": 0.006937145259617406, "relative_gap_to_previous": 0.00014014434867913952, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "04cfXysms9CnnUBy", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.108.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 227000, "runtime_str": "227 us", "start_timestamp": "23:14:19.226.226507", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 102, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu2": {"time": {"ts": 1678058059226747000, "dur": 217000, "relative_dur": 0.015205661831686637, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "217 us"}}, "gpu3": {"time": {"ts": 1678058059265582000, "dur": 232000, "relative_dur": 0.016256744446780182, "relative_gap_to_previous": 7.007217433956976e-05, "parent_is_longest": true, "runtime_str": "232 us"}}}, "id": "1gDvRaeYVD9DLQan", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.109.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 232000, "runtime_str": "232 us", "start_timestamp": "23:14:19.226.226747", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 103, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"idx": 104, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu2": {"time": {"ts": 1678058059227089000, "dur": 215000, "relative_dur": 0.027756261296152853, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "215 us"}}, "gpu3": {"time": {"ts": 1678058059265816000, "dur": 3477000, "relative_dur": 0.4488768396591789, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3.48 ms"}}}, "id": "KAFrfTvLiEW0e9Js", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.111.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3477000, "runtime_str": "3.48 ms", "start_timestamp": "23:14:19.227.227089", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 105, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu2": {"time": {"ts": 1678058059227433000, "dur": 50000, "relative_dur": 0.006454944487477408, "relative_gap_to_previous": 0.016653756777691712, "parent_is_longest": true, "runtime_str": "50 us"}}, "gpu3": {"time": {"ts": 1678058059269294000, "dur": 321000, "relative_dur": 0.04144074360960496, "relative_gap_to_previous": 0.00012909888974954814, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "7n6mslKbzvvlFxiu", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.112.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "23:14:19.227.227433", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 106, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu2": {"time": {"ts": 1678058059227609000, "dur": 67000, "relative_dur": 0.008649625613219726, "relative_gap_to_previous": 0.016266460108443067, "parent_is_longest": true, "runtime_str": "67 us"}}, "gpu3": {"time": {"ts": 1678058059269616000, "dur": 388000, "relative_dur": 0.050090369222824685, "relative_gap_to_previous": 0.00012909888974954814, "parent_is_longest": true, "runtime_str": "388 us"}}}, "id": "S5ifNAdLYioQ50sc", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.113.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 388000, "runtime_str": "388 us", "start_timestamp": "23:14:19.227.227609", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 107, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu2": {"time": {"ts": 1678058059227800000, "dur": 103000, "relative_dur": 0.01329718564420346, "relative_gap_to_previous": 0.01600826232894397, "parent_is_longest": true, "runtime_str": "103 us"}}, "gpu3": {"time": {"ts": 1678058059270006000, "dur": 3456000, "relative_dur": 0.44616576297443844, "relative_gap_to_previous": 0.0002581977794990963, "parent_is_longest": true, "runtime_str": "3.46 ms"}}}, "id": "H9dBE6O2BX1BagJU", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.114.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3456000, "runtime_str": "3.46 ms", "start_timestamp": "23:14:19.227.227800", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 108, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu2": {"time": {"ts": 1678058059228033000, "dur": 69000, "relative_dur": 0.008907823392718823, "relative_gap_to_previous": 0.01678285566744126, "parent_is_longest": true, "runtime_str": "69 us"}}, "gpu3": {"time": {"ts": 1678058059273464000, "dur": 98000, "relative_dur": 0.012651691195455719, "relative_gap_to_previous": 0.0002581977794990963, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "dkjWXGkiJvUNKpu3", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.115.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 98000, "runtime_str": "98 us", "start_timestamp": "23:14:19.228.228033", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1678058059227089000, "dur": 1013000, "relative_dur": 0.07098311260598417, "relative_gap_to_previous": 0.007848083526031812, "parent_is_longest": true, "runtime_str": "1.01 ms"}}, "gpu3": {"time": {"ts": 1678058059265816000, "dur": 7746000, "relative_dur": 0.5427790624343073, "relative_gap_to_previous": 0.00014014434867913952, "parent_is_longest": true, "runtime_str": "7.75 ms"}}}, "id": "nFT2pRG4kaTDosSM", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.110.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7746000, "runtime_str": "7.75 ms", "start_timestamp": "23:14:19.227.227089", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1678058059223879000, "dur": 4210000, "relative_dur": 0.050710672127198264, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "4.21 ms"}}, "gpu3": {"time": {"ts": 1678058059259291000, "dur": 14271000, "relative_dur": 0.17189833774993976, "relative_gap_to_previous": 1.2045290291496025e-05, "parent_is_longest": true, "runtime_str": "14.3 ms"}}}, "id": "J01mGFvpeUpy1uaJ", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.96.pt.trace.json", "trace_disk_size": "37.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 14271000, "runtime_str": "14.3 ms", "start_timestamp": "23:14:19.223.223879", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 314}, {"idx": 109, "name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"idx": 110, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu2": {"time": {"ts": 1678058059228117000, "dur": 340000, "relative_dur": 0.023952095808383235, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "340 us"}}, "gpu3": {"time": {"ts": 1678058059273563000, "dur": 228000, "relative_dur": 0.016061993659739343, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "228 us"}}}, "id": "5eXpH0xqDlQCF97K", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.117.pt.trace.json", "trace_disk_size": "2.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 340000, "runtime_str": "340 us", "start_timestamp": "23:14:19.228.228117", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 20}, {"idx": 111, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"idx": 112, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059228580000, "dur": 296000, "relative_dur": 0.04999155548049316, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "296 us"}}, "gpu3": {"time": {"ts": 1678058059273791000, "dur": 970000, "relative_dur": 0.16382367843269718, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "970 us"}}}, "id": "cSeQR99dcyHX7JnC", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.119.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 970000, "runtime_str": "970 us", "start_timestamp": "23:14:19.228.228580", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 35}, {"idx": 113, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059229019000, "dur": 123000, "relative_dur": 0.02077351798682655, "relative_gap_to_previous": 0.021955750717784158, "parent_is_longest": true, "runtime_str": "123 us"}}, "gpu3": {"time": {"ts": 1678058059274763000, "dur": 970000, "relative_dur": 0.16382367843269718, "relative_gap_to_previous": 0.00033778078027360244, "parent_is_longest": true, "runtime_str": "970 us"}}}, "id": "8mxD8tVGVtTUGn8f", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.120.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 970000, "runtime_str": "970 us", "start_timestamp": "23:14:19.229.229019", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 114, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059229284000, "dur": 121000, "relative_dur": 0.020435737206552947, "relative_gap_to_previous": 0.021786860327647355, "parent_is_longest": true, "runtime_str": "121 us"}}, "gpu3": {"time": {"ts": 1678058059275735000, "dur": 971000, "relative_dur": 0.16399256882283397, "relative_gap_to_previous": 0.00033778078027360244, "parent_is_longest": true, "runtime_str": "971 us"}}}, "id": "06lNhMYgpzM41qs7", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.121.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 971000, "runtime_str": "971 us", "start_timestamp": "23:14:19.229.229284", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 115, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059229422000, "dur": 235000, "relative_dur": 0.03968924168214828, "relative_gap_to_previous": 0.0006755615605472049, "parent_is_longest": true, "runtime_str": "235 us"}}, "gpu3": {"time": {"ts": 1678058059276707000, "dur": 82000, "relative_dur": 0.0138490119912177, "relative_gap_to_previous": 0.00016889039013680122, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "nabI7hckta3C7PNc", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.122.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 235000, "runtime_str": "235 us", "start_timestamp": "23:14:19.229.229422", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 116, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059229791000, "dur": 72000, "relative_dur": 0.012160108089849688, "relative_gap_to_previous": 0.020435737206552947, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1678058059276790000, "dur": 776000, "relative_dur": 0.13105894274615773, "relative_gap_to_previous": 0.00016889039013680122, "parent_is_longest": true, "runtime_str": "776 us"}}}, "id": "dtRKdWs2Qes1GHdt", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.123.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 776000, "runtime_str": "776 us", "start_timestamp": "23:14:19.229.229791", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 117, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059230016000, "dur": 50000, "relative_dur": 0.00844451950684006, "relative_gap_to_previous": 0.02364465461915217, "parent_is_longest": true, "runtime_str": "50 us"}}, "gpu3": {"time": {"ts": 1678058059277568000, "dur": 321000, "relative_dur": 0.05421381523391319, "relative_gap_to_previous": 0.00033778078027360244, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "A75gpRjvzjbZTb05", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.124.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "23:14:19.230.230016", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 118, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059230200000, "dur": 62000, "relative_dur": 0.010471204188481676, "relative_gap_to_previous": 0.020435737206552947, "parent_is_longest": true, "runtime_str": "62 us"}}, "gpu3": {"time": {"ts": 1678058059277891000, "dur": 388000, "relative_dur": 0.06552947137307887, "relative_gap_to_previous": 0.00033778078027360244, "parent_is_longest": true, "runtime_str": "388 us"}}}, "id": "5gYibyMJIplI93RJ", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.125.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 388000, "runtime_str": "388 us", "start_timestamp": "23:14:19.230.230200", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 119, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059230395000, "dur": 55000, "relative_dur": 0.009288971457524067, "relative_gap_to_previous": 0.020266846816416145, "parent_is_longest": true, "runtime_str": "55 us"}}, "gpu3": {"time": {"ts": 1678058059278280000, "dur": 458000, "relative_dur": 0.07735179868265496, "relative_gap_to_previous": 0.00016889039013680122, "parent_is_longest": true, "runtime_str": "458 us"}}}, "id": "HgZ0YvvZ6rbAJj12", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.126.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 458000, "runtime_str": "458 us", "start_timestamp": "23:14:19.230.230395", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 120, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059230582000, "dur": 140000, "relative_dur": 0.02364465461915217, "relative_gap_to_previous": 0.020097956426279345, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1678058059278741000, "dur": 971000, "relative_dur": 0.16399256882283397, "relative_gap_to_previous": 0.0005066711704104037, "parent_is_longest": true, "runtime_str": "971 us"}}}, "id": "Zet5TzCja0IGM3AC", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.127.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 971000, "runtime_str": "971 us", "start_timestamp": "23:14:19.230.230582", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059228580000, "dur": 2129000, "relative_dur": 0.14998238816484677, "relative_gap_to_previous": 0.007749207467418105, "parent_is_longest": true, "runtime_str": "2.13 ms"}}, "gpu3": {"time": {"ts": 1678058059273791000, "dur": 5921000, "relative_dur": 0.41711870376893273, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "5.92 ms"}}}, "id": "5N9X2u1DjW2zNCvg", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.118.pt.trace.json", "trace_disk_size": "23.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5921000, "runtime_str": "5.92 ms", "start_timestamp": "23:14:19.228.228580", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 191}, {"idx": 121, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu2": {"time": {"ts": 1678058059230727000, "dur": 224000, "relative_dur": 0.015780204297287777, "relative_gap_to_previous": 0.0003522367030644593, "parent_is_longest": true, "runtime_str": "224 us"}}, "gpu3": {"time": {"ts": 1678058059279713000, "dur": 99000, "relative_dur": 0.006974286720676294, "relative_gap_to_previous": 7.044734061289187e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "BtR63ZfQYej3FBlY", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.128.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 224000, "runtime_str": "224 us", "start_timestamp": "23:14:19.230.230727", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 122, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu2": {"time": {"ts": 1678058059230964000, "dur": 215000, "relative_dur": 0.01514617823177175, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "215 us"}}, "gpu3": {"time": {"ts": 1678058059279813000, "dur": 230000, "relative_dur": 0.01620288834096513, "relative_gap_to_previous": 7.044734061289187e-05, "parent_is_longest": true, "runtime_str": "230 us"}}}, "id": "uwvxuE599fwRdePf", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.129.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 230000, "runtime_str": "230 us", "start_timestamp": "23:14:19.230.230964", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 123, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"idx": 124, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu2": {"time": {"ts": 1678058059231305000, "dur": 213000, "relative_dur": 0.027615713730066122, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "213 us"}}, "gpu3": {"time": {"ts": 1678058059280045000, "dur": 3443000, "relative_dur": 0.4463892130169843, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3.44 ms"}}}, "id": "VEh4197jWjndc9bE", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.131.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3443000, "runtime_str": "3.44 ms", "start_timestamp": "23:14:19.231.231305", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 125, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu2": {"time": {"ts": 1678058059231651000, "dur": 49000, "relative_dur": 0.006352910670296901, "relative_gap_to_previous": 0.01724361467652016, "parent_is_longest": true, "runtime_str": "49 us"}}, "gpu3": {"time": {"ts": 1678058059283488000, "dur": 319000, "relative_dur": 0.04135874497601452, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "319 us"}}}, "id": "cwo7PMKBmOInZ8JC", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.132.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 319000, "runtime_str": "319 us", "start_timestamp": "23:14:19.231.231651", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 126, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu2": {"time": {"ts": 1678058059231823000, "dur": 64000, "relative_dur": 0.008297679242836769, "relative_gap_to_previous": 0.015947102294826914, "parent_is_longest": true, "runtime_str": "64 us"}}, "gpu3": {"time": {"ts": 1678058059283809000, "dur": 390000, "relative_dur": 0.050563982886036564, "relative_gap_to_previous": 0.000259302476338649, "parent_is_longest": true, "runtime_str": "390 us"}}}, "id": "PXAjE3T7VTRqJJIW", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.133.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 390000, "runtime_str": "390 us", "start_timestamp": "23:14:19.231.231823", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 127, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu2": {"time": {"ts": 1678058059232011000, "dur": 102000, "relative_dur": 0.013224426293271101, "relative_gap_to_previous": 0.01607675353299624, "parent_is_longest": true, "runtime_str": "102 us"}}, "gpu3": {"time": {"ts": 1678058059284201000, "dur": 3457000, "relative_dur": 0.4482043303513549, "relative_gap_to_previous": 0.000259302476338649, "parent_is_longest": true, "runtime_str": "3.46 ms"}}}, "id": "PQZ8OZNlSaB6fvyi", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.134.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3457000, "runtime_str": "3.46 ms", "start_timestamp": "23:14:19.232.232011", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 128, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu2": {"time": {"ts": 1678058059232241000, "dur": 66000, "relative_dur": 0.008556981719175419, "relative_gap_to_previous": 0.016595358485673537, "parent_is_longest": true, "runtime_str": "66 us"}}, "gpu3": {"time": {"ts": 1678058059287659000, "dur": 99000, "relative_dur": 0.012835472578763127, "relative_gap_to_previous": 0.0001296512381693245, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "sSXZTK3LLcHUeYkW", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.135.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "23:14:19.232.232241", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1678058059231305000, "dur": 1002000, "relative_dur": 0.07058823529411765, "relative_gap_to_previous": 0.00796054948925678, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1678058059280045000, "dur": 7713000, "relative_dur": 0.543360338147235, "relative_gap_to_previous": 0.00014089468122578373, "parent_is_longest": true, "runtime_str": "7.71 ms"}}}, "id": "myfZc7YesAw1ioCf", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.130.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7713000, "runtime_str": "7.71 ms", "start_timestamp": "23:14:19.231.231305", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1678058059228117000, "dur": 4177000, "relative_dur": 0.0503131775475789, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "4.18 ms"}}, "gpu3": {"time": {"ts": 1678058059273563000, "dur": 14195000, "relative_dur": 0.17098289568778607, "relative_gap_to_previous": 1.2045290291496025e-05, "parent_is_longest": true, "runtime_str": "14.2 ms"}}}, "id": "ae3ncrnLr5fbppjN", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.116.pt.trace.json", "trace_disk_size": "37.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 14195000, "runtime_str": "14.2 ms", "start_timestamp": "23:14:19.228.228117", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 314}], "resources": {"cpu2": {"time": {"ts": 1678058059204313000, "dur": 27909000, "relative_dur": 0.3282291923931835, "relative_gap_to_previous": 0.0003881028825459549, "parent_is_longest": true, "runtime_str": "27.9 ms"}}, "gpu3": {"time": {"ts": 1678058059204738000, "dur": 83020000, "relative_dur": 0.9763727669383386, "relative_gap_to_previous": 1.1760693410483483e-05, "parent_is_longest": true, "runtime_str": "83 ms"}}}, "id": "c5BRfti5GrifHzS9", "pretty_name": "TransformerEncoder", "trace_file": "/results/Transformer/Transformer.15.pt.trace.json", "trace_disk_size": "226.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 83020000, "runtime_str": "83 ms", "start_timestamp": "23:14:19.204.204313", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 1885}], "resources": {"cpu2": {"time": {"ts": 1678058059201438000, "dur": 30741000, "relative_dur": 0.12908359507533132, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "30.7 ms"}}, "gpu3": {"time": {"ts": 1678058059202729000, "dur": 85029000, "relative_dur": 0.35704267934225775, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "85 ms"}}}, "id": "sFUJW2TVyjFjWtW8", "pretty_name": "Encoder", "trace_file": "/results/Transformer/Transformer.11.pt.trace.json", "trace_disk_size": "257.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 85029000, "runtime_str": "85 ms", "start_timestamp": "23:14:19.201.201438", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 2092}, {"idx": 129, "name": "Decoder", "type": "Decoder", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 212, in forward\n    with hotline.annotate('Decoder'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 212, "ops": [{"idx": 130, "name": "Embedding", "type": "Embedding", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 301, in forward\n    with hotline.annotate('Embedding'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 301, "resources": {"cpu2": {"time": {"ts": 1678058059232322000, "dur": 1699000, "relative_dur": 0.011280042491037047, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.70 ms"}}, "gpu3": {"time": {"ts": 1678058059287759000, "dur": 2652000, "relative_dur": 0.017607223476297968, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2.65 ms"}}}, "id": "paQXbakbyjlDsAeb", "pretty_name": "Embedding", "trace_file": "/results/Transformer/Transformer.137.pt.trace.json", "trace_disk_size": "53.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 2652000, "runtime_str": "2.65 ms", "start_timestamp": "23:14:19.232.232322", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 320}, {"idx": 131, "name": "PositionalEncoding", "type": "PositionalEncoding", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 310, in forward\n    with hotline.annotate('PositionalEncoding'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 310, "ops": [{"idx": 132, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 311, in forward\n    tgt = self.pos_encoder(tgt, targets_positions, decode=decode, cache=cache) # ret float32(32, 256)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 372, in forward\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 372, "resources": {"cpu2": {"time": {"ts": 1678058059289783000, "dur": 269000, "relative_dur": 0.9471830985915493, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "269 us"}}, "gpu3": {"time": {"ts": 1678058059290412000, "dur": 284000, "relative_dur": 1.0, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "284 us"}}}, "id": "HF6UyGkSCDverzac", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.139.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 284000, "runtime_str": "284 us", "start_timestamp": "23:14:19.289.289783", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059289783000, "dur": 269000, "relative_dur": 0.0017859514008763777, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "269 us"}}, "gpu3": {"time": {"ts": 1678058059290412000, "dur": 284000, "relative_dur": 0.0018855397689549862, "relative_gap_to_previous": 6.639224538573894e-06, "parent_is_longest": true, "runtime_str": "284 us"}}}, "id": "L2EaSztV54SC3Ln5", "pretty_name": "PositionalEncoding", "trace_file": "/results/Transformer/Transformer.138.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 284000, "runtime_str": "284 us", "start_timestamp": "23:14:19.289.289783", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}, {"idx": 133, "name": "TransformerDecoder", "type": "TransformerDecoder", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 312, in forward\n    with hotline.annotate('TransformerDecoder'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 312, "ops": [{"idx": 134, "name": "Layer1", "type": "Layer1", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"idx": 135, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"idx": 136, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059290173000, "dur": 627000, "relative_dur": 0.10410094637223975, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "627 us"}}, "gpu3": {"time": {"ts": 1678058059290697000, "dur": 1078000, "relative_dur": 0.17898057446455254, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.08 ms"}}}, "id": "HMEQRm8u8bLG6u0w", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.143.pt.trace.json", "trace_disk_size": "6.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1078000, "runtime_str": "1.08 ms", "start_timestamp": "23:14:19.290.290173", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"idx": 137, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059290941000, "dur": 122000, "relative_dur": 0.02025568653494936, "relative_gap_to_previous": 0.021085837622447284, "parent_is_longest": true, "runtime_str": "122 us"}}, "gpu3": {"time": {"ts": 1678058059291776000, "dur": 970000, "relative_dur": 0.16104931097459738, "relative_gap_to_previous": 0.00016603021749958492, "parent_is_longest": true, "runtime_str": "970 us"}}}, "id": "0l5vclVkd0dExe9q", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.144.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 970000, "runtime_str": "970 us", "start_timestamp": "23:14:19.290.290941", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 138, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059291203000, "dur": 121000, "relative_dur": 0.020089656317449776, "relative_gap_to_previous": 0.0209198074049477, "parent_is_longest": true, "runtime_str": "121 us"}}, "gpu3": {"time": {"ts": 1678058059292748000, "dur": 970000, "relative_dur": 0.16104931097459738, "relative_gap_to_previous": 0.00033206043499916984, "parent_is_longest": true, "runtime_str": "970 us"}}}, "id": "jZeyL7mq16B5yCI5", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.145.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 970000, "runtime_str": "970 us", "start_timestamp": "23:14:19.291.291203", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 139, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059291341000, "dur": 222000, "relative_dur": 0.03685870828490785, "relative_gap_to_previous": 0.0004980906524987548, "parent_is_longest": true, "runtime_str": "222 us"}}, "gpu3": {"time": {"ts": 1678058059293719000, "dur": 82000, "relative_dur": 0.013614477834965964, "relative_gap_to_previous": 0.00016603021749958492, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "kjesgsiaZKy5tUOe", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.146.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 222000, "runtime_str": "222 us", "start_timestamp": "23:14:19.291.291341", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 140, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059291695000, "dur": 72000, "relative_dur": 0.011954175659970114, "relative_gap_to_previous": 0.01959156566495102, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1678058059293803000, "dur": 775000, "relative_dur": 0.12867341856217832, "relative_gap_to_previous": 0.00033206043499916984, "parent_is_longest": true, "runtime_str": "775 us"}}}, "id": "T2r6Y4DHUJsnNbOf", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.147.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 775000, "runtime_str": "775 us", "start_timestamp": "23:14:19.291.291695", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 141, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059291917000, "dur": 47000, "relative_dur": 0.007803420222480492, "relative_gap_to_previous": 0.02258010957994355, "parent_is_longest": true, "runtime_str": "47 us"}}, "gpu3": {"time": {"ts": 1678058059294579000, "dur": 321000, "relative_dur": 0.05329569981736676, "relative_gap_to_previous": 0.00016603021749958492, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "cWgBvktcE6oYVbca", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.148.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "23:14:19.291.291917", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 142, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059292097000, "dur": 62000, "relative_dur": 0.010293873484974266, "relative_gap_to_previous": 0.019757595882450605, "parent_is_longest": true, "runtime_str": "62 us"}}, "gpu3": {"time": {"ts": 1678058059294902000, "dur": 388000, "relative_dur": 0.06441972438983895, "relative_gap_to_previous": 0.00033206043499916984, "parent_is_longest": true, "runtime_str": "388 us"}}}, "id": "zZh5ZVdPKT0QBeve", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.149.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 388000, "runtime_str": "388 us", "start_timestamp": "23:14:19.292.292097", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 143, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059292290000, "dur": 57000, "relative_dur": 0.00946372239747634, "relative_gap_to_previous": 0.019425535447451438, "parent_is_longest": true, "runtime_str": "57 us"}}, "gpu3": {"time": {"ts": 1678058059295291000, "dur": 459000, "relative_dur": 0.07620786983230948, "relative_gap_to_previous": 0.00016603021749958492, "parent_is_longest": true, "runtime_str": "459 us"}}}, "id": "PfYgcqGlR2kObyCK", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.150.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 459000, "runtime_str": "459 us", "start_timestamp": "23:14:19.292.292290", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 144, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059292477000, "dur": 135000, "relative_dur": 0.022414079362443964, "relative_gap_to_previous": 0.019259505229951852, "parent_is_longest": true, "runtime_str": "135 us"}}, "gpu3": {"time": {"ts": 1678058059295751000, "dur": 969000, "relative_dur": 0.1608832807570978, "relative_gap_to_previous": 0.00016603021749958492, "parent_is_longest": true, "runtime_str": "969 us"}}}, "id": "Uyc5ChOtslTLbcqK", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.151.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 969000, "runtime_str": "969 us", "start_timestamp": "23:14:19.292.292477", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059290173000, "dur": 2425000, "relative_dur": 0.11943459416863672, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2.42 ms"}}, "gpu3": {"time": {"ts": 1678058059290697000, "dur": 6023000, "relative_dur": 0.2966410559495666, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6.02 ms"}}}, "id": "66qqm231ULM4Z0jb", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.142.pt.trace.json", "trace_disk_size": "25.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 6023000, "runtime_str": "6.02 ms", "start_timestamp": "23:14:19.290.290173", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 208}, {"idx": 145, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu2": {"time": {"ts": 1678058059292617000, "dur": 232000, "relative_dur": 0.011426319936958234, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "232 us"}}, "gpu3": {"time": {"ts": 1678058059296721000, "dur": 99000, "relative_dur": 0.004875886524822695, "relative_gap_to_previous": 4.9251379038613084e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "23G3LEnASZaUKzTq", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.152.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 232000, "runtime_str": "232 us", "start_timestamp": "23:14:19.292.292617", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 146, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu2": {"time": {"ts": 1678058059292863000, "dur": 217000, "relative_dur": 0.010687549251379038, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "217 us"}}, "gpu3": {"time": {"ts": 1678058059296821000, "dur": 233000, "relative_dur": 0.011475571315996847, "relative_gap_to_previous": 4.9251379038613084e-05, "parent_is_longest": true, "runtime_str": "233 us"}}}, "id": "Sjt08hewxl9Rk7xQ", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.153.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 233000, "runtime_str": "233 us", "start_timestamp": "23:14:19.292.292863", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 147, "name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"idx": 148, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059293202000, "dur": 302000, "relative_dur": 0.05100489782131397, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "302 us"}}, "gpu3": {"time": {"ts": 1678058059297056000, "dur": 971000, "relative_dur": 0.16399256882283397, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "971 us"}}}, "id": "dSJIV7ac44tbZaW3", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.155.pt.trace.json", "trace_disk_size": "4.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 971000, "runtime_str": "971 us", "start_timestamp": "23:14:19.293.293202", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 37}, {"idx": 149, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059293645000, "dur": 123000, "relative_dur": 0.02077351798682655, "relative_gap_to_previous": 0.021617969937510556, "parent_is_longest": true, "runtime_str": "123 us"}}, "gpu3": {"time": {"ts": 1678058059298028000, "dur": 970000, "relative_dur": 0.16382367843269718, "relative_gap_to_previous": 0.00016889039013680122, "parent_is_longest": true, "runtime_str": "970 us"}}}, "id": "z43GCnRYIzwvjTwN", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.156.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 970000, "runtime_str": "970 us", "start_timestamp": "23:14:19.293.293645", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 150, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059293907000, "dur": 122000, "relative_dur": 0.02060462759668975, "relative_gap_to_previous": 0.021280189157236954, "parent_is_longest": true, "runtime_str": "122 us"}}, "gpu3": {"time": {"ts": 1678058059299000000, "dur": 970000, "relative_dur": 0.16382367843269718, "relative_gap_to_previous": 0.00033778078027360244, "parent_is_longest": true, "runtime_str": "970 us"}}}, "id": "Gh4N2AXN61hXpysw", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.157.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 970000, "runtime_str": "970 us", "start_timestamp": "23:14:19.293.293907", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 151, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059294046000, "dur": 223000, "relative_dur": 0.03766255700050667, "relative_gap_to_previous": 0.0006755615605472049, "parent_is_longest": true, "runtime_str": "223 us"}}, "gpu3": {"time": {"ts": 1678058059299971000, "dur": 82000, "relative_dur": 0.0138490119912177, "relative_gap_to_previous": 0.00016889039013680122, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "2P99fdsEVOPdqb0E", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.158.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 223000, "runtime_str": "223 us", "start_timestamp": "23:14:19.294.294046", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 152, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059294403000, "dur": 72000, "relative_dur": 0.012160108089849688, "relative_gap_to_previous": 0.020435737206552947, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1678058059300054000, "dur": 777000, "relative_dur": 0.13122783313629455, "relative_gap_to_previous": 0.00016889039013680122, "parent_is_longest": true, "runtime_str": "777 us"}}}, "id": "8ZfsG1alSg3wfnpA", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.159.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 777000, "runtime_str": "777 us", "start_timestamp": "23:14:19.294.294403", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 153, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059294624000, "dur": 47000, "relative_dur": 0.007937848336429658, "relative_gap_to_previous": 0.022969093058604964, "parent_is_longest": true, "runtime_str": "47 us"}}, "gpu3": {"time": {"ts": 1678058059300832000, "dur": 321000, "relative_dur": 0.05421381523391319, "relative_gap_to_previous": 0.00016889039013680122, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "MzD7MPPm1C4Of2hF", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.160.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "23:14:19.294.294624", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 154, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059294802000, "dur": 62000, "relative_dur": 0.010471204188481676, "relative_gap_to_previous": 0.019929066036142543, "parent_is_longest": true, "runtime_str": "62 us"}}, "gpu3": {"time": {"ts": 1678058059301155000, "dur": 390000, "relative_dur": 0.06586725215335247, "relative_gap_to_previous": 0.00033778078027360244, "parent_is_longest": true, "runtime_str": "390 us"}}}, "id": "kcxC1lzH4ZFe5YTX", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.161.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 390000, "runtime_str": "390 us", "start_timestamp": "23:14:19.294.294802", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 155, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059294995000, "dur": 56000, "relative_dur": 0.009457861847660868, "relative_gap_to_previous": 0.019929066036142543, "parent_is_longest": true, "runtime_str": "56 us"}}, "gpu3": {"time": {"ts": 1678058059301546000, "dur": 458000, "relative_dur": 0.07735179868265496, "relative_gap_to_previous": 0.00016889039013680122, "parent_is_longest": true, "runtime_str": "458 us"}}}, "id": "uhVW9SNzKcwN2eho", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.162.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 458000, "runtime_str": "458 us", "start_timestamp": "23:14:19.294.294995", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 156, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059295179000, "dur": 135000, "relative_dur": 0.022800202668468165, "relative_gap_to_previous": 0.01942239486573214, "parent_is_longest": true, "runtime_str": "135 us"}}, "gpu3": {"time": {"ts": 1678058059302007000, "dur": 970000, "relative_dur": 0.16382367843269718, "relative_gap_to_previous": 0.0005066711704104037, "parent_is_longest": true, "runtime_str": "970 us"}}}, "id": "XYns3NtCUPEopIvu", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.163.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 970000, "runtime_str": "970 us", "start_timestamp": "23:14:19.295.295179", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059293202000, "dur": 2099000, "relative_dur": 0.10337864460204886, "relative_gap_to_previous": 0.0046788810086682425, "parent_is_longest": true, "runtime_str": "2.10 ms"}}, "gpu3": {"time": {"ts": 1678058059297056000, "dur": 5921000, "relative_dur": 0.2916174152876281, "relative_gap_to_previous": 9.850275807722617e-05, "parent_is_longest": true, "runtime_str": "5.92 ms"}}}, "id": "bzLM4z39KiGiUGgY", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.154.pt.trace.json", "trace_disk_size": "23.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5921000, "runtime_str": "5.92 ms", "start_timestamp": "23:14:19.293.293202", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 193}, {"idx": 157, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu2": {"time": {"ts": 1678058059295320000, "dur": 218000, "relative_dur": 0.010736800630417652, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "218 us"}}, "gpu3": {"time": {"ts": 1678058059302978000, "dur": 99000, "relative_dur": 0.004875886524822695, "relative_gap_to_previous": 4.9251379038613084e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "I9fwwqaMQp0pwcKf", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.164.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 218000, "runtime_str": "218 us", "start_timestamp": "23:14:19.295.295320", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 158, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu2": {"time": {"ts": 1678058059295550000, "dur": 215000, "relative_dur": 0.010589046493301812, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "215 us"}}, "gpu3": {"time": {"ts": 1678058059303079000, "dur": 228000, "relative_dur": 0.011229314420803783, "relative_gap_to_previous": 9.850275807722617e-05, "parent_is_longest": true, "runtime_str": "228 us"}}}, "id": "PPgGXiG5zs0CewXC", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.165.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 228000, "runtime_str": "228 us", "start_timestamp": "23:14:19.295.295550", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 159, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"idx": 160, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu2": {"time": {"ts": 1678058059295886000, "dur": 210000, "relative_dur": 0.027301092043681748, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "210 us"}}, "gpu3": {"time": {"ts": 1678058059303309000, "dur": 3443000, "relative_dur": 0.44760790431617264, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3.44 ms"}}}, "id": "8frotb0UjWtUQS34", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.167.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3443000, "runtime_str": "3.44 ms", "start_timestamp": "23:14:19.295.295886", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 161, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu2": {"time": {"ts": 1678058059296227000, "dur": 52000, "relative_dur": 0.006760270410816433, "relative_gap_to_previous": 0.01703068122724909, "parent_is_longest": true, "runtime_str": "52 us"}}, "gpu3": {"time": {"ts": 1678058059306753000, "dur": 320000, "relative_dur": 0.04160166406656266, "relative_gap_to_previous": 0.00013000520020800833, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "bYa9nRyXO1FZg0dh", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.168.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "23:14:19.296.296227", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 162, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu2": {"time": {"ts": 1678058059296398000, "dur": 65000, "relative_dur": 0.00845033801352054, "relative_gap_to_previous": 0.01547061882475299, "parent_is_longest": true, "runtime_str": "65 us"}}, "gpu3": {"time": {"ts": 1678058059307074000, "dur": 386000, "relative_dur": 0.050182007280291215, "relative_gap_to_previous": 0.00013000520020800833, "parent_is_longest": true, "runtime_str": "386 us"}}}, "id": "SxnF0oDZVjmauSPq", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.169.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 386000, "runtime_str": "386 us", "start_timestamp": "23:14:19.296.296398", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 163, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu2": {"time": {"ts": 1678058059296582000, "dur": 102000, "relative_dur": 0.01326053042121685, "relative_gap_to_previous": 0.01547061882475299, "parent_is_longest": true, "runtime_str": "102 us"}}, "gpu3": {"time": {"ts": 1678058059307462000, "dur": 3439000, "relative_dur": 0.4470878835153406, "relative_gap_to_previous": 0.00026001040041601667, "parent_is_longest": true, "runtime_str": "3.44 ms"}}}, "id": "BeBbV0xLC0y9NKfy", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.170.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3439000, "runtime_str": "3.44 ms", "start_timestamp": "23:14:19.296.296582", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 164, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu2": {"time": {"ts": 1678058059296809000, "dur": 68000, "relative_dur": 0.008840353614144566, "relative_gap_to_previous": 0.01625065002600104, "parent_is_longest": true, "runtime_str": "68 us"}}, "gpu3": {"time": {"ts": 1678058059310903000, "dur": 98000, "relative_dur": 0.012740509620384815, "relative_gap_to_previous": 0.00026001040041601667, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "3TxpGq7xeatMlBcq", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.171.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 98000, "runtime_str": "98 us", "start_timestamp": "23:14:19.296.296809", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1678058059295886000, "dur": 991000, "relative_dur": 0.048808116627265564, "relative_gap_to_previous": 0.004629629629629629, "parent_is_longest": true, "runtime_str": "991 us"}}, "gpu3": {"time": {"ts": 1678058059303309000, "dur": 7692000, "relative_dur": 0.37884160756501184, "relative_gap_to_previous": 9.850275807722617e-05, "parent_is_longest": true, "runtime_str": "7.69 ms"}}}, "id": "5dcyUVaWt11wzGZX", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.166.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7692000, "runtime_str": "7.69 ms", "start_timestamp": "23:14:19.295.295886", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1678058059290173000, "dur": 6677000, "relative_dur": 0.05505033432545408, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6.68 ms"}}, "gpu3": {"time": {"ts": 1678058059290697000, "dur": 20304000, "relative_dur": 0.16740182539224496, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "20.3 ms"}}}, "id": "C0P0YcKle9O9Xwiw", "pretty_name": "Layer1", "trace_file": "/results/Transformer/Transformer.141.pt.trace.json", "trace_disk_size": "64.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 20304000, "runtime_str": "20.3 ms", "start_timestamp": "23:14:19.290.290173", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 536}, {"idx": 165, "name": "Layer2", "type": "Layer2", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"idx": 166, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"idx": 167, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059296891000, "dur": 625000, "relative_dur": 0.10298236941835558, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "625 us"}}, "gpu3": {"time": {"ts": 1678058059311003000, "dur": 1181000, "relative_dur": 0.1945954852529247, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.18 ms"}}}, "id": "sbGzC7k2oqJp2thv", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.174.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1181000, "runtime_str": "1.18 ms", "start_timestamp": "23:14:19.296.296891", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 51}, {"idx": 168, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059297657000, "dur": 122000, "relative_dur": 0.020102158510463007, "relative_gap_to_previous": 0.02109078925687922, "parent_is_longest": true, "runtime_str": "122 us"}}, "gpu3": {"time": {"ts": 1678058059312185000, "dur": 953000, "relative_dur": 0.1570275168891086, "relative_gap_to_previous": 0.00016477179106936892, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "TzXzNQ0URD3gMoiG", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.175.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.297.297657", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 169, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059297918000, "dur": 122000, "relative_dur": 0.020102158510463007, "relative_gap_to_previous": 0.020761245674740483, "parent_is_longest": true, "runtime_str": "122 us"}}, "gpu3": {"time": {"ts": 1678058059313141000, "dur": 952000, "relative_dur": 0.1568627450980392, "relative_gap_to_previous": 0.0004943153732081067, "parent_is_longest": true, "runtime_str": "952 us"}}}, "id": "8344xjt13akzIvP1", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.176.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "23:14:19.297.297918", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 170, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059298057000, "dur": 219000, "relative_dur": 0.03608502224419179, "relative_gap_to_previous": 0.0006590871642774757, "parent_is_longest": true, "runtime_str": "219 us"}}, "gpu3": {"time": {"ts": 1678058059314095000, "dur": 82000, "relative_dur": 0.013511286867688252, "relative_gap_to_previous": 0.00032954358213873783, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "Z75G5LzgChF2dcHc", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.177.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 219000, "runtime_str": "219 us", "start_timestamp": "23:14:19.298.298057", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 171, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059298410000, "dur": 72000, "relative_dur": 0.011863568956994563, "relative_gap_to_previous": 0.01993738671939364, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1678058059314179000, "dur": 777000, "relative_dur": 0.12802768166089964, "relative_gap_to_previous": 0.00032954358213873783, "parent_is_longest": true, "runtime_str": "777 us"}}}, "id": "OFpp4fBx1RvrKd4C", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.178.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 777000, "runtime_str": "777 us", "start_timestamp": "23:14:19.298.298410", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 172, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059298633000, "dur": 48000, "relative_dur": 0.007909045971329708, "relative_gap_to_previous": 0.022738507167572912, "parent_is_longest": true, "runtime_str": "48 us"}}, "gpu3": {"time": {"ts": 1678058059314957000, "dur": 320000, "relative_dur": 0.052726973142198055, "relative_gap_to_previous": 0.00016477179106936892, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "goX6pYfsekWGReXl", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.179.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "23:14:19.298.298633", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 173, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059298812000, "dur": 62000, "relative_dur": 0.010215851046300873, "relative_gap_to_previous": 0.019443071346185534, "parent_is_longest": true, "runtime_str": "62 us"}}, "gpu3": {"time": {"ts": 1678058059315278000, "dur": 388000, "relative_dur": 0.06393145493491514, "relative_gap_to_previous": 0.00016477179106936892, "parent_is_longest": true, "runtime_str": "388 us"}}}, "id": "vmq2fJLFH6UNW5le", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.180.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 388000, "runtime_str": "388 us", "start_timestamp": "23:14:19.298.298812", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 174, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059299006000, "dur": 56000, "relative_dur": 0.00922722029988466, "relative_gap_to_previous": 0.0196078431372549, "parent_is_longest": true, "runtime_str": "56 us"}}, "gpu3": {"time": {"ts": 1678058059315667000, "dur": 450000, "relative_dur": 0.07414730598121602, "relative_gap_to_previous": 0.00016477179106936892, "parent_is_longest": true, "runtime_str": "450 us"}}}, "id": "wNZhl1xqlyNZ7ntq", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.181.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 450000, "runtime_str": "450 us", "start_timestamp": "23:14:19.299.299006", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 175, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059299191000, "dur": 134000, "relative_dur": 0.022079420003295436, "relative_gap_to_previous": 0.019113527764046796, "parent_is_longest": true, "runtime_str": "134 us"}}, "gpu3": {"time": {"ts": 1678058059316119000, "dur": 953000, "relative_dur": 0.1570275168891086, "relative_gap_to_previous": 0.00032954358213873783, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "b5pT9dak5wI1bIyJ", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.182.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.299.299191", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059296891000, "dur": 2421000, "relative_dur": 0.12014292094685128, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2.42 ms"}}, "gpu3": {"time": {"ts": 1678058059311003000, "dur": 6069000, "relative_dur": 0.3011761202917969, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6.07 ms"}}}, "id": "6gOwJziXQwXkdPzk", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.173.pt.trace.json", "trace_disk_size": "25.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 6069000, "runtime_str": "6.07 ms", "start_timestamp": "23:14:19.296.296891", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 207}, {"idx": 176, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu2": {"time": {"ts": 1678058059299331000, "dur": 232000, "relative_dur": 0.011513076274130316, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "232 us"}}, "gpu3": {"time": {"ts": 1678058059317073000, "dur": 98000, "relative_dur": 0.004863282219244702, "relative_gap_to_previous": 4.962532876780309e-05, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "Uyz7OGpUiKl5NCtQ", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.183.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 232000, "runtime_str": "232 us", "start_timestamp": "23:14:19.299.299331", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 177, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu2": {"time": {"ts": 1678058059299576000, "dur": 212000, "relative_dur": 0.010520569698774254, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "212 us"}}, "gpu3": {"time": {"ts": 1678058059317173000, "dur": 230000, "relative_dur": 0.01141382561659471, "relative_gap_to_previous": 9.925065753560618e-05, "parent_is_longest": true, "runtime_str": "230 us"}}}, "id": "eZ43DdVyoIHtkpaD", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.184.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 230000, "runtime_str": "230 us", "start_timestamp": "23:14:19.299.299576", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 178, "name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"idx": 179, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059299909000, "dur": 301000, "relative_dur": 0.051549922932008906, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "301 us"}}, "gpu3": {"time": {"ts": 1678058059317404000, "dur": 952000, "relative_dur": 0.16304161671519096, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "952 us"}}}, "id": "Ew2iJuFfN6F6e9vn", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.186.pt.trace.json", "trace_disk_size": "4.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "23:14:19.299.299909", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 37}, {"idx": 180, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059300352000, "dur": 123000, "relative_dur": 0.021065250899126562, "relative_gap_to_previous": 0.0220928241137181, "parent_is_longest": true, "runtime_str": "123 us"}}, "gpu3": {"time": {"ts": 1678058059318358000, "dur": 953000, "relative_dur": 0.16321287891762287, "relative_gap_to_previous": 0.0003425244048638465, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "PZ835HbXpDKrWSj7", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.187.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.300.300352", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 181, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059300615000, "dur": 121000, "relative_dur": 0.020722726494262718, "relative_gap_to_previous": 0.021750299708854257, "parent_is_longest": true, "runtime_str": "121 us"}}, "gpu3": {"time": {"ts": 1678058059319312000, "dur": 953000, "relative_dur": 0.16321287891762287, "relative_gap_to_previous": 0.00017126220243192326, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "IYsgP02Umf9NVQN4", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.188.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.300.300615", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 182, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059300753000, "dur": 220000, "relative_dur": 0.03767768453502312, "relative_gap_to_previous": 0.000685048809727693, "parent_is_longest": true, "runtime_str": "220 us"}}, "gpu3": {"time": {"ts": 1678058059320266000, "dur": 82000, "relative_dur": 0.014043500599417709, "relative_gap_to_previous": 0.00017126220243192326, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "Vaug7lDMdqTB7FsG", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.189.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 220000, "runtime_str": "220 us", "start_timestamp": "23:14:19.300.300753", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 183, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059301104000, "dur": 72000, "relative_dur": 0.012330878575098476, "relative_gap_to_previous": 0.020208939886966946, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1678058059320349000, "dur": 777000, "relative_dur": 0.1330707312896044, "relative_gap_to_previous": 0.00017126220243192326, "parent_is_longest": true, "runtime_str": "777 us"}}}, "id": "8NbSujMikhOROjNe", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.190.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 777000, "runtime_str": "777 us", "start_timestamp": "23:14:19.301.301104", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 184, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059301326000, "dur": 47000, "relative_dur": 0.008049323514300395, "relative_gap_to_previous": 0.02346292173317349, "parent_is_longest": true, "runtime_str": "47 us"}}, "gpu3": {"time": {"ts": 1678058059321128000, "dur": 320000, "relative_dur": 0.05480390477821545, "relative_gap_to_previous": 0.0003425244048638465, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "don9c1FcF55AFPV7", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.191.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "23:14:19.301.301326", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 185, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059301504000, "dur": 61000, "relative_dur": 0.010446994348347319, "relative_gap_to_previous": 0.020208939886966946, "parent_is_longest": true, "runtime_str": "61 us"}}, "gpu3": {"time": {"ts": 1678058059321449000, "dur": 387000, "relative_dur": 0.0662784723411543, "relative_gap_to_previous": 0.00017126220243192326, "parent_is_longest": true, "runtime_str": "387 us"}}}, "id": "IlYC1ziS2VCdePQw", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.192.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 387000, "runtime_str": "387 us", "start_timestamp": "23:14:19.301.301504", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 186, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059301700000, "dur": 56000, "relative_dur": 0.009590683336187704, "relative_gap_to_previous": 0.020893988696694638, "parent_is_longest": true, "runtime_str": "56 us"}}, "gpu3": {"time": {"ts": 1678058059321838000, "dur": 450000, "relative_dur": 0.07706799109436548, "relative_gap_to_previous": 0.0003425244048638465, "parent_is_longest": true, "runtime_str": "450 us"}}}, "id": "lSf4vGoBy6iQBU0G", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.193.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 450000, "runtime_str": "450 us", "start_timestamp": "23:14:19.301.301700", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 187, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059301885000, "dur": 139000, "relative_dur": 0.023805446138037336, "relative_gap_to_previous": 0.0198664154821031, "parent_is_longest": true, "runtime_str": "139 us"}}, "gpu3": {"time": {"ts": 1678058059322290000, "dur": 953000, "relative_dur": 0.16321287891762287, "relative_gap_to_previous": 0.0003425244048638465, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "vbVE4uR3XDzXNaGY", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.194.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.301.301885", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059299909000, "dur": 2102000, "relative_dur": 0.10431244106992209, "relative_gap_to_previous": 0.004714406232941293, "parent_is_longest": true, "runtime_str": "2.10 ms"}}, "gpu3": {"time": {"ts": 1678058059317404000, "dur": 5839000, "relative_dur": 0.2897622946752022, "relative_gap_to_previous": 4.962532876780309e-05, "parent_is_longest": true, "runtime_str": "5.84 ms"}}}, "id": "FIIZuNcq20RfBeba", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.185.pt.trace.json", "trace_disk_size": "23.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5839000, "runtime_str": "5.84 ms", "start_timestamp": "23:14:19.299.299909", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 193}, {"idx": 188, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu2": {"time": {"ts": 1678058059302029000, "dur": 220000, "relative_dur": 0.010917572328916679, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "220 us"}}, "gpu3": {"time": {"ts": 1678058059323244000, "dur": 98000, "relative_dur": 0.004863282219244702, "relative_gap_to_previous": 4.962532876780309e-05, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "T6NOTizbYib8qr01", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.195.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 220000, "runtime_str": "220 us", "start_timestamp": "23:14:19.302.302029", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 189, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu2": {"time": {"ts": 1678058059302262000, "dur": 215000, "relative_dur": 0.010669445685077664, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "215 us"}}, "gpu3": {"time": {"ts": 1678058059323343000, "dur": 233000, "relative_dur": 0.011562701602898118, "relative_gap_to_previous": 4.962532876780309e-05, "parent_is_longest": true, "runtime_str": "233 us"}}}, "id": "Evb7RRmH75E71rmO", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.196.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 233000, "runtime_str": "233 us", "start_timestamp": "23:14:19.302.302262", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 190, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"idx": 191, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu2": {"time": {"ts": 1678058059302600000, "dur": 216000, "relative_dur": 0.028507324798733007, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "216 us"}}, "gpu3": {"time": {"ts": 1678058059323577000, "dur": 3376000, "relative_dur": 0.44555892833575295, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3.38 ms"}}}, "id": "0BbYLAq2sW9lep1I", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.198.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3376000, "runtime_str": "3.38 ms", "start_timestamp": "23:14:19.302.302600", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 192, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu2": {"time": {"ts": 1678058059302943000, "dur": 51000, "relative_dur": 0.006730896133034183, "relative_gap_to_previous": 0.01676125115481061, "parent_is_longest": true, "runtime_str": "51 us"}}, "gpu3": {"time": {"ts": 1678058059326955000, "dur": 320000, "relative_dur": 0.04223307377590075, "relative_gap_to_previous": 0.0002639567110993797, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "ONRapxs2ZACNZ5ul", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.199.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "23:14:19.302.302943", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 193, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu2": {"time": {"ts": 1678058059303116000, "dur": 66000, "relative_dur": 0.008710571466279531, "relative_gap_to_previous": 0.016101359377062162, "parent_is_longest": true, "runtime_str": "66 us"}}, "gpu3": {"time": {"ts": 1678058059327277000, "dur": 386000, "relative_dur": 0.050943645242180285, "relative_gap_to_previous": 0.0002639567110993797, "parent_is_longest": true, "runtime_str": "386 us"}}}, "id": "ELMfTDOLqYZ55SqL", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.200.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 386000, "runtime_str": "386 us", "start_timestamp": "23:14:19.303.303116", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 194, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu2": {"time": {"ts": 1678058059303303000, "dur": 104000, "relative_dur": 0.013725748977167744, "relative_gap_to_previous": 0.015969381021512474, "parent_is_longest": true, "runtime_str": "104 us"}}, "gpu3": {"time": {"ts": 1678058059327665000, "dur": 3389000, "relative_dur": 0.4472746469578989, "relative_gap_to_previous": 0.0002639567110993797, "parent_is_longest": true, "runtime_str": "3.39 ms"}}}, "id": "W8iujXzu42UKoE2z", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.201.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3389000, "runtime_str": "3.39 ms", "start_timestamp": "23:14:19.303.303303", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 195, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu2": {"time": {"ts": 1678058059303533000, "dur": 67000, "relative_dur": 0.00884254982182922, "relative_gap_to_previous": 0.016629272799260922, "parent_is_longest": true, "runtime_str": "67 us"}}, "gpu3": {"time": {"ts": 1678058059331056000, "dur": 98000, "relative_dur": 0.012933878843869605, "relative_gap_to_previous": 0.0002639567110993797, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "Wcnhkb5stZorytbf", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.202.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 98000, "runtime_str": "98 us", "start_timestamp": "23:14:19.303.303533", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1678058059302600000, "dur": 1000000, "relative_dur": 0.04962532876780309, "relative_gap_to_previous": 0.004813656890476899, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1678058059323577000, "dur": 7577000, "relative_dur": 0.376011116073644, "relative_gap_to_previous": 4.962532876780309e-05, "parent_is_longest": true, "runtime_str": "7.58 ms"}}}, "id": "k9x2hovUrtuFkS6k", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.197.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7577000, "runtime_str": "7.58 ms", "start_timestamp": "23:14:19.302.302600", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1678058059296891000, "dur": 6683000, "relative_dur": 0.055099802949978974, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6.68 ms"}}, "gpu3": {"time": {"ts": 1678058059311003000, "dur": 20151000, "relative_dur": 0.16614037546686014, "relative_gap_to_previous": 1.648954150829836e-05, "parent_is_longest": true, "runtime_str": "20.1 ms"}}}, "id": "Nh0JzFgTShj4D9Hk", "pretty_name": "Layer2", "trace_file": "/results/Transformer/Transformer.172.pt.trace.json", "trace_disk_size": "64.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 20151000, "runtime_str": "20.1 ms", "start_timestamp": "23:14:19.296.296891", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 535}, {"idx": 196, "name": "Layer3", "type": "Layer3", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"idx": 197, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"idx": 198, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059303615000, "dur": 633000, "relative_dur": 0.10430054374691053, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "633 us"}}, "gpu3": {"time": {"ts": 1678058059331156000, "dur": 1182000, "relative_dur": 0.19476025704399408, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.18 ms"}}}, "id": "Kg1KljOcpNB6yxkl", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.205.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1182000, "runtime_str": "1.18 ms", "start_timestamp": "23:14:19.303.303615", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 51}, {"idx": 199, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059304389000, "dur": 124000, "relative_dur": 0.020431702092601745, "relative_gap_to_previous": 0.02109078925687922, "parent_is_longest": true, "runtime_str": "124 us"}}, "gpu3": {"time": {"ts": 1678058059332340000, "dur": 953000, "relative_dur": 0.1570275168891086, "relative_gap_to_previous": 0.00032954358213873783, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "S4wePY1saxH52KcR", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.206.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.304.304389", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 200, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059304658000, "dur": 123000, "relative_dur": 0.020266930301532378, "relative_gap_to_previous": 0.021749876421156698, "parent_is_longest": true, "runtime_str": "123 us"}}, "gpu3": {"time": {"ts": 1678058059333294000, "dur": 953000, "relative_dur": 0.1570275168891086, "relative_gap_to_previous": 0.00016477179106936892, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "dFnhp1QlH8MVQihI", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.207.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.304.304658", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 201, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059304799000, "dur": 222000, "relative_dur": 0.0365793376173999, "relative_gap_to_previous": 0.0008238589553468446, "parent_is_longest": true, "runtime_str": "222 us"}}, "gpu3": {"time": {"ts": 1678058059334248000, "dur": 81000, "relative_dur": 0.013346515076618883, "relative_gap_to_previous": 0.00016477179106936892, "parent_is_longest": true, "runtime_str": "81 us"}}}, "id": "rHGg91wXhOcF7Rgu", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.208.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 222000, "runtime_str": "222 us", "start_timestamp": "23:14:19.304.304799", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 202, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059305153000, "dur": 74000, "relative_dur": 0.012193112539133301, "relative_gap_to_previous": 0.0196078431372549, "parent_is_longest": true, "runtime_str": "74 us"}}, "gpu3": {"time": {"ts": 1678058059334331000, "dur": 777000, "relative_dur": 0.12802768166089964, "relative_gap_to_previous": 0.00032954358213873783, "parent_is_longest": true, "runtime_str": "777 us"}}}, "id": "kl7hMTNw2iyl1oBG", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.209.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 777000, "runtime_str": "777 us", "start_timestamp": "23:14:19.305.305153", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 203, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059305378000, "dur": 49000, "relative_dur": 0.008073817762399077, "relative_gap_to_previous": 0.022738507167572912, "parent_is_longest": true, "runtime_str": "49 us"}}, "gpu3": {"time": {"ts": 1678058059335110000, "dur": 321000, "relative_dur": 0.052891744933267426, "relative_gap_to_previous": 0.00032954358213873783, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "qx62hjMsPJXEGQSR", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.210.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "23:14:19.305.305378", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 204, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059305561000, "dur": 61000, "relative_dur": 0.010051079255231504, "relative_gap_to_previous": 0.01993738671939364, "parent_is_longest": true, "runtime_str": "61 us"}}, "gpu3": {"time": {"ts": 1678058059335432000, "dur": 387000, "relative_dur": 0.06376668314384577, "relative_gap_to_previous": 0.00016477179106936892, "parent_is_longest": true, "runtime_str": "387 us"}}}, "id": "5AD1vjPNqW8mSJBc", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.211.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 387000, "runtime_str": "387 us", "start_timestamp": "23:14:19.305.305561", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 205, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059305755000, "dur": 57000, "relative_dur": 0.009391992090954029, "relative_gap_to_previous": 0.019772614928324272, "parent_is_longest": true, "runtime_str": "57 us"}}, "gpu3": {"time": {"ts": 1678058059335820000, "dur": 451000, "relative_dur": 0.07431207777228538, "relative_gap_to_previous": 0.00016477179106936892, "parent_is_longest": true, "runtime_str": "451 us"}}}, "id": "70hzx5E8uzeegpJA", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.212.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 451000, "runtime_str": "451 us", "start_timestamp": "23:14:19.305.305755", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 206, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059305942000, "dur": 136000, "relative_dur": 0.022408963585434174, "relative_gap_to_previous": 0.019278299555116164, "parent_is_longest": true, "runtime_str": "136 us"}}, "gpu3": {"time": {"ts": 1678058059336272000, "dur": 953000, "relative_dur": 0.1570275168891086, "relative_gap_to_previous": 0.00016477179106936892, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "O5fDaa5exB5B8lYS", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.213.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.305.305942", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059303615000, "dur": 2450000, "relative_dur": 0.12163034304721243, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2.45 ms"}}, "gpu3": {"time": {"ts": 1678058059331156000, "dur": 6069000, "relative_dur": 0.30129573549123767, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6.07 ms"}}}, "id": "AJp7wleQCfDlHoDw", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.204.pt.trace.json", "trace_disk_size": "25.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 6069000, "runtime_str": "6.07 ms", "start_timestamp": "23:14:19.303.303615", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 207}, {"idx": 207, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu2": {"time": {"ts": 1678058059306083000, "dur": 228000, "relative_dur": 0.011319068659087524, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "228 us"}}, "gpu3": {"time": {"ts": 1678058059337226000, "dur": 98000, "relative_dur": 0.004865213721888497, "relative_gap_to_previous": 4.964503797845405e-05, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "g5F932RpX3WikN3O", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.214.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 228000, "runtime_str": "228 us", "start_timestamp": "23:14:19.306.306083", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 208, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu2": {"time": {"ts": 1678058059306325000, "dur": 215000, "relative_dur": 0.010673683165367622, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "215 us"}}, "gpu3": {"time": {"ts": 1678058059337326000, "dur": 228000, "relative_dur": 0.011319068659087524, "relative_gap_to_previous": 9.92900759569081e-05, "parent_is_longest": true, "runtime_str": "228 us"}}}, "id": "jPfXsp0zuushjA0F", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.215.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 228000, "runtime_str": "228 us", "start_timestamp": "23:14:19.306.306325", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 209, "name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"idx": 210, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059306662000, "dur": 301000, "relative_dur": 0.051594103531025025, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "301 us"}}, "gpu3": {"time": {"ts": 1678058059337556000, "dur": 952000, "relative_dur": 0.1631813507027768, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "952 us"}}}, "id": "U49W2qqbnFqLaPGC", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.217.pt.trace.json", "trace_disk_size": "4.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "23:14:19.306.306662", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 37}, {"idx": 211, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059307105000, "dur": 124000, "relative_dur": 0.02125471374700034, "relative_gap_to_previous": 0.02211175865615358, "parent_is_longest": true, "runtime_str": "124 us"}}, "gpu3": {"time": {"ts": 1678058059338511000, "dur": 953000, "relative_dur": 0.16335275968460747, "relative_gap_to_previous": 0.0005142269454919438, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "FBckL2XyPo0JZDnq", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.218.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.307.307105", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 212, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059307370000, "dur": 123000, "relative_dur": 0.021083304765169696, "relative_gap_to_previous": 0.021940349674322936, "parent_is_longest": true, "runtime_str": "123 us"}}, "gpu3": {"time": {"ts": 1678058059339466000, "dur": 952000, "relative_dur": 0.1631813507027768, "relative_gap_to_previous": 0.00034281796366129587, "parent_is_longest": true, "runtime_str": "952 us"}}}, "id": "4TaMPGcZHz1K0P4j", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.219.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "23:14:19.307.307370", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 213, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059307510000, "dur": 223000, "relative_dur": 0.03822420294823449, "relative_gap_to_previous": 0.0006856359273225917, "parent_is_longest": true, "runtime_str": "223 us"}}, "gpu3": {"time": {"ts": 1678058059340420000, "dur": 82000, "relative_dur": 0.01405553651011313, "relative_gap_to_previous": 0.00034281796366129587, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "e0ZxvC1nvVefBR11", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.220.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 223000, "runtime_str": "223 us", "start_timestamp": "23:14:19.307.307510", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 214, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059307867000, "dur": 73000, "relative_dur": 0.012512855673637299, "relative_gap_to_previous": 0.0207404868015084, "parent_is_longest": true, "runtime_str": "73 us"}}, "gpu3": {"time": {"ts": 1678058059340503000, "dur": 772000, "relative_dur": 0.1323277339732602, "relative_gap_to_previous": 0.00017140898183064793, "parent_is_longest": true, "runtime_str": "772 us"}}}, "id": "Phx8VKIax0DoxVmy", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.221.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 772000, "runtime_str": "772 us", "start_timestamp": "23:14:19.307.307867", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 215, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059308090000, "dur": 48000, "relative_dur": 0.0082276311278711, "relative_gap_to_previous": 0.023483030510798766, "parent_is_longest": true, "runtime_str": "48 us"}}, "gpu3": {"time": {"ts": 1678058059341276000, "dur": 321000, "relative_dur": 0.055022283167637985, "relative_gap_to_previous": 0.00017140898183064793, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "HFCKGQkI5iKSXNG6", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.222.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "23:14:19.308.308090", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 216, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059308272000, "dur": 63000, "relative_dur": 0.01079876585533082, "relative_gap_to_previous": 0.0207404868015084, "parent_is_longest": true, "runtime_str": "63 us"}}, "gpu3": {"time": {"ts": 1678058059341598000, "dur": 386000, "relative_dur": 0.0661638669866301, "relative_gap_to_previous": 0.00017140898183064793, "parent_is_longest": true, "runtime_str": "386 us"}}}, "id": "bcA3Tjy8wXzFArv1", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.223.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 386000, "runtime_str": "386 us", "start_timestamp": "23:14:19.308.308272", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 217, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059308467000, "dur": 57000, "relative_dur": 0.009770311964346932, "relative_gap_to_previous": 0.0203976688378471, "parent_is_longest": true, "runtime_str": "57 us"}}, "gpu3": {"time": {"ts": 1678058059341986000, "dur": 451000, "relative_dur": 0.07730545080562222, "relative_gap_to_previous": 0.00034281796366129587, "parent_is_longest": true, "runtime_str": "451 us"}}}, "id": "qVrup4TNIvoLoBwX", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.224.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 451000, "runtime_str": "451 us", "start_timestamp": "23:14:19.308.308467", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 218, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059308655000, "dur": 137000, "relative_dur": 0.023483030510798766, "relative_gap_to_previous": 0.020226259856016456, "parent_is_longest": true, "runtime_str": "137 us"}}, "gpu3": {"time": {"ts": 1678058059342438000, "dur": 952000, "relative_dur": 0.1631813507027768, "relative_gap_to_previous": 0.00017140898183064793, "parent_is_longest": true, "runtime_str": "952 us"}}}, "id": "n2yiZJgE0MG65GUT", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.225.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "23:14:19.308.308655", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059306662000, "dur": 2117000, "relative_dur": 0.10509854540038722, "relative_gap_to_previous": 0.0047659236459315895, "parent_is_longest": true, "runtime_str": "2.12 ms"}}, "gpu3": {"time": {"ts": 1678058059337556000, "dur": 5834000, "relative_dur": 0.2896291515663009, "relative_gap_to_previous": 9.92900759569081e-05, "parent_is_longest": true, "runtime_str": "5.83 ms"}}}, "id": "58viwKqPKVkgQMWM", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.216.pt.trace.json", "trace_disk_size": "23.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5834000, "runtime_str": "5.83 ms", "start_timestamp": "23:14:19.306.306662", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 193}, {"idx": 219, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu2": {"time": {"ts": 1678058059308797000, "dur": 222000, "relative_dur": 0.0110211984312168, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "222 us"}}, "gpu3": {"time": {"ts": 1678058059343392000, "dur": 99000, "relative_dur": 0.004914858759866952, "relative_gap_to_previous": 9.92900759569081e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "rxwPzMXi4j1o4R5J", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.226.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 222000, "runtime_str": "222 us", "start_timestamp": "23:14:19.308.308797", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 220, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu2": {"time": {"ts": 1678058059309031000, "dur": 219000, "relative_dur": 0.010872263317281437, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "219 us"}}, "gpu3": {"time": {"ts": 1678058059343492000, "dur": 226000, "relative_dur": 0.011219778583130616, "relative_gap_to_previous": 4.964503797845405e-05, "parent_is_longest": true, "runtime_str": "226 us"}}}, "id": "nHWBsxVsENI3DcQZ", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.227.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 226000, "runtime_str": "226 us", "start_timestamp": "23:14:19.309.309031", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 221, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"idx": 222, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu2": {"time": {"ts": 1678058059309372000, "dur": 231000, "relative_dur": 0.030478955007256895, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "231 us"}}, "gpu3": {"time": {"ts": 1678058059343720000, "dur": 3377000, "relative_dur": 0.4455732946298984, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3.38 ms"}}}, "id": "0iemhu42m6U2mlHR", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.229.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3377000, "runtime_str": "3.38 ms", "start_timestamp": "23:14:19.309.309372", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 223, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu2": {"time": {"ts": 1678058059309729000, "dur": 52000, "relative_dur": 0.00686106346483705, "relative_gap_to_previous": 0.01662488454941285, "parent_is_longest": true, "runtime_str": "52 us"}}, "gpu3": {"time": {"ts": 1678058059347098000, "dur": 320000, "relative_dur": 0.04222192901438184, "relative_gap_to_previous": 0.00013194352816994325, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "y1XVJl3IbAD7R7vv", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.230.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "23:14:19.309.309729", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 224, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu2": {"time": {"ts": 1678058059309903000, "dur": 67000, "relative_dur": 0.0088402163873862, "relative_gap_to_previous": 0.016097110436733078, "parent_is_longest": true, "runtime_str": "67 us"}}, "gpu3": {"time": {"ts": 1678058059347419000, "dur": 388000, "relative_dur": 0.051194088929937986, "relative_gap_to_previous": 0.00013194352816994325, "parent_is_longest": true, "runtime_str": "388 us"}}}, "id": "8lGe8PuWsWAbI3jq", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.231.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 388000, "runtime_str": "388 us", "start_timestamp": "23:14:19.309.309903", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 225, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu2": {"time": {"ts": 1678058059310093000, "dur": 105000, "relative_dur": 0.013854070457844044, "relative_gap_to_previous": 0.016229053964903022, "parent_is_longest": true, "runtime_str": "105 us"}}, "gpu3": {"time": {"ts": 1678058059347809000, "dur": 3390000, "relative_dur": 0.44728856049610766, "relative_gap_to_previous": 0.0002638870563398865, "parent_is_longest": true, "runtime_str": "3.39 ms"}}}, "id": "XWAHnLr3tKdwNdhz", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.232.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3390000, "runtime_str": "3.39 ms", "start_timestamp": "23:14:19.310.310093", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 226, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu2": {"time": {"ts": 1678058059310324000, "dur": 68000, "relative_dur": 0.008972159915556142, "relative_gap_to_previous": 0.01662488454941285, "parent_is_longest": true, "runtime_str": "68 us"}}, "gpu3": {"time": {"ts": 1678058059351201000, "dur": 98000, "relative_dur": 0.012930465760654439, "relative_gap_to_previous": 0.0002638870563398865, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "FaHRkMeZrneQdeXs", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.233.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 98000, "runtime_str": "98 us", "start_timestamp": "23:14:19.310.310324", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1678058059309372000, "dur": 1020000, "relative_dur": 0.050637938738023136, "relative_gap_to_previous": 0.0047659236459315895, "parent_is_longest": true, "runtime_str": "1.02 ms"}}, "gpu3": {"time": {"ts": 1678058059343720000, "dur": 7579000, "relative_dur": 0.3762597428387033, "relative_gap_to_previous": 9.92900759569081e-05, "parent_is_longest": true, "runtime_str": "7.58 ms"}}}, "id": "AH2jRJx97O1hua4H", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.228.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7579000, "runtime_str": "7.58 ms", "start_timestamp": "23:14:19.309.309372", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1678058059303615000, "dur": 6751000, "relative_dur": 0.05566044736126112, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6.75 ms"}}, "gpu3": {"time": {"ts": 1678058059331156000, "dur": 20143000, "relative_dur": 0.16607441730082695, "relative_gap_to_previous": 1.648954150829836e-05, "parent_is_longest": true, "runtime_str": "20.1 ms"}}}, "id": "sA44FdKCXZW7hlxj", "pretty_name": "Layer3", "trace_file": "/results/Transformer/Transformer.203.pt.trace.json", "trace_disk_size": "64.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 20143000, "runtime_str": "20.1 ms", "start_timestamp": "23:14:19.303.303615", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 535}, {"idx": 227, "name": "Layer4", "type": "Layer4", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"idx": 228, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"idx": 229, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059310406000, "dur": 631000, "relative_dur": 0.10393674847636304, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "631 us"}}, "gpu3": {"time": {"ts": 1678058059351300000, "dur": 1184000, "relative_dur": 0.19502553121396804, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.18 ms"}}}, "id": "6fVuSQxY21PuiD5c", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.236.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1184000, "runtime_str": "1.18 ms", "start_timestamp": "23:14:19.310.310406", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 51}, {"idx": 230, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059311179000, "dur": 125000, "relative_dur": 0.0205896886839071, "relative_gap_to_previous": 0.021248558721792127, "parent_is_longest": true, "runtime_str": "125 us"}}, "gpu3": {"time": {"ts": 1678058059352487000, "dur": 953000, "relative_dur": 0.15697578652610772, "relative_gap_to_previous": 0.0004941525284137704, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "S9tge6Jx5KDxhDmF", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.237.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.311.311179", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 231, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059311443000, "dur": 125000, "relative_dur": 0.0205896886839071, "relative_gap_to_previous": 0.020754406193378355, "parent_is_longest": true, "runtime_str": "125 us"}}, "gpu3": {"time": {"ts": 1678058059353441000, "dur": 953000, "relative_dur": 0.15697578652610772, "relative_gap_to_previous": 0.00016471750947125678, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "l618GSbqw9WSg23Q", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.238.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.311.311443", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 232, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059311585000, "dur": 222000, "relative_dur": 0.03656728710261901, "relative_gap_to_previous": 0.0006588700378850271, "parent_is_longest": true, "runtime_str": "222 us"}}, "gpu3": {"time": {"ts": 1678058059354396000, "dur": 82000, "relative_dur": 0.013506835776643057, "relative_gap_to_previous": 0.00032943501894251357, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "rpwmZpDDNWAgVUAT", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.239.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 222000, "runtime_str": "222 us", "start_timestamp": "23:14:19.311.311585", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 233, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059311941000, "dur": 74000, "relative_dur": 0.012189095700873002, "relative_gap_to_previous": 0.019930818646022072, "parent_is_longest": true, "runtime_str": "74 us"}}, "gpu3": {"time": {"ts": 1678058059354479000, "dur": 776000, "relative_dur": 0.12782078734969526, "relative_gap_to_previous": 0.00016471750947125678, "parent_is_longest": true, "runtime_str": "776 us"}}}, "id": "aK1tae8hl7RY7vRo", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.240.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 776000, "runtime_str": "776 us", "start_timestamp": "23:14:19.311.311941", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 234, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059312166000, "dur": 47000, "relative_dur": 0.00774172294514907, "relative_gap_to_previous": 0.022731016307033437, "parent_is_longest": true, "runtime_str": "47 us"}}, "gpu3": {"time": {"ts": 1678058059355256000, "dur": 321000, "relative_dur": 0.05287432054027343, "relative_gap_to_previous": 0.00016471750947125678, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "uCu6Z0u4rmqNkweI", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.241.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "23:14:19.312.312166", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 235, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059312345000, "dur": 61000, "relative_dur": 0.010047768077746665, "relative_gap_to_previous": 0.01960138362707956, "parent_is_longest": true, "runtime_str": "61 us"}}, "gpu3": {"time": {"ts": 1678058059355579000, "dur": 387000, "relative_dur": 0.06374567616537638, "relative_gap_to_previous": 0.00032943501894251357, "parent_is_longest": true, "runtime_str": "387 us"}}}, "id": "lBEUFd4uQIAe84UO", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.242.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 387000, "runtime_str": "387 us", "start_timestamp": "23:14:19.312.312345", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 236, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059312537000, "dur": 57000, "relative_dur": 0.009388898039861638, "relative_gap_to_previous": 0.019436666117608303, "parent_is_longest": true, "runtime_str": "57 us"}}, "gpu3": {"time": {"ts": 1678058059355967000, "dur": 450000, "relative_dur": 0.07412287926206555, "relative_gap_to_previous": 0.00016471750947125678, "parent_is_longest": true, "runtime_str": "450 us"}}}, "id": "95CxXhYJH4elJhXI", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.243.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 450000, "runtime_str": "450 us", "start_timestamp": "23:14:19.312.312537", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 237, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059312724000, "dur": 146000, "relative_dur": 0.02404875638280349, "relative_gap_to_previous": 0.019271948608137045, "parent_is_longest": true, "runtime_str": "146 us"}}, "gpu3": {"time": {"ts": 1678058059356419000, "dur": 952000, "relative_dur": 0.15681106901663647, "relative_gap_to_previous": 0.00032943501894251357, "parent_is_longest": true, "runtime_str": "952 us"}}}, "id": "DlQVB6r75MsG9Yvh", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.244.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "23:14:19.312.312724", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059310406000, "dur": 2451000, "relative_dur": 0.12160754155296452, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2.45 ms"}}, "gpu3": {"time": {"ts": 1678058059351300000, "dur": 6071000, "relative_dur": 0.30121557926072934, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6.07 ms"}}}, "id": "UBuG13APpsrIbrm8", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.235.pt.trace.json", "trace_disk_size": "25.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 6071000, "runtime_str": "6.07 ms", "start_timestamp": "23:14:19.310.310406", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 207}, {"idx": 238, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu2": {"time": {"ts": 1678058059312876000, "dur": 221000, "relative_dur": 0.010965021086579013, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "221 us"}}, "gpu3": {"time": {"ts": 1678058059357372000, "dur": 99000, "relative_dur": 0.0049119325229471595, "relative_gap_to_previous": 4.961548002976929e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "6SJUA1FhWDbxLufj", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.245.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 221000, "runtime_str": "221 us", "start_timestamp": "23:14:19.312.312876", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 239, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu2": {"time": {"ts": 1678058059313110000, "dur": 212000, "relative_dur": 0.010518481766311089, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "212 us"}}, "gpu3": {"time": {"ts": 1678058059357473000, "dur": 230000, "relative_dur": 0.011411560406846936, "relative_gap_to_previous": 9.923096005953858e-05, "parent_is_longest": true, "runtime_str": "230 us"}}}, "id": "MTWIZ2djfrmSRY9w", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.246.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 230000, "runtime_str": "230 us", "start_timestamp": "23:14:19.313.313110", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 240, "name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"idx": 241, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059313445000, "dur": 301000, "relative_dur": 0.05151463289406127, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "301 us"}}, "gpu3": {"time": {"ts": 1678058059357704000, "dur": 952000, "relative_dur": 0.16293000171144958, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "952 us"}}}, "id": "CXFjpKYryxgWJz1S", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.248.pt.trace.json", "trace_disk_size": "4.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "23:14:19.313.313445", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 37}, {"idx": 242, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059313888000, "dur": 123000, "relative_dur": 0.02105083005305494, "relative_gap_to_previous": 0.022077699811740546, "parent_is_longest": true, "runtime_str": "123 us"}}, "gpu3": {"time": {"ts": 1678058059358658000, "dur": 952000, "relative_dur": 0.16293000171144958, "relative_gap_to_previous": 0.0003422899195618689, "parent_is_longest": true, "runtime_str": "952 us"}}}, "id": "sWDPllUJuSaNaYxl", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.249.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "23:14:19.313.313888", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 243, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059314151000, "dur": 122000, "relative_dur": 0.020879685093274, "relative_gap_to_previous": 0.021735409892178675, "parent_is_longest": true, "runtime_str": "122 us"}}, "gpu3": {"time": {"ts": 1678058059359611000, "dur": 954000, "relative_dur": 0.16327229163101148, "relative_gap_to_previous": 0.00017114495978093444, "parent_is_longest": true, "runtime_str": "954 us"}}}, "id": "EuiGbarNEzOPqwat", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.250.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 954000, "runtime_str": "954 us", "start_timestamp": "23:14:19.314.314151", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 244, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059314291000, "dur": 220000, "relative_dur": 0.03765189115180558, "relative_gap_to_previous": 0.0008557247989046722, "parent_is_longest": true, "runtime_str": "220 us"}}, "gpu3": {"time": {"ts": 1678058059360566000, "dur": 82000, "relative_dur": 0.014033886702036626, "relative_gap_to_previous": 0.00017114495978093444, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "6AVpOAHoUmSgnu8Q", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.251.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 220000, "runtime_str": "220 us", "start_timestamp": "23:14:19.314.314291", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 245, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059314645000, "dur": 73000, "relative_dur": 0.012493582064008215, "relative_gap_to_previous": 0.02070854013349307, "parent_is_longest": true, "runtime_str": "73 us"}}, "gpu3": {"time": {"ts": 1678058059360649000, "dur": 781000, "relative_dur": 0.1336642135889098, "relative_gap_to_previous": 0.00017114495978093444, "parent_is_longest": true, "runtime_str": "781 us"}}}, "id": "QwbvAH7vAP0io2GU", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.252.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 781000, "runtime_str": "781 us", "start_timestamp": "23:14:19.314.314645", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 246, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059314869000, "dur": 47000, "relative_dur": 0.00804381310970392, "relative_gap_to_previous": 0.023618004449768953, "parent_is_longest": true, "runtime_str": "47 us"}}, "gpu3": {"time": {"ts": 1678058059361431000, "dur": 321000, "relative_dur": 0.05493753208967996, "relative_gap_to_previous": 0.00017114495978093444, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "tu8GyjS36V81RU42", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.253.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "23:14:19.314.314869", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 247, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059315048000, "dur": 62000, "relative_dur": 0.010610987506417936, "relative_gap_to_previous": 0.020366250213931198, "parent_is_longest": true, "runtime_str": "62 us"}}, "gpu3": {"time": {"ts": 1678058059361753000, "dur": 388000, "relative_dur": 0.06640424439500257, "relative_gap_to_previous": 0.00017114495978093444, "parent_is_longest": true, "runtime_str": "388 us"}}}, "id": "R0yar7qPD4tj4EhK", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.254.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 388000, "runtime_str": "388 us", "start_timestamp": "23:14:19.315.315048", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 248, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059315241000, "dur": 56000, "relative_dur": 0.009584117747732329, "relative_gap_to_previous": 0.020195105254150265, "parent_is_longest": true, "runtime_str": "56 us"}}, "gpu3": {"time": {"ts": 1678058059362143000, "dur": 450000, "relative_dur": 0.0770152319014205, "relative_gap_to_previous": 0.0003422899195618689, "parent_is_longest": true, "runtime_str": "450 us"}}}, "id": "N4zMeozvuF8pdKnZ", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.255.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 450000, "runtime_str": "450 us", "start_timestamp": "23:14:19.315.315241", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 249, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059315427000, "dur": 136000, "relative_dur": 0.023275714530207086, "relative_gap_to_previous": 0.02002396029436933, "parent_is_longest": true, "runtime_str": "136 us"}}, "gpu3": {"time": {"ts": 1678058059362594000, "dur": 953000, "relative_dur": 0.16310114667123055, "relative_gap_to_previous": 0.00017114495978093444, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "Mz58AzitBvEfpPmd", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.256.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.315.315427", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059313445000, "dur": 2105000, "relative_dur": 0.10444058546266435, "relative_gap_to_previous": 0.004812701562887621, "parent_is_longest": true, "runtime_str": "2.10 ms"}}, "gpu3": {"time": {"ts": 1678058059357704000, "dur": 5843000, "relative_dur": 0.28990324981394194, "relative_gap_to_previous": 4.961548002976929e-05, "parent_is_longest": true, "runtime_str": "5.84 ms"}}}, "id": "SQX1OimMBcxpiXB2", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.247.pt.trace.json", "trace_disk_size": "23.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5843000, "runtime_str": "5.84 ms", "start_timestamp": "23:14:19.313.313445", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 193}, {"idx": 250, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu2": {"time": {"ts": 1678058059315569000, "dur": 217000, "relative_dur": 0.010766559166459936, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "217 us"}}, "gpu3": {"time": {"ts": 1678058059363548000, "dur": 98000, "relative_dur": 0.00486231704291739, "relative_gap_to_previous": 4.961548002976929e-05, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "cm1r71WPwNAWz3Ca", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.257.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 217000, "runtime_str": "217 us", "start_timestamp": "23:14:19.315.315569", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 251, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu2": {"time": {"ts": 1678058059315799000, "dur": 216000, "relative_dur": 0.010716943686430166, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "216 us"}}, "gpu3": {"time": {"ts": 1678058059363648000, "dur": 226000, "relative_dur": 0.011213098486727859, "relative_gap_to_previous": 9.923096005953858e-05, "parent_is_longest": true, "runtime_str": "226 us"}}}, "id": "ehIYWk7GN5Gwh7gz", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.258.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 226000, "runtime_str": "226 us", "start_timestamp": "23:14:19.315.315799", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 252, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"idx": 253, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu2": {"time": {"ts": 1678058059316143000, "dur": 213000, "relative_dur": 0.028100263852242745, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "213 us"}}, "gpu3": {"time": {"ts": 1678058059363875000, "dur": 3376000, "relative_dur": 0.44538258575197887, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3.38 ms"}}}, "id": "MMxmNsWscRzA39Cg", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.260.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3376000, "runtime_str": "3.38 ms", "start_timestamp": "23:14:19.316.316143", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 254, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu2": {"time": {"ts": 1678058059316482000, "dur": 49000, "relative_dur": 0.006464379947229552, "relative_gap_to_previous": 0.01662269129287599, "parent_is_longest": true, "runtime_str": "49 us"}}, "gpu3": {"time": {"ts": 1678058059367252000, "dur": 320000, "relative_dur": 0.04221635883905013, "relative_gap_to_previous": 0.00013192612137203166, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "SkDMh3jzRY7xDxVC", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.261.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "23:14:19.316.316482", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 255, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu2": {"time": {"ts": 1678058059316652000, "dur": 65000, "relative_dur": 0.008575197889182058, "relative_gap_to_previous": 0.015963060686015832, "parent_is_longest": true, "runtime_str": "65 us"}}, "gpu3": {"time": {"ts": 1678058059367573000, "dur": 389000, "relative_dur": 0.05131926121372032, "relative_gap_to_previous": 0.00013192612137203166, "parent_is_longest": true, "runtime_str": "389 us"}}}, "id": "xzZIWwLFI16DOsxl", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.262.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 389000, "runtime_str": "389 us", "start_timestamp": "23:14:19.316.316652", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 256, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu2": {"time": {"ts": 1678058059316838000, "dur": 103000, "relative_dur": 0.013588390501319261, "relative_gap_to_previous": 0.015963060686015832, "parent_is_longest": true, "runtime_str": "103 us"}}, "gpu3": {"time": {"ts": 1678058059367964000, "dur": 3392000, "relative_dur": 0.4474934036939314, "relative_gap_to_previous": 0.0002638522427440633, "parent_is_longest": true, "runtime_str": "3.39 ms"}}}, "id": "0hr4GNt9soYOx6vY", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.263.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3392000, "runtime_str": "3.39 ms", "start_timestamp": "23:14:19.316.316838", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 257, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu2": {"time": {"ts": 1678058059317066000, "dur": 68000, "relative_dur": 0.008970976253298154, "relative_gap_to_previous": 0.016490765171503958, "parent_is_longest": true, "runtime_str": "68 us"}}, "gpu3": {"time": {"ts": 1678058059371357000, "dur": 98000, "relative_dur": 0.012928759894459104, "relative_gap_to_previous": 0.00013192612137203166, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "9gQK4MNGyuaIvToo", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.264.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 98000, "runtime_str": "98 us", "start_timestamp": "23:14:19.317.317066", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1678058059316143000, "dur": 991000, "relative_dur": 0.04916894070950136, "relative_gap_to_previous": 0.0050607789630364676, "parent_is_longest": true, "runtime_str": "991 us"}}, "gpu3": {"time": {"ts": 1678058059363875000, "dur": 7580000, "relative_dur": 0.3760853386256512, "relative_gap_to_previous": 4.961548002976929e-05, "parent_is_longest": true, "runtime_str": "7.58 ms"}}}, "id": "QSuKlVQzcVfnEURl", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.259.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7580000, "runtime_str": "7.58 ms", "start_timestamp": "23:14:19.316.316143", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1678058059310406000, "dur": 6702000, "relative_dur": 0.05525645359430781, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6.70 ms"}}, "gpu3": {"time": {"ts": 1678058059351300000, "dur": 20155000, "relative_dur": 0.16617335454987675, "relative_gap_to_previous": 8.24477075414918e-06, "parent_is_longest": true, "runtime_str": "20.2 ms"}}}, "id": "ZcVfblsExnD7VYIR", "pretty_name": "Layer4", "trace_file": "/results/Transformer/Transformer.234.pt.trace.json", "trace_disk_size": "64.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 20155000, "runtime_str": "20.2 ms", "start_timestamp": "23:14:19.310.310406", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 535}, {"idx": 258, "name": "Layer5", "type": "Layer5", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"idx": 259, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"idx": 260, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059317149000, "dur": 630000, "relative_dur": 0.10382333553065261, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "630 us"}}, "gpu3": {"time": {"ts": 1678058059371456000, "dur": 1186000, "relative_dur": 0.1954515491100857, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.19 ms"}}}, "id": "v74Mmbs3UM8Js4Xh", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.267.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1186000, "runtime_str": "1.19 ms", "start_timestamp": "23:14:19.317.317149", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 51}, {"idx": 261, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059317921000, "dur": 123000, "relative_dur": 0.02027027027027027, "relative_gap_to_previous": 0.021259063941990772, "parent_is_longest": true, "runtime_str": "123 us"}}, "gpu3": {"time": {"ts": 1678058059372644000, "dur": 953000, "relative_dur": 0.1570533948582729, "relative_gap_to_previous": 0.0003295978905735003, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "DIAZQfsTKSiFVkit", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.268.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.317.317921", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 262, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059318182000, "dur": 123000, "relative_dur": 0.02027027027027027, "relative_gap_to_previous": 0.02059986816084377, "parent_is_longest": true, "runtime_str": "123 us"}}, "gpu3": {"time": {"ts": 1678058059373598000, "dur": 953000, "relative_dur": 0.1570533948582729, "relative_gap_to_previous": 0.00016479894528675015, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "a8ywylhBJCFkelhd", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.269.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.318.318182", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 263, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059318322000, "dur": 219000, "relative_dur": 0.036090969017798284, "relative_gap_to_previous": 0.0006591957811470006, "parent_is_longest": true, "runtime_str": "219 us"}}, "gpu3": {"time": {"ts": 1678058059374552000, "dur": 82000, "relative_dur": 0.013513513513513514, "relative_gap_to_previous": 0.00016479894528675015, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "U0RN7hcrR8XEcsC6", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.270.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 219000, "runtime_str": "219 us", "start_timestamp": "23:14:19.318.318322", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 264, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059318675000, "dur": 72000, "relative_dur": 0.011865524060646011, "relative_gap_to_previous": 0.01994067237969677, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1678058059374636000, "dur": 771000, "relative_dur": 0.12705998681608438, "relative_gap_to_previous": 0.0003295978905735003, "parent_is_longest": true, "runtime_str": "771 us"}}}, "id": "dn5d7rVcMKRu8S7Y", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.271.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 771000, "runtime_str": "771 us", "start_timestamp": "23:14:19.318.318675", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 265, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059318897000, "dur": 48000, "relative_dur": 0.007910349373764008, "relative_gap_to_previous": 0.022577455504284773, "parent_is_longest": true, "runtime_str": "48 us"}}, "gpu3": {"time": {"ts": 1678058059375408000, "dur": 320000, "relative_dur": 0.05273566249176005, "relative_gap_to_previous": 0.00016479894528675015, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "YMdWF1qnGgVwKwHZ", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.272.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "23:14:19.318.318897", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 266, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059319077000, "dur": 62000, "relative_dur": 0.01021753460777851, "relative_gap_to_previous": 0.01961107448912327, "parent_is_longest": true, "runtime_str": "62 us"}}, "gpu3": {"time": {"ts": 1678058059375730000, "dur": 389000, "relative_dur": 0.06410678971654582, "relative_gap_to_previous": 0.0003295978905735003, "parent_is_longest": true, "runtime_str": "389 us"}}}, "id": "6v8CNxSLSwsUzpBH", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.273.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 389000, "runtime_str": "389 us", "start_timestamp": "23:14:19.319.319077", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 267, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059319272000, "dur": 57000, "relative_dur": 0.009393539881344759, "relative_gap_to_previous": 0.01977587343441002, "parent_is_longest": true, "runtime_str": "57 us"}}, "gpu3": {"time": {"ts": 1678058059376121000, "dur": 450000, "relative_dur": 0.07415952537903757, "relative_gap_to_previous": 0.0003295978905735003, "parent_is_longest": true, "runtime_str": "450 us"}}}, "id": "yeQdiDMdGISdxhnI", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.274.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 450000, "runtime_str": "450 us", "start_timestamp": "23:14:19.319.319272", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 268, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059319466000, "dur": 135000, "relative_dur": 0.022247857613711272, "relative_gap_to_previous": 0.02043506921555702, "parent_is_longest": true, "runtime_str": "135 us"}}, "gpu3": {"time": {"ts": 1678058059376572000, "dur": 952000, "relative_dur": 0.15688859591298615, "relative_gap_to_previous": 0.00016479894528675015, "parent_is_longest": true, "runtime_str": "952 us"}}}, "id": "YtjBRmpGuWzkSITg", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.275.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "23:14:19.319.319466", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059317149000, "dur": 2439000, "relative_dur": 0.12104819097721971, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2.44 ms"}}, "gpu3": {"time": {"ts": 1678058059371456000, "dur": 6068000, "relative_dur": 0.3011563849322547, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6.07 ms"}}}, "id": "5vkuedTBeVLrHoFo", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.266.pt.trace.json", "trace_disk_size": "25.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 6068000, "runtime_str": "6.07 ms", "start_timestamp": "23:14:19.317.317149", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 207}, {"idx": 269, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu2": {"time": {"ts": 1678058059319606000, "dur": 226000, "relative_dur": 0.011216437540324581, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "226 us"}}, "gpu3": {"time": {"ts": 1678058059377525000, "dur": 98000, "relative_dur": 0.004863764951114199, "relative_gap_to_previous": 4.9630254603206117e-05, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "TuHf2X9UdajrSXpH", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.276.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 226000, "runtime_str": "226 us", "start_timestamp": "23:14:19.319.319606", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 270, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu2": {"time": {"ts": 1678058059319852000, "dur": 213000, "relative_dur": 0.010571244230482902, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "213 us"}}, "gpu3": {"time": {"ts": 1678058059377624000, "dur": 231000, "relative_dur": 0.011464588813340613, "relative_gap_to_previous": 4.9630254603206117e-05, "parent_is_longest": true, "runtime_str": "231 us"}}}, "id": "kWbTNm38fOEbrgo4", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.277.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 231000, "runtime_str": "231 us", "start_timestamp": "23:14:19.319.319852", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 271, "name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"idx": 272, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059320188000, "dur": 300000, "relative_dur": 0.05140507196710076, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "300 us"}}, "gpu3": {"time": {"ts": 1678058059377857000, "dur": 953000, "relative_dur": 0.16329677861549005, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "nrzwyONnnuXR71ed", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.279.pt.trace.json", "trace_disk_size": "4.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.320.320188", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 37}, {"idx": 273, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059320630000, "dur": 128000, "relative_dur": 0.02193283070596299, "relative_gap_to_previous": 0.021590130226182315, "parent_is_longest": true, "runtime_str": "128 us"}}, "gpu3": {"time": {"ts": 1678058059378811000, "dur": 953000, "relative_dur": 0.16329677861549005, "relative_gap_to_previous": 0.00017135023989033586, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "t0N9m89hDyqoAZep", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.280.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.320.320630", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 274, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059320897000, "dur": 123000, "relative_dur": 0.02107607950651131, "relative_gap_to_previous": 0.02107607950651131, "parent_is_longest": true, "runtime_str": "123 us"}}, "gpu3": {"time": {"ts": 1678058059379766000, "dur": 953000, "relative_dur": 0.16329677861549005, "relative_gap_to_previous": 0.0003427004797806717, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "FQeoVpzFTloDE9zO", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.281.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.320.320897", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 275, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059321037000, "dur": 221000, "relative_dur": 0.037868403015764225, "relative_gap_to_previous": 0.00017135023989033586, "parent_is_longest": true, "runtime_str": "221 us"}}, "gpu3": {"time": {"ts": 1678058059380720000, "dur": 82000, "relative_dur": 0.01405071967100754, "relative_gap_to_previous": 0.00017135023989033586, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "YMv5CiCLmzBLXtoy", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.282.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 221000, "runtime_str": "221 us", "start_timestamp": "23:14:19.321.321037", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 276, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059321391000, "dur": 72000, "relative_dur": 0.01233721727210418, "relative_gap_to_previous": 0.020047978067169295, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1678058059380803000, "dur": 776000, "relative_dur": 0.13296778615490062, "relative_gap_to_previous": 0.00017135023989033586, "parent_is_longest": true, "runtime_str": "776 us"}}}, "id": "ys1RswdGvkTinD0W", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.283.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 776000, "runtime_str": "776 us", "start_timestamp": "23:14:19.321.321391", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 277, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059321618000, "dur": 47000, "relative_dur": 0.008053461274845785, "relative_gap_to_previous": 0.023817683344756684, "parent_is_longest": true, "runtime_str": "47 us"}}, "gpu3": {"time": {"ts": 1678058059381580000, "dur": 320000, "relative_dur": 0.05483207676490747, "relative_gap_to_previous": 0.00017135023989033586, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "lTx87bUnrXVYCkqF", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.284.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "23:14:19.321.321618", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 278, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059321798000, "dur": 61000, "relative_dur": 0.010452364633310486, "relative_gap_to_previous": 0.020047978067169295, "parent_is_longest": true, "runtime_str": "61 us"}}, "gpu3": {"time": {"ts": 1678058059381902000, "dur": 386000, "relative_dur": 0.06614119259766964, "relative_gap_to_previous": 0.0003427004797806717, "parent_is_longest": true, "runtime_str": "386 us"}}}, "id": "2lST5C9ruei3ZoN8", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.285.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 386000, "runtime_str": "386 us", "start_timestamp": "23:14:19.321.321798", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 279, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059321991000, "dur": 57000, "relative_dur": 0.009766963673749143, "relative_gap_to_previous": 0.019876627827278958, "parent_is_longest": true, "runtime_str": "57 us"}}, "gpu3": {"time": {"ts": 1678058059382289000, "dur": 449000, "relative_dur": 0.07693625771076079, "relative_gap_to_previous": 0.00017135023989033586, "parent_is_longest": true, "runtime_str": "449 us"}}}, "id": "uihFJCFICid3pdSd", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.286.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 449000, "runtime_str": "449 us", "start_timestamp": "23:14:19.321.321991", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 280, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059322178000, "dur": 135000, "relative_dur": 0.023132282385195338, "relative_gap_to_previous": 0.019533927347498287, "parent_is_longest": true, "runtime_str": "135 us"}}, "gpu3": {"time": {"ts": 1678058059382741000, "dur": 952000, "relative_dur": 0.16312542837559973, "relative_gap_to_previous": 0.0005140507196710075, "parent_is_longest": true, "runtime_str": "952 us"}}}, "id": "oKnpvwMklao78BRP", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.287.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "23:14:19.322.322178", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059320188000, "dur": 2109000, "relative_dur": 0.10467020695816169, "relative_gap_to_previous": 0.004665243932701375, "parent_is_longest": true, "runtime_str": "2.11 ms"}}, "gpu3": {"time": {"ts": 1678058059377857000, "dur": 5836000, "relative_dur": 0.2896421658643109, "relative_gap_to_previous": 9.926050920641223e-05, "parent_is_longest": true, "runtime_str": "5.84 ms"}}}, "id": "gMgdPxmArh82ghHP", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.278.pt.trace.json", "trace_disk_size": "23.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5836000, "runtime_str": "5.84 ms", "start_timestamp": "23:14:19.320.320188", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 193}, {"idx": 281, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu2": {"time": {"ts": 1678058059322319000, "dur": 218000, "relative_dur": 0.010819395503498933, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "218 us"}}, "gpu3": {"time": {"ts": 1678058059383694000, "dur": 99000, "relative_dur": 0.004913395205717405, "relative_gap_to_previous": 4.9630254603206117e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "oHXr55WVTxppSIce", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.288.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 218000, "runtime_str": "218 us", "start_timestamp": "23:14:19.322.322319", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 282, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu2": {"time": {"ts": 1678058059322549000, "dur": 215000, "relative_dur": 0.010670504739689314, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "215 us"}}, "gpu3": {"time": {"ts": 1678058059383794000, "dur": 231000, "relative_dur": 0.011464588813340613, "relative_gap_to_previous": 4.9630254603206117e-05, "parent_is_longest": true, "runtime_str": "231 us"}}}, "id": "fcTk5gDswytljg2Y", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.289.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 231000, "runtime_str": "231 us", "start_timestamp": "23:14:19.322.322549", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 283, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"idx": 284, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu2": {"time": {"ts": 1678058059322894000, "dur": 213000, "relative_dur": 0.0281076801266825, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "213 us"}}, "gpu3": {"time": {"ts": 1678058059384027000, "dur": 3376000, "relative_dur": 0.44550013196093957, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3.38 ms"}}}, "id": "ExbvzzA0P4F5afkT", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.291.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3376000, "runtime_str": "3.38 ms", "start_timestamp": "23:14:19.322.322894", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 285, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu2": {"time": {"ts": 1678058059323232000, "dur": 51000, "relative_dur": 0.006730007917656373, "relative_gap_to_previous": 0.01649511744523621, "parent_is_longest": true, "runtime_str": "51 us"}}, "gpu3": {"time": {"ts": 1678058059387405000, "dur": 319000, "relative_dur": 0.04209553972024281, "relative_gap_to_previous": 0.0002639218791237794, "parent_is_longest": true, "runtime_str": "319 us"}}}, "id": "ZewcBZ48JvdAW1VJ", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.292.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 319000, "runtime_str": "319 us", "start_timestamp": "23:14:19.323.323232", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 286, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu2": {"time": {"ts": 1678058059323403000, "dur": 65000, "relative_dur": 0.008577461071522829, "relative_gap_to_previous": 0.01583531274742676, "parent_is_longest": true, "runtime_str": "65 us"}}, "gpu3": {"time": {"ts": 1678058059387725000, "dur": 387000, "relative_dur": 0.0510688836104513, "relative_gap_to_previous": 0.0001319609395618897, "parent_is_longest": true, "runtime_str": "387 us"}}}, "id": "TOInFCs0IYGDbMIL", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.293.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 387000, "runtime_str": "387 us", "start_timestamp": "23:14:19.323.323403", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 287, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu2": {"time": {"ts": 1678058059323588000, "dur": 105000, "relative_dur": 0.013855898653998416, "relative_gap_to_previous": 0.01583531274742676, "parent_is_longest": true, "runtime_str": "105 us"}}, "gpu3": {"time": {"ts": 1678058059388114000, "dur": 3391000, "relative_dur": 0.4474795460543679, "relative_gap_to_previous": 0.0002639218791237794, "parent_is_longest": true, "runtime_str": "3.39 ms"}}}, "id": "O30DwIK34EWScART", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.294.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3391000, "runtime_str": "3.39 ms", "start_timestamp": "23:14:19.323.323588", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 288, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu2": {"time": {"ts": 1678058059323818000, "dur": 67000, "relative_dur": 0.008841382950646608, "relative_gap_to_previous": 0.01649511744523621, "parent_is_longest": true, "runtime_str": "67 us"}}, "gpu3": {"time": {"ts": 1678058059391507000, "dur": 98000, "relative_dur": 0.012932172077065188, "relative_gap_to_previous": 0.0002639218791237794, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "kjhdYzx8RBVmhrsN", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.295.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 98000, "runtime_str": "98 us", "start_timestamp": "23:14:19.323.323818", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1678058059322894000, "dur": 991000, "relative_dur": 0.04918358231177726, "relative_gap_to_previous": 0.005012655714923818, "parent_is_longest": true, "runtime_str": "991 us"}}, "gpu3": {"time": {"ts": 1678058059384027000, "dur": 7578000, "relative_dur": 0.3760980693830959, "relative_gap_to_previous": 9.926050920641223e-05, "parent_is_longest": true, "runtime_str": "7.58 ms"}}}, "id": "PUX7hwPZIdQ35mDS", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.290.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7578000, "runtime_str": "7.58 ms", "start_timestamp": "23:14:19.322.322894", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1678058059317149000, "dur": 6707000, "relative_dur": 0.05529767744807856, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6.71 ms"}}, "gpu3": {"time": {"ts": 1678058059371456000, "dur": 20149000, "relative_dur": 0.16612388592535185, "relative_gap_to_previous": 8.24477075414918e-06, "parent_is_longest": true, "runtime_str": "20.1 ms"}}}, "id": "TPfr059d4gndEBSG", "pretty_name": "Layer5", "trace_file": "/results/Transformer/Transformer.265.pt.trace.json", "trace_disk_size": "64.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 20149000, "runtime_str": "20.1 ms", "start_timestamp": "23:14:19.317.317149", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 535}, {"idx": 289, "name": "Layer6", "type": "Layer6", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"idx": 290, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"idx": 291, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059323900000, "dur": 629000, "relative_dur": 0.10359025032938077, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "629 us"}}, "gpu3": {"time": {"ts": 1678058059391606000, "dur": 1182000, "relative_dur": 0.19466403162055335, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.18 ms"}}}, "id": "5CvaJW2jvudK1CyV", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.298.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1182000, "runtime_str": "1.18 ms", "start_timestamp": "23:14:19.323.323900", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 51}, {"idx": 292, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059324671000, "dur": 123000, "relative_dur": 0.020256916996047432, "relative_gap_to_previous": 0.020915678524374176, "parent_is_longest": true, "runtime_str": "123 us"}}, "gpu3": {"time": {"ts": 1678058059392790000, "dur": 954000, "relative_dur": 0.15711462450592886, "relative_gap_to_previous": 0.00032938076416337287, "parent_is_longest": true, "runtime_str": "954 us"}}}, "id": "cE2vq4nV3Z2p2SPN", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.299.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 954000, "runtime_str": "954 us", "start_timestamp": "23:14:19.324.324671", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 293, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059324933000, "dur": 121000, "relative_dur": 0.019927536231884056, "relative_gap_to_previous": 0.020421607378129116, "parent_is_longest": true, "runtime_str": "121 us"}}, "gpu3": {"time": {"ts": 1678058059393746000, "dur": 953000, "relative_dur": 0.15694993412384717, "relative_gap_to_previous": 0.00032938076416337287, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "EnRgfunmryAHBYGs", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.300.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.324.324933", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 294, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059325071000, "dur": 219000, "relative_dur": 0.03606719367588933, "relative_gap_to_previous": 0.00032938076416337287, "parent_is_longest": true, "runtime_str": "219 us"}}, "gpu3": {"time": {"ts": 1678058059394701000, "dur": 82000, "relative_dur": 0.013504611330698288, "relative_gap_to_previous": 0.00032938076416337287, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "rxvIhwhugfwsTilV", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.301.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 219000, "runtime_str": "219 us", "start_timestamp": "23:14:19.325.325071", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 295, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059325424000, "dur": 72000, "relative_dur": 0.011857707509881422, "relative_gap_to_previous": 0.019598155467720684, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1678058059394784000, "dur": 777000, "relative_dur": 0.12796442687747037, "relative_gap_to_previous": 0.00016469038208168644, "parent_is_longest": true, "runtime_str": "777 us"}}}, "id": "ZWrF0tvb9RclMrSd", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.302.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 777000, "runtime_str": "777 us", "start_timestamp": "23:14:19.325.325424", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 296, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059325649000, "dur": 48000, "relative_dur": 0.007905138339920948, "relative_gap_to_previous": 0.022727272727272728, "parent_is_longest": true, "runtime_str": "48 us"}}, "gpu3": {"time": {"ts": 1678058059395563000, "dur": 320000, "relative_dur": 0.052700922266139656, "relative_gap_to_previous": 0.00032938076416337287, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "PuqZWYR6Fh0ERrmi", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.303.pt.trace.json", "trace_disk_size": "902 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "23:14:19.325.325649", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 297, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059325830000, "dur": 62000, "relative_dur": 0.010210803689064558, "relative_gap_to_previous": 0.019433465085639, "parent_is_longest": true, "runtime_str": "62 us"}}, "gpu3": {"time": {"ts": 1678058059395885000, "dur": 386000, "relative_dur": 0.06357048748353096, "relative_gap_to_previous": 0.00032938076416337287, "parent_is_longest": true, "runtime_str": "386 us"}}}, "id": "LNgwYKOkMIPlRsPX", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.304.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 386000, "runtime_str": "386 us", "start_timestamp": "23:14:19.325.325830", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 298, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059326026000, "dur": 58000, "relative_dur": 0.009552042160737812, "relative_gap_to_previous": 0.019598155467720684, "parent_is_longest": true, "runtime_str": "58 us"}}, "gpu3": {"time": {"ts": 1678058059396272000, "dur": 451000, "relative_dur": 0.07427536231884058, "relative_gap_to_previous": 0.00016469038208168644, "parent_is_longest": true, "runtime_str": "451 us"}}}, "id": "aKa3fZHpKBQykEqz", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.305.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 451000, "runtime_str": "451 us", "start_timestamp": "23:14:19.326.326026", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 299, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059326221000, "dur": 139000, "relative_dur": 0.022891963109354412, "relative_gap_to_previous": 0.020092226613965744, "parent_is_longest": true, "runtime_str": "139 us"}}, "gpu3": {"time": {"ts": 1678058059396726000, "dur": 952000, "relative_dur": 0.15678524374176547, "relative_gap_to_previous": 0.0004940711462450593, "parent_is_longest": true, "runtime_str": "952 us"}}}, "id": "rzacIoR0xpqP5CcX", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.306.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "23:14:19.326.326221", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059323900000, "dur": 2445000, "relative_dur": 0.12130383012502481, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2.44 ms"}}, "gpu3": {"time": {"ts": 1678058059391606000, "dur": 6072000, "relative_dur": 0.30125024806509226, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6.07 ms"}}}, "id": "W0TLl0BrQVJuS1T9", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.297.pt.trace.json", "trace_disk_size": "25.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 6072000, "runtime_str": "6.07 ms", "start_timestamp": "23:14:19.323.323900", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 207}, {"idx": 300, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu2": {"time": {"ts": 1678058059326366000, "dur": 218000, "relative_dur": 0.010815638023417345, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "218 us"}}, "gpu3": {"time": {"ts": 1678058059397680000, "dur": 98000, "relative_dur": 0.0048620758086922004, "relative_gap_to_previous": 9.922603691208574e-05, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "xnVciBGnjkcRCEoo", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.307.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 218000, "runtime_str": "218 us", "start_timestamp": "23:14:19.326.326366", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 301, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu2": {"time": {"ts": 1678058059326597000, "dur": 216000, "relative_dur": 0.01071641198650526, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "216 us"}}, "gpu3": {"time": {"ts": 1678058059397780000, "dur": 226000, "relative_dur": 0.011212542171065688, "relative_gap_to_previous": 9.922603691208574e-05, "parent_is_longest": true, "runtime_str": "226 us"}}}, "id": "QuJuHliVOTSk79WJ", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.308.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 226000, "runtime_str": "226 us", "start_timestamp": "23:14:19.326.326597", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 302, "name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"idx": 303, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1678058059326934000, "dur": 299000, "relative_dur": 0.051216169921205895, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "299 us"}}, "gpu3": {"time": {"ts": 1678058059398007000, "dur": 953000, "relative_dur": 0.1632408359027064, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "WhjaErctYkPFGRlW", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.310.pt.trace.json", "trace_disk_size": "4.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.326.326934", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 37}, {"idx": 304, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1678058059327375000, "dur": 124000, "relative_dur": 0.021240150736553613, "relative_gap_to_previous": 0.021925316889345667, "parent_is_longest": true, "runtime_str": "124 us"}}, "gpu3": {"time": {"ts": 1678058059398962000, "dur": 953000, "relative_dur": 0.1632408359027064, "relative_gap_to_previous": 0.00034258307639602604, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "zTdurD6xp8wT2xzu", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.311.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.327.327375", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 305, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1678058059327638000, "dur": 120000, "relative_dur": 0.020554984583761562, "relative_gap_to_previous": 0.021411442274751627, "parent_is_longest": true, "runtime_str": "120 us"}}, "gpu3": {"time": {"ts": 1678058059399916000, "dur": 954000, "relative_dur": 0.16341212744090441, "relative_gap_to_previous": 0.00017129153819801302, "parent_is_longest": true, "runtime_str": "954 us"}}}, "id": "jlVv4IUQOctRA83U", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.312.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 954000, "runtime_str": "954 us", "start_timestamp": "23:14:19.327.327638", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 306, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1678058059327775000, "dur": 219000, "relative_dur": 0.03751284686536485, "relative_gap_to_previous": 0.0005138746145940391, "parent_is_longest": true, "runtime_str": "219 us"}}, "gpu3": {"time": {"ts": 1678058059400871000, "dur": 82000, "relative_dur": 0.014045906132237067, "relative_gap_to_previous": 0.00017129153819801302, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "SHMvj0eW2tFAG4Oh", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.313.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 219000, "runtime_str": "219 us", "start_timestamp": "23:14:19.327.327775", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 307, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1678058059328127000, "dur": 72000, "relative_dur": 0.012332990750256937, "relative_gap_to_previous": 0.02038369304556355, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1678058059400955000, "dur": 773000, "relative_dur": 0.13240835902706408, "relative_gap_to_previous": 0.00034258307639602604, "parent_is_longest": true, "runtime_str": "773 us"}}}, "id": "tPbc8NNxYGLkCTtK", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.314.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 773000, "runtime_str": "773 us", "start_timestamp": "23:14:19.328.328127", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 308, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1678058059328351000, "dur": 47000, "relative_dur": 0.008050702295306612, "relative_gap_to_previous": 0.023638232271325797, "parent_is_longest": true, "runtime_str": "47 us"}}, "gpu3": {"time": {"ts": 1678058059401730000, "dur": 321000, "relative_dur": 0.05498458376156218, "relative_gap_to_previous": 0.00034258307639602604, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "THCg8u32vbj27EYE", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.315.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "23:14:19.328.328351", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 309, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1678058059328531000, "dur": 60000, "relative_dur": 0.010277492291880781, "relative_gap_to_previous": 0.02038369304556355, "parent_is_longest": true, "runtime_str": "60 us"}}, "gpu3": {"time": {"ts": 1678058059402052000, "dur": 387000, "relative_dur": 0.06628982528263104, "relative_gap_to_previous": 0.00017129153819801302, "parent_is_longest": true, "runtime_str": "387 us"}}}, "id": "WFfyfMXEVQRcgkIx", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.316.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 387000, "runtime_str": "387 us", "start_timestamp": "23:14:19.328.328531", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 310, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1678058059328724000, "dur": 58000, "relative_dur": 0.009934909215484755, "relative_gap_to_previous": 0.02038369304556355, "parent_is_longest": true, "runtime_str": "58 us"}}, "gpu3": {"time": {"ts": 1678058059402441000, "dur": 450000, "relative_dur": 0.07708119218910586, "relative_gap_to_previous": 0.00034258307639602604, "parent_is_longest": true, "runtime_str": "450 us"}}}, "id": "JKUeGwIaYmU1TDoM", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.317.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 450000, "runtime_str": "450 us", "start_timestamp": "23:14:19.328.328724", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 311, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1678058059328914000, "dur": 143000, "relative_dur": 0.024494689962315862, "relative_gap_to_previous": 0.020212401507365536, "parent_is_longest": true, "runtime_str": "143 us"}}, "gpu3": {"time": {"ts": 1678058059402892000, "dur": 953000, "relative_dur": 0.1632408359027064, "relative_gap_to_previous": 0.00017129153819801302, "parent_is_longest": true, "runtime_str": "953 us"}}}, "id": "p4XZw4kYuFpcjEa1", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.318.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.328.328914", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1678058059326934000, "dur": 2109000, "relative_dur": 0.1046338559237944, "relative_gap_to_previous": 0.0045643976979559435, "parent_is_longest": true, "runtime_str": "2.11 ms"}}, "gpu3": {"time": {"ts": 1678058059398007000, "dur": 5838000, "relative_dur": 0.28964080174637824, "relative_gap_to_previous": 4.961301845604287e-05, "parent_is_longest": true, "runtime_str": "5.84 ms"}}}, "id": "CxndeqUmtIp9riN2", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.309.pt.trace.json", "trace_disk_size": "23.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5838000, "runtime_str": "5.84 ms", "start_timestamp": "23:14:19.326.326934", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 193}, {"idx": 312, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu2": {"time": {"ts": 1678058059329062000, "dur": 220000, "relative_dur": 0.01091486406032943, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "220 us"}}, "gpu3": {"time": {"ts": 1678058059403846000, "dur": 100000, "relative_dur": 0.004961301845604287, "relative_gap_to_previous": 4.961301845604287e-05, "parent_is_longest": true, "runtime_str": "100 us"}}}, "id": "TBnaGQ63VdZn5pDC", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.319.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 220000, "runtime_str": "220 us", "start_timestamp": "23:14:19.329.329062", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 313, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu2": {"time": {"ts": 1678058059329295000, "dur": 222000, "relative_dur": 0.011014090097241516, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "222 us"}}, "gpu3": {"time": {"ts": 1678058059403947000, "dur": 232000, "relative_dur": 0.011510220281801944, "relative_gap_to_previous": 4.961301845604287e-05, "parent_is_longest": true, "runtime_str": "232 us"}}}, "id": "g7hU6psCQL1RFNdU", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.320.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 232000, "runtime_str": "232 us", "start_timestamp": "23:14:19.329.329295", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 314, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"idx": 315, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu2": {"time": {"ts": 1678058059329639000, "dur": 212000, "relative_dur": 0.027964648463263422, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "212 us"}}, "gpu3": {"time": {"ts": 1678058059404181000, "dur": 3377000, "relative_dur": 0.4454557446247197, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3.38 ms"}}}, "id": "5jjtOYPvE7edSJ2p", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.322.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3377000, "runtime_str": "3.38 ms", "start_timestamp": "23:14:19.329.329639", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 316, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu2": {"time": {"ts": 1678058059329976000, "dur": 48000, "relative_dur": 0.006331618519984171, "relative_gap_to_previous": 0.016488589895792112, "parent_is_longest": true, "runtime_str": "48 us"}}, "gpu3": {"time": {"ts": 1678058059407560000, "dur": 320000, "relative_dur": 0.04221079013322781, "relative_gap_to_previous": 0.00026381743833267377, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "kAG3IwQdfmRyq9ur", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.323.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "23:14:19.329.329976", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 317, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu2": {"time": {"ts": 1678058059330146000, "dur": 66000, "relative_dur": 0.008705975464978234, "relative_gap_to_previous": 0.016092863738293103, "parent_is_longest": true, "runtime_str": "66 us"}}, "gpu3": {"time": {"ts": 1678058059407881000, "dur": 388000, "relative_dur": 0.05118058303653872, "relative_gap_to_previous": 0.00013190871916633689, "parent_is_longest": true, "runtime_str": "388 us"}}}, "id": "aDFPRAjmdPAiGh44", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.324.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 388000, "runtime_str": "388 us", "start_timestamp": "23:14:19.330.330146", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 318, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu2": {"time": {"ts": 1678058059330333000, "dur": 103000, "relative_dur": 0.0135865980741327, "relative_gap_to_previous": 0.015960955019126765, "parent_is_longest": true, "runtime_str": "103 us"}}, "gpu3": {"time": {"ts": 1678058059408271000, "dur": 3391000, "relative_dur": 0.4473024666930484, "relative_gap_to_previous": 0.00026381743833267377, "parent_is_longest": true, "runtime_str": "3.39 ms"}}}, "id": "K8k9VAdIuu0LHSVG", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.325.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3391000, "runtime_str": "3.39 ms", "start_timestamp": "23:14:19.330.330333", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 319, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu2": {"time": {"ts": 1678058059330562000, "dur": 68000, "relative_dur": 0.00896979290331091, "relative_gap_to_previous": 0.01662049861495845, "parent_is_longest": true, "runtime_str": "68 us"}}, "gpu3": {"time": {"ts": 1678058059411663000, "dur": 99000, "relative_dur": 0.013058963197467353, "relative_gap_to_previous": 0.00013190871916633689, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "dGEhPfLZcAF5hLmC", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.326.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "23:14:19.330.330562", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1678058059329639000, "dur": 991000, "relative_dur": 0.04916650128993848, "relative_gap_to_previous": 0.004614010716411986, "parent_is_longest": true, "runtime_str": "991 us"}}, "gpu3": {"time": {"ts": 1678058059404181000, "dur": 7581000, "relative_dur": 0.37611629291526094, "relative_gap_to_previous": 9.922603691208574e-05, "parent_is_longest": true, "runtime_str": "7.58 ms"}}}, "id": "N7ydXlgy2rxY0P6e", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.321.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7581000, "runtime_str": "7.58 ms", "start_timestamp": "23:14:19.329.329639", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1678058059323900000, "dur": 6701000, "relative_dur": 0.05524820882355366, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6.70 ms"}}, "gpu3": {"time": {"ts": 1678058059391606000, "dur": 20156000, "relative_dur": 0.1661815993206309, "relative_gap_to_previous": 8.24477075414918e-06, "parent_is_longest": true, "runtime_str": "20.2 ms"}}}, "id": "dptA04xWM1h2v29k", "pretty_name": "Layer6", "trace_file": "/results/Transformer/Transformer.296.pt.trace.json", "trace_disk_size": "64.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 20156000, "runtime_str": "20.2 ms", "start_timestamp": "23:14:19.323.323900", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 535}, {"idx": 320, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 604, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 604, "resources": {"cpu2": {"time": {"ts": 1678058059330644000, "dur": 216000, "relative_dur": 0.0017808704828962232, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "216 us"}}, "gpu3": {"time": {"ts": 1678058059411764000, "dur": 222000, "relative_dur": 0.001830339107421118, "relative_gap_to_previous": 1.648954150829836e-05, "parent_is_longest": true, "runtime_str": "222 us"}}}, "id": "0wGmIvAPgA2qgMi6", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.327.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 222000, "runtime_str": "222 us", "start_timestamp": "23:14:19.330.330644", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}], "resources": {"cpu2": {"time": {"ts": 1678058059290173000, "dur": 40524000, "relative_dur": 0.2690479352011685, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "40.5 ms"}}, "gpu3": {"time": {"ts": 1678058059290697000, "dur": 121289000, "relative_dur": 0.8052649050590891, "relative_gap_to_previous": 6.639224538573894e-06, "parent_is_longest": true, "runtime_str": "121 ms"}}}, "id": "iJzPQq0FlvJH0M0S", "pretty_name": "TransformerDecoder", "trace_file": "/results/Transformer/Transformer.140.pt.trace.json", "trace_disk_size": "390.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 121289000, "runtime_str": "121 ms", "start_timestamp": "23:14:19.290.290173", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 3227}, {"idx": 321, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 326, in forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 326, "resources": {"cpu2": {"time": {"ts": 1678058059330976000, "dur": 89000, "relative_dur": 0.0005908909839330766, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "89 us"}}, "gpu3": {"time": {"ts": 1678058059411987000, "dur": 26392000, "relative_dur": 0.17522241402204222, "relative_gap_to_previous": 6.639224538573894e-06, "parent_is_longest": true, "runtime_str": "26.4 ms"}}}, "id": "XCIDry8gaUWuBD5J", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.328.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 26392000, "runtime_str": "26.4 ms", "start_timestamp": "23:14:19.330.330976", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 15}], "resources": {"cpu2": {"time": {"ts": 1678058059232322000, "dur": 42943000, "relative_dur": 0.18032064094596637, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "42.9 ms"}}, "gpu3": {"time": {"ts": 1678058059287759000, "dur": 150620000, "relative_dur": 0.6324638460117238, "relative_gap_to_previous": 4.199069486201858e-06, "parent_is_longest": true, "runtime_str": "151 ms"}}}, "id": "OSi4zcpxln8VGfWs", "pretty_name": "Decoder", "trace_file": "/results/Transformer/Transformer.136.pt.trace.json", "trace_disk_size": "450.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 150620000, "runtime_str": "151 ms", "start_timestamp": "23:14:19.232.232322", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 3592}], "is_model_pass": "Forward", "idx": 3, "id": "B7u4BKPrvPkFAE7Z", "pretty_name": "Forward", "trace_file": "/results/Transformer/Transformer.10.pt.trace.json", "trace_disk_size": "712.4 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "runtime": 238148000, "runtime_str": "238 ms", "start_timestamp": "23:14:19.201.201437", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5713}, {"name": "Calc Loss", "type": "training loop", "instances": 10, "resources": {"cpu2": {"time": {"ts": 1678058059331405000, "dur": 544000, "relative_dur": 0.000743089202121629, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "544 us"}}, "gpu3": {"time": {"ts": 1678058059440879000, "dur": 40527000, "relative_dur": 0.05535877958526334, "relative_gap_to_previous": 2.7319455960354008e-06, "parent_is_longest": true, "runtime_str": "40.5 ms"}}}, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 148, in update_params\n    with hotline.annotate('Calc Loss'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "source_file_num": 148, "ops": [{"name": "aten::to", "type": "generated", "generated_depth": 3, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1678058059331409000, "dur": 1000, "relative_dur": 2.467490808596738e-05, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1 us"}, "res_name": "aten::to"}}, "id": "UowQThvyM4xo3qib", "pretty_name": "aten::to", "trace_file": "/results/Transformer/Transformer.330.pt.trace.json", "trace_disk_size": "91 Bytes", "runtime": 1000, "runtime_str": "1 us", "start_timestamp": "23:14:19.331.331409", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 1}, {"name": "aten::_log_softmax", "type": "generated", "generated_depth": 3, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1678058059331412000, "dur": 55000, "relative_dur": 0.001357119944728206, "relative_gap_to_previous": 4.934981617193476e-05, "parent_is_longest": true, "runtime_str": "55 us"}, "res_name": "aten::_log_softmax"}, "gpu3": {"time": {"ts": 1678058059440879000, "dur": 39282000, "relative_dur": 0.9692797394329706, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "39.3 ms"}, "res_name": "cunn_SpatialSoftMaxForward  LogSoftMaxForwardEpilogue>(86%) and 1 other\u2026"}}, "id": "iOUfQAJRz1h8JA2G", "pretty_name": "aten::_log_softmax", "trace_file": "/results/Transformer/Transformer.331.pt.trace.json", "trace_disk_size": "2.1 kB", "runtime": 39282000, "runtime_str": "39.3 ms", "start_timestamp": "23:14:19.331.331412", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 11}, {"name": "aten::nll_loss_nd(19%) and 12 others\u2026", "type": "generated", "instances": 52, "id": "mhvpWVSs0SPXjtQ0", "resources": {"cpu2": {"time": {"ts": 1678058059331470000, "dur": 479000, "relative_dur": 0.011819280973178374, "relative_gap_to_previous": 7.402472425790214e-05, "parent_is_longest": true, "runtime_str": "479 us"}, "res_name": "aten::nll_loss_nd(19%) and 12 others\u2026"}, "gpu3": {"time": {"ts": 1678058059480162000, "dur": 1244000, "relative_dur": 0.03069558565894342, "relative_gap_to_previous": 2.467490808596738e-05, "parent_is_longest": true, "runtime_str": "1.24 ms"}, "res_name": "reduce_kernel<128 4 ReduceOpfunc_wrapper_tsum_functor::operatorTensorIterator&::{lambda #1}>    4>ReduceOpfunc_wrapper_tsum_functor::operatorTensorIterator&::{lambda #1}>    4>(97%) and 10 others\u2026"}}, "pretty_name": "aten::nll_loss_nd(19%) and 12 others\u2026", "trace_file": "/results/Transformer/Transformer.332.pt.trace.json", "trace_disk_size": "10.4 kB", "runtime": 1244000, "runtime_str": "1.24 ms", "start_timestamp": "23:14:19.331.331470", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 65}], "idx": 322, "id": "dSDc9MTBIod32kjk", "pretty_name": "Calc Loss", "trace_file": "/results/Transformer/Transformer.329.pt.trace.json", "trace_disk_size": "12.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "runtime": 40527000, "runtime_str": "40.5 ms", "start_timestamp": "23:14:19.331.331405", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 79}, {"name": "Zero Grad", "type": "training loop", "instances": 5, "resources": {"cpu2": {"time": {"ts": 1678058059331953000, "dur": 2883000, "relative_dur": 0.00393809957668503, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2.88 ms"}}, "gpu3": {"time": {"ts": 1678058059481408000, "dur": 1472000, "relative_dur": 0.002010711958682055, "relative_gap_to_previous": 2.7319455960354008e-06, "parent_is_longest": true, "runtime_str": "1.47 ms"}}}, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 154, in update_params\n    with hotline.annotate('Zero Grad'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "source_file_num": 154, "ops": [{"name": "aten::zero_(100%) and 1 other\u2026", "type": "generated", "generated_depth": 2, "instances": 186, "resources": {"cpu2": {"time": {"ts": 1678058059331954000, "dur": 2740000, "relative_dur": 0.9503988900450919, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2.74 ms"}, "res_name": "aten::zero_(100%) and 1 other\u2026"}, "gpu3": {"time": {"ts": 1678058059481408000, "dur": 1469000, "relative_dur": 0.5095386749913284, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.47 ms"}, "res_name": "vectorized_elementwise_kernel<4 FillFunctor Array<char 1> FillFunctor Array<char 1>"}}, "id": "bz17Ytdyzq5XAVYc", "pretty_name": "aten::zero_(100%) and 1 other\u2026", "trace_file": "/results/Transformer/Transformer.334.pt.trace.json", "trace_disk_size": "101.9 kB", "runtime": 2740000, "runtime_str": "2.74 ms", "start_timestamp": "23:14:19.331.331954", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 741}, {"name": "aten::ones_like(84%) and 3 others\u2026", "type": "generated", "generated_depth": 1, "instances": 4, "resources": {"cpu2": {"time": {"ts": 1678058059334777000, "dur": 59000, "relative_dur": 0.02046479361775928, "relative_gap_to_previous": 0.02878945542837322, "parent_is_longest": true, "runtime_str": "59 us"}}, "gpu3": {"time": {"ts": 1678058059482878000, "dur": 2000, "relative_dur": 0.000693721817551162, "relative_gap_to_previous": 0.000346860908775581, "parent_is_longest": true, "runtime_str": "2 us"}}}, "ops": [{"name": "aten::zeros", "type": "generated", "generated_depth": 2, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1678058059334777000, "dur": 4000, "relative_dur": 0.06779661016949153, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "4 us"}, "res_name": "aten::zeros"}}, "id": "0ucSe2o7JsP8iXs9", "pretty_name": "aten::zeros", "trace_file": "/results/Transformer/Transformer.336.pt.trace.json", "trace_disk_size": "187 Bytes", "runtime": 4000, "runtime_str": "4 us", "start_timestamp": "23:14:19.334.334777", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 2}, {"name": "aten::empty", "type": "generated", "generated_depth": 2, "instances": 2, "resources": {"cpu2": {"time": {"ts": 1678058059334781000, "dur": 8000, "relative_dur": 0.13559322033898305, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "8 us"}}}, "ops": [{"name": "aten::zero_", "type": "generated", "generated_depth": 3, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1678058059334781000, "dur": 0, "relative_dur": 0.0, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "0 us"}, "res_name": "aten::zero_"}}, "id": "SotYWN3UGLvfR6xI", "pretty_name": "aten::zero_", "trace_file": "/results/Transformer/Transformer.338.pt.trace.json", "trace_disk_size": "94 Bytes", "runtime": 0, "runtime_str": "0 us", "start_timestamp": "23:14:19.334.334781", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 1}], "id": "anroROdH1oB3ur2q", "pretty_name": "aten::empty", "trace_file": "/results/Transformer/Transformer.337.pt.trace.json", "trace_disk_size": "187 Bytes", "runtime": 8000, "runtime_str": "8 us", "start_timestamp": "23:14:19.334.334781", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 2}, {"name": "aten::ones_like", "type": "generated", "generated_depth": 2, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1678058059334809000, "dur": 27000, "relative_dur": 0.4576271186440678, "relative_gap_to_previous": 0.3389830508474576, "parent_is_longest": true, "runtime_str": "27 us"}}, "gpu3": {"time": {"ts": 1678058059482878000, "dur": 2000, "relative_dur": 0.03389830508474576, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "2 us"}}}, "ops": [{"name": "aten::empty_like", "type": "generated", "generated_depth": 3, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1678058059334810000, "dur": 9000, "relative_dur": 0.3333333333333333, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "9 us"}, "res_name": "aten::empty_like"}}, "id": "n21Dv5wG9qli8kCc", "pretty_name": "aten::empty_like", "trace_file": "/results/Transformer/Transformer.340.pt.trace.json", "trace_disk_size": "200 Bytes", "runtime": 9000, "runtime_str": "9 us", "start_timestamp": "23:14:19.334.334810", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 2}, {"name": "aten::fill_", "type": "generated", "generated_depth": 3, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1678058059334820000, "dur": 15000, "relative_dur": 0.5555555555555556, "relative_gap_to_previous": 0.037037037037037035, "parent_is_longest": true, "runtime_str": "15 us"}, "res_name": "aten::fill_"}, "gpu3": {"time": {"ts": 1678058059482878000, "dur": 2000, "relative_dur": 0.07407407407407407, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "2 us"}, "res_name": "vectorized_elementwise_kernel<4 FillFunctor Array<char 1> FillFunctor Array<char 1>"}}, "id": "Wflj2P2fMIGA4WSw", "pretty_name": "aten::fill_", "trace_file": "/results/Transformer/Transformer.341.pt.trace.json", "trace_disk_size": "458 Bytes", "runtime": 15000, "runtime_str": "15 us", "start_timestamp": "23:14:19.334.334820", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 3}], "id": "Tqptsr50yuVPuklF", "pretty_name": "aten::ones_like", "trace_file": "/results/Transformer/Transformer.339.pt.trace.json", "trace_disk_size": "755 Bytes", "runtime": 27000, "runtime_str": "27 us", "start_timestamp": "23:14:19.334.334809", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 6}], "id": "IpDHH9mJwjFvZlhy", "pretty_name": "aten::ones_like(84%) and 3 others\u2026", "trace_file": "/results/Transformer/Transformer.335.pt.trace.json", "trace_disk_size": "1.1 kB", "runtime": 59000, "runtime_str": "59 us", "start_timestamp": "23:14:19.334.334777", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 10}], "idx": 323, "id": "JCzxu6gvxz5J5AQ7", "pretty_name": "Zero Grad", "trace_file": "/results/Transformer/Transformer.333.pt.trace.json", "trace_disk_size": "103.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "runtime": 2883000, "runtime_str": "2.88 ms", "start_timestamp": "23:14:19.331.331953", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 752}, {"name": "Backward", "type": "training loop", "instances": 1061, "resources": {"cpu2": {"time": {"ts": 1678058059902969000, "dur": 254000, "relative_dur": 0.048489302384032325, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "254 us"}}, "cpu4": {"time": {"ts": 1678058059335228000, "dur": 35498000, "relative_dur": 0.048489302384032325, "relative_gap_to_previous": 0, "parent_is_longest": false, "runtime_str": "35.5 ms"}}, "gpu3": {"time": {"ts": 1678058059482880000, "dur": 420837000, "relative_dur": 0.5748518943993749, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "421 ms"}}}, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 157, in update_params\n    with hotline.annotate('Backward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "source_file_num": 157, "ops": [{"name": "Decoder", "type": "Decoder", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 212, in forward\n    with hotline.annotate('Decoder'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 212, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 326, in forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 326, "resources": {"cpu4": {"time": {"ts": 1678058059335228000, "dur": 895000, "relative_dur": 0.003235087871492912, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "895 us"}}, "gpu3": {"time": {"ts": 1678058059482880000, "dur": 67709000, "relative_dur": 0.244742530380909, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "67.7 ms"}}}, "is_backward_op": true, "id": "uAK8DB4GlRIQ9mx1", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.344.pt.trace.json", "trace_disk_size": "19.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 67709000, "runtime_str": "67.7 ms", "start_timestamp": "23:14:19.335.335228", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 135}, {"name": "TransformerDecoder", "type": "TransformerDecoder", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 312, in forward\n    with hotline.annotate('TransformerDecoder'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 312, "ops": [{"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 604, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 604, "resources": {"cpu4": {"time": {"ts": 1678058059336149000, "dur": 80000, "relative_dur": 0.000386225275547595, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "80 us"}}, "gpu3": {"time": {"ts": 1678058059550590000, "dur": 250000, "relative_dur": 0.0012069539860862343, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "250 us"}}}, "is_backward_op": true, "id": "FutM9eSU8ZjWcE50", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.346.pt.trace.json", "trace_disk_size": "2.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 250000, "runtime_str": "250 us", "start_timestamp": "23:14:19.336.336149", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"name": "Layer6", "type": "Layer6", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu4": {"time": {"ts": 1678058059336239000, "dur": 86000, "relative_dur": 0.006859695301906357, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "86 us"}}, "gpu3": {"time": {"ts": 1678058059550842000, "dur": 96000, "relative_dur": 0.00765733429050012, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "JTa6yfjnwpn5TlM3", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.349.pt.trace.json", "trace_disk_size": "2.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "23:14:19.336.336239", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu4": {"time": {"ts": 1678058059336329000, "dur": 149000, "relative_dur": 0.011884820930047061, "relative_gap_to_previous": 0.00031905559543750497, "parent_is_longest": true, "runtime_str": "149 us"}}, "gpu3": {"time": {"ts": 1678058059550939000, "dur": 5663000, "relative_dur": 0.4517029592406477, "relative_gap_to_previous": 7.976389885937624e-05, "parent_is_longest": true, "runtime_str": "5.66 ms"}}}, "is_backward_op": true, "id": "IWfvJGMCVWqrjsYZ", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.350.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5663000, "runtime_str": "5.66 ms", "start_timestamp": "23:14:19.336.336329", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu4": {"time": {"ts": 1678058059336486000, "dur": 99000, "relative_dur": 0.007896625987078248, "relative_gap_to_previous": 0.0006381111908750099, "parent_is_longest": true, "runtime_str": "99 us"}}, "gpu3": {"time": {"ts": 1678058059556604000, "dur": 422000, "relative_dur": 0.03366036531865678, "relative_gap_to_previous": 0.00015952779771875248, "parent_is_longest": true, "runtime_str": "422 us"}}}, "is_backward_op": true, "id": "NwKLydQE2K2WqzsZ", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.351.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 422000, "runtime_str": "422 us", "start_timestamp": "23:14:19.336.336486", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu4": {"time": {"ts": 1678058059336589000, "dur": 28000, "relative_dur": 0.002233389168062535, "relative_gap_to_previous": 0.00031905559543750497, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1678058059557027000, "dur": 476000, "relative_dur": 0.03796761585706309, "relative_gap_to_previous": 7.976389885937624e-05, "parent_is_longest": true, "runtime_str": "476 us"}}}, "is_backward_op": true, "id": "JpOJA0rsOzCNz3yP", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.352.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 476000, "runtime_str": "476 us", "start_timestamp": "23:14:19.336.336589", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu4": {"time": {"ts": 1678058059336621000, "dur": 149000, "relative_dur": 0.011884820930047061, "relative_gap_to_previous": 0.00031905559543750497, "parent_is_longest": true, "runtime_str": "149 us"}}, "gpu3": {"time": {"ts": 1678058059557505000, "dur": 5874000, "relative_dur": 0.4685331418999761, "relative_gap_to_previous": 0.00015952779771875248, "parent_is_longest": true, "runtime_str": "5.87 ms"}}}, "is_backward_op": true, "id": "gaa0SQShNqff7PRt", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.353.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5874000, "runtime_str": "5.87 ms", "start_timestamp": "23:14:19.336.336621", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1678058059336239000, "dur": 531000, "relative_dur": 0.015579157375894848, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "531 us"}}, "gpu3": {"time": {"ts": 1678058059550842000, "dur": 12537000, "relative_dur": 0.3678265461800258, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12.5 ms"}}}, "is_backward_op": true, "id": "q90xGWF539ag4LYQ", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.348.pt.trace.json", "trace_disk_size": "17.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12537000, "runtime_str": "12.5 ms", "start_timestamp": "23:14:19.336.336239", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 134}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu4": {"time": {"ts": 1678058059336777000, "dur": 140000, "relative_dur": 0.004107499119821618, "relative_gap_to_previous": 0.00020537495599108087, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1678058059563380000, "dur": 433000, "relative_dur": 0.012703907992019717, "relative_gap_to_previous": 2.9339279427297264e-05, "parent_is_longest": true, "runtime_str": "433 us"}}}, "is_backward_op": true, "id": "n0sGZPyperOS0KMi", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.354.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 433000, "runtime_str": "433 us", "start_timestamp": "23:14:19.336.336777", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu4": {"time": {"ts": 1678058059336923000, "dur": 78000, "relative_dur": 0.0022884637953291868, "relative_gap_to_previous": 0.0001760356765637836, "parent_is_longest": true, "runtime_str": "78 us"}}, "gpu3": {"time": {"ts": 1678058059563814000, "dur": 96000, "relative_dur": 0.0028165708250205376, "relative_gap_to_previous": 2.9339279427297264e-05, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "242KmxyEKOHCumSM", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.355.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "23:14:19.336.336923", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059337005000, "dur": 145000, "relative_dur": 0.014074936905455252, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "145 us"}}, "gpu3": {"time": {"ts": 1678058059563912000, "dur": 1758000, "relative_dur": 0.17064647641234712, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.76 ms"}}}, "is_backward_op": true, "id": "FE3o3Z3RKsQ4WUnF", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.357.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1758000, "runtime_str": "1.76 ms", "start_timestamp": "23:14:19.337.337005", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059337156000, "dur": 148000, "relative_dur": 0.014366142496602601, "relative_gap_to_previous": 0.0005824111822947001, "parent_is_longest": true, "runtime_str": "148 us"}}, "gpu3": {"time": {"ts": 1678058059565670000, "dur": 814000, "relative_dur": 0.0790137837313143, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "814 us"}}}, "is_backward_op": true, "id": "zVNEyNdjdnr2uk5J", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.358.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 814000, "runtime_str": "814 us", "start_timestamp": "23:14:19.337.337156", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059337309000, "dur": 28000, "relative_dur": 0.0027179188507086, "relative_gap_to_previous": 0.00048534265191225003, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1678058059566485000, "dur": 360000, "relative_dur": 0.034944670937682006, "relative_gap_to_previous": 9.706853038245001e-05, "parent_is_longest": true, "runtime_str": "360 us"}}}, "is_backward_op": true, "id": "6Dd9os8Wg7yEZDzL", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.359.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "23:14:19.337.337309", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059337342000, "dur": 41000, "relative_dur": 0.0039798097456804505, "relative_gap_to_previous": 0.00048534265191225003, "parent_is_longest": true, "runtime_str": "41 us"}}, "gpu3": {"time": {"ts": 1678058059566847000, "dur": 951000, "relative_dur": 0.09231217239370997, "relative_gap_to_previous": 0.00019413706076490002, "parent_is_longest": true, "runtime_str": "951 us"}}}, "is_backward_op": true, "id": "5cWFiCkXKiTxSOfU", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.360.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 951000, "runtime_str": "951 us", "start_timestamp": "23:14:19.337.337342", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059337388000, "dur": 70000, "relative_dur": 0.0067947971267715, "relative_gap_to_previous": 0.00048534265191225003, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1678058059567799000, "dur": 865000, "relative_dur": 0.08396427878081926, "relative_gap_to_previous": 9.706853038245001e-05, "parent_is_longest": true, "runtime_str": "865 us"}}}, "is_backward_op": true, "id": "CoEAvSUWzNFzudV8", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.361.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 865000, "runtime_str": "865 us", "start_timestamp": "23:14:19.337.337388", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059337463000, "dur": 36000, "relative_dur": 0.0034944670937682005, "relative_gap_to_previous": 0.00048534265191225003, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1678058059568665000, "dur": 80000, "relative_dur": 0.0077654824305960005, "relative_gap_to_previous": 9.706853038245001e-05, "parent_is_longest": true, "runtime_str": "80 us"}}}, "is_backward_op": true, "id": "4jXsw30OASFIbSAi", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.362.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 80000, "runtime_str": "80 us", "start_timestamp": "23:14:19.337.337463", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059337503000, "dur": 213000, "relative_dur": 0.020675596971461853, "relative_gap_to_previous": 0.00038827412152980003, "parent_is_longest": true, "runtime_str": "213 us"}}, "gpu3": {"time": {"ts": 1678058059568747000, "dur": 1834000, "relative_dur": 0.1780236847214133, "relative_gap_to_previous": 0.00019413706076490002, "parent_is_longest": true, "runtime_str": "1.83 ms"}}}, "is_backward_op": true, "id": "oANldMbt2V2cmqez", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.363.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1834000, "runtime_str": "1.83 ms", "start_timestamp": "23:14:19.337.337503", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059337722000, "dur": 181000, "relative_dur": 0.01756940399922345, "relative_gap_to_previous": 0.0005824111822947001, "parent_is_longest": true, "runtime_str": "181 us"}}, "gpu3": {"time": {"ts": 1678058059570583000, "dur": 1848000, "relative_dur": 0.1793826441467676, "relative_gap_to_previous": 0.00019413706076490002, "parent_is_longest": true, "runtime_str": "1.85 ms"}}}, "is_backward_op": true, "id": "xG2JzhB9Xevp06fh", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.364.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1848000, "runtime_str": "1.85 ms", "start_timestamp": "23:14:19.337.337722", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059337909000, "dur": 176000, "relative_dur": 0.0170840613473112, "relative_gap_to_previous": 0.0005824111822947001, "parent_is_longest": true, "runtime_str": "176 us"}}, "gpu3": {"time": {"ts": 1678058059572432000, "dur": 1782000, "relative_dur": 0.1729761211415259, "relative_gap_to_previous": 9.706853038245001e-05, "parent_is_longest": true, "runtime_str": "1.78 ms"}}}, "is_backward_op": true, "id": "4CjfVeuHjJExun89", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.365.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1782000, "runtime_str": "1.78 ms", "start_timestamp": "23:14:19.337.337909", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059337005000, "dur": 1080000, "relative_dur": 0.031686421781481046, "relative_gap_to_previous": 0.00011735711770918906, "parent_is_longest": true, "runtime_str": "1.08 ms"}}, "gpu3": {"time": {"ts": 1678058059563912000, "dur": 10302000, "relative_dur": 0.30225325666001646, "relative_gap_to_previous": 5.867855885459453e-05, "parent_is_longest": true, "runtime_str": "10.3 ms"}}}, "is_backward_op": true, "id": "urCntF0t3yShkOCJ", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.356.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10302000, "runtime_str": "10.3 ms", "start_timestamp": "23:14:19.337.337005", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu4": {"time": {"ts": 1678058059338090000, "dur": 147000, "relative_dur": 0.004312874075812698, "relative_gap_to_previous": 0.00014669639713648633, "parent_is_longest": true, "runtime_str": "147 us"}}, "gpu3": {"time": {"ts": 1678058059574215000, "dur": 393000, "relative_dur": 0.011530336814927825, "relative_gap_to_previous": 2.9339279427297264e-05, "parent_is_longest": true, "runtime_str": "393 us"}}}, "is_backward_op": true, "id": "4dMFa5aQr27sh0Y5", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.366.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 393000, "runtime_str": "393 us", "start_timestamp": "23:14:19.338.338090", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 37}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu4": {"time": {"ts": 1678058059338243000, "dur": 77000, "relative_dur": 0.0022591245159018893, "relative_gap_to_previous": 0.0001760356765637836, "parent_is_longest": true, "runtime_str": "77 us"}}, "gpu3": {"time": {"ts": 1678058059574609000, "dur": 96000, "relative_dur": 0.0028165708250205376, "relative_gap_to_previous": 2.9339279427297264e-05, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "nTVQQH6rSPJ7AViU", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.367.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "23:14:19.338.338243", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059338325000, "dur": 137000, "relative_dur": 0.013406399843428908, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "137 us"}}, "gpu3": {"time": {"ts": 1678058059574707000, "dur": 1727000, "relative_dur": 0.1689989235737352, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.73 ms"}}}, "is_backward_op": true, "id": "Y7WwzJ42wVGjhBDG", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.369.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1727000, "runtime_str": "1.73 ms", "start_timestamp": "23:14:19.338.338325", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059338468000, "dur": 139000, "relative_dur": 0.013602113709756336, "relative_gap_to_previous": 0.0005871415989822879, "parent_is_longest": true, "runtime_str": "139 us"}}, "gpu3": {"time": {"ts": 1678058059576435000, "dur": 795000, "relative_dur": 0.07779626186515315, "relative_gap_to_previous": 9.785693316371466e-05, "parent_is_longest": true, "runtime_str": "795 us"}}}, "is_backward_op": true, "id": "tV2HwWXfq2N2zd7t", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.370.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 795000, "runtime_str": "795 us", "start_timestamp": "23:14:19.338.338468", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059338613000, "dur": 28000, "relative_dur": 0.00273999412858401, "relative_gap_to_previous": 0.0005871415989822879, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1678058059577231000, "dur": 359000, "relative_dur": 0.03513063900577356, "relative_gap_to_previous": 9.785693316371466e-05, "parent_is_longest": true, "runtime_str": "359 us"}}}, "is_backward_op": true, "id": "gtykOexthPsaxp2Z", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.371.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "23:14:19.338.338613", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059338645000, "dur": 41000, "relative_dur": 0.004012134259712301, "relative_gap_to_previous": 0.0003914277326548586, "parent_is_longest": true, "runtime_str": "41 us"}}, "gpu3": {"time": {"ts": 1678058059577592000, "dur": 950000, "relative_dur": 0.09296408650552891, "relative_gap_to_previous": 0.0001957138663274293, "parent_is_longest": true, "runtime_str": "950 us"}}}, "is_backward_op": true, "id": "DdWfzEOcFHVSA1Yo", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.372.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 950000, "runtime_str": "950 us", "start_timestamp": "23:14:19.338.338645", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059338691000, "dur": 104000, "relative_dur": 0.010177121049026324, "relative_gap_to_previous": 0.0004892846658185732, "parent_is_longest": true, "runtime_str": "104 us"}}, "gpu3": {"time": {"ts": 1678058059578544000, "dur": 850000, "relative_dur": 0.08317839318915746, "relative_gap_to_previous": 0.0001957138663274293, "parent_is_longest": true, "runtime_str": "850 us"}}}, "is_backward_op": true, "id": "jSTjheuz1Q8pHrtI", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.373.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 850000, "runtime_str": "850 us", "start_timestamp": "23:14:19.338.338691", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059338801000, "dur": 37000, "relative_dur": 0.003620706527057442, "relative_gap_to_previous": 0.0005871415989822879, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1678058059579395000, "dur": 81000, "relative_dur": 0.007926411586260887, "relative_gap_to_previous": 9.785693316371466e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "PrAfAFxqWXmj1l9X", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.374.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "23:14:19.338.338801", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059338842000, "dur": 214000, "relative_dur": 0.020941383697034934, "relative_gap_to_previous": 0.0003914277326548586, "parent_is_longest": true, "runtime_str": "214 us"}}, "gpu3": {"time": {"ts": 1678058059579477000, "dur": 1812000, "relative_dur": 0.17731676289265094, "relative_gap_to_previous": 9.785693316371466e-05, "parent_is_longest": true, "runtime_str": "1.81 ms"}}}, "is_backward_op": true, "id": "6uZAciACiDTmWUsD", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.375.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1812000, "runtime_str": "1.81 ms", "start_timestamp": "23:14:19.338.338842", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059339062000, "dur": 179000, "relative_dur": 0.017516391036304924, "relative_gap_to_previous": 0.0005871415989822879, "parent_is_longest": true, "runtime_str": "179 us"}}, "gpu3": {"time": {"ts": 1678058059581290000, "dur": 1852000, "relative_dur": 0.18123104021919953, "relative_gap_to_previous": 9.785693316371466e-05, "parent_is_longest": true, "runtime_str": "1.85 ms"}}}, "is_backward_op": true, "id": "liIrIxW6zuOKPU6V", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.376.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1852000, "runtime_str": "1.85 ms", "start_timestamp": "23:14:19.339.339062", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059339246000, "dur": 173000, "relative_dur": 0.016929249437322633, "relative_gap_to_previous": 0.0004892846658185732, "parent_is_longest": true, "runtime_str": "173 us"}}, "gpu3": {"time": {"ts": 1678058059583143000, "dur": 1783000, "relative_dur": 0.17447891183090322, "relative_gap_to_previous": 9.785693316371466e-05, "parent_is_longest": true, "runtime_str": "1.78 ms"}}}, "is_backward_op": true, "id": "R3iscLvFqUFHIGfO", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.377.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1783000, "runtime_str": "1.78 ms", "start_timestamp": "23:14:19.339.339246", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059338325000, "dur": 1094000, "relative_dur": 0.03209717169346321, "relative_gap_to_previous": 0.00014669639713648633, "parent_is_longest": true, "runtime_str": "1.09 ms"}}, "gpu3": {"time": {"ts": 1678058059574707000, "dur": 10219000, "relative_dur": 0.29981809646755075, "relative_gap_to_previous": 5.867855885459453e-05, "parent_is_longest": true, "runtime_str": "10.2 ms"}}}, "is_backward_op": true, "id": "Ts0iTMmmP7voAFue", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.368.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10219000, "runtime_str": "10.2 ms", "start_timestamp": "23:14:19.338.338325", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}], "resources": {"cpu4": {"time": {"ts": 1678058059336239000, "dur": 3180000, "relative_dur": 0.015352454703016901, "relative_gap_to_previous": 4.827815944344938e-05, "parent_is_longest": true, "runtime_str": "3.18 ms"}}, "gpu3": {"time": {"ts": 1678058059550842000, "dur": 34084000, "relative_dur": 0.16455127864705285, "relative_gap_to_previous": 9.655631888689876e-06, "parent_is_longest": true, "runtime_str": "34.1 ms"}}}, "is_backward_op": true, "id": "3rG6qFL4ZKaNXPyK", "pretty_name": "Layer6", "trace_file": "/results/Transformer/Transformer.347.pt.trace.json", "trace_disk_size": "109.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 34084000, "runtime_str": "34.1 ms", "start_timestamp": "23:14:19.336.336239", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 839}, {"name": "Layer5", "type": "Layer5", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu4": {"time": {"ts": 1678058059339425000, "dur": 270000, "relative_dur": 0.02090106827682304, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "270 us"}}, "gpu3": {"time": {"ts": 1678058059584927000, "dur": 692000, "relative_dur": 0.053568663879857564, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "692 us"}}}, "is_backward_op": true, "id": "tBIEF0Vq4fQhDfDs", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.380.pt.trace.json", "trace_disk_size": "9.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 692000, "runtime_str": "692 us", "start_timestamp": "23:14:19.339.339425", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 62}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu4": {"time": {"ts": 1678058059339699000, "dur": 141000, "relative_dur": 0.01091500232234092, "relative_gap_to_previous": 0.0003096454559529339, "parent_is_longest": true, "runtime_str": "141 us"}}, "gpu3": {"time": {"ts": 1678058059585621000, "dur": 5552000, "relative_dur": 0.4297878928626722, "relative_gap_to_previous": 0.00015482272797646694, "parent_is_longest": true, "runtime_str": "5.55 ms"}}}, "is_backward_op": true, "id": "kQFxF4OqG86aukt3", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.381.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5552000, "runtime_str": "5.55 ms", "start_timestamp": "23:14:19.339.339699", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu4": {"time": {"ts": 1678058059339847000, "dur": 98000, "relative_dur": 0.0075863136708468806, "relative_gap_to_previous": 0.0005418795479176343, "parent_is_longest": true, "runtime_str": "98 us"}}, "gpu3": {"time": {"ts": 1678058059591174000, "dur": 423000, "relative_dur": 0.03274500696702276, "relative_gap_to_previous": 7.741136398823347e-05, "parent_is_longest": true, "runtime_str": "423 us"}}}, "is_backward_op": true, "id": "N7BagHfdQg7oz3sz", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.382.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 423000, "runtime_str": "423 us", "start_timestamp": "23:14:19.339.339847", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu4": {"time": {"ts": 1678058059339949000, "dur": 24000, "relative_dur": 0.0018578727357176034, "relative_gap_to_previous": 0.0003096454559529339, "parent_is_longest": true, "runtime_str": "24 us"}}, "gpu3": {"time": {"ts": 1678058059591599000, "dur": 477000, "relative_dur": 0.036925220622387366, "relative_gap_to_previous": 0.00015482272797646694, "parent_is_longest": true, "runtime_str": "477 us"}}}, "is_backward_op": true, "id": "f1Vww5oMI7vRl36X", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.383.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 477000, "runtime_str": "477 us", "start_timestamp": "23:14:19.339.339949", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu4": {"time": {"ts": 1678058059339977000, "dur": 146000, "relative_dur": 0.011302059142282087, "relative_gap_to_previous": 0.0003096454559529339, "parent_is_longest": true, "runtime_str": "146 us"}}, "gpu3": {"time": {"ts": 1678058059592078000, "dur": 5767000, "relative_dur": 0.44643133612014246, "relative_gap_to_previous": 0.00015482272797646694, "parent_is_longest": true, "runtime_str": "5.77 ms"}}}, "is_backward_op": true, "id": "lxURXAjyVsxBzW2L", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.384.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5767000, "runtime_str": "5.77 ms", "start_timestamp": "23:14:19.339.339977", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1678058059339425000, "dur": 698000, "relative_dur": 0.02022133379685961, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "698 us"}}, "gpu3": {"time": {"ts": 1678058059584927000, "dur": 12918000, "relative_dur": 0.3742395272031983, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12.9 ms"}}}, "is_backward_op": true, "id": "OedQMww6FCHAQmH7", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.379.pt.trace.json", "trace_disk_size": "23.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12918000, "runtime_str": "12.9 ms", "start_timestamp": "23:14:19.339.339425", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 177}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu4": {"time": {"ts": 1678058059340130000, "dur": 138000, "relative_dur": 0.003997914131757344, "relative_gap_to_previous": 0.0002027927458137783, "parent_is_longest": true, "runtime_str": "138 us"}}, "gpu3": {"time": {"ts": 1678058059597846000, "dur": 433000, "relative_dur": 0.012544179848195145, "relative_gap_to_previous": 2.8970392259111188e-05, "parent_is_longest": true, "runtime_str": "433 us"}}}, "is_backward_op": true, "id": "aCmBgLez8wm47FJV", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.385.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 433000, "runtime_str": "433 us", "start_timestamp": "23:14:19.340.340130", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu4": {"time": {"ts": 1678058059340273000, "dur": 78000, "relative_dur": 0.002259690596210673, "relative_gap_to_previous": 0.00014485196129555595, "parent_is_longest": true, "runtime_str": "78 us"}}, "gpu3": {"time": {"ts": 1678058059598280000, "dur": 96000, "relative_dur": 0.0027811576568746743, "relative_gap_to_previous": 2.8970392259111188e-05, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "8GQTFdHbjoGwiOf1", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.386.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "23:14:19.340.340273", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059340355000, "dur": 140000, "relative_dur": 0.013685239491691105, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1678058059598377000, "dur": 1729000, "relative_dur": 0.16901270772238514, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.73 ms"}}}, "is_backward_op": true, "id": "nruoyrkgE425Vun8", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.388.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1729000, "runtime_str": "1.73 ms", "start_timestamp": "23:14:19.340.340355", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059340501000, "dur": 141000, "relative_dur": 0.01378299120234604, "relative_gap_to_previous": 0.0005865102639296188, "parent_is_longest": true, "runtime_str": "141 us"}}, "gpu3": {"time": {"ts": 1678058059600108000, "dur": 796000, "relative_dur": 0.07781036168132942, "relative_gap_to_previous": 0.00019550342130987292, "parent_is_longest": true, "runtime_str": "796 us"}}}, "is_backward_op": true, "id": "D7UVGqrHFjFEk3tU", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.389.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 796000, "runtime_str": "796 us", "start_timestamp": "23:14:19.340.340501", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059340648000, "dur": 27000, "relative_dur": 0.0026392961876832845, "relative_gap_to_previous": 0.0005865102639296188, "parent_is_longest": true, "runtime_str": "27 us"}}, "gpu3": {"time": {"ts": 1678058059600906000, "dur": 360000, "relative_dur": 0.03519061583577713, "relative_gap_to_previous": 0.00019550342130987292, "parent_is_longest": true, "runtime_str": "360 us"}}}, "is_backward_op": true, "id": "kREYdld77BkRAIoC", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.390.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "23:14:19.340.340648", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059340680000, "dur": 38000, "relative_dur": 0.0037145650048875855, "relative_gap_to_previous": 0.0004887585532746823, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1678058059601267000, "dur": 952000, "relative_dur": 0.09305962854349951, "relative_gap_to_previous": 9.775171065493646e-05, "parent_is_longest": true, "runtime_str": "952 us"}}}, "is_backward_op": true, "id": "ohPImYxcXMa8wR7i", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.391.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "23:14:19.340.340680", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059340722000, "dur": 70000, "relative_dur": 0.006842619745845552, "relative_gap_to_previous": 0.00039100684261974585, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1678058059602221000, "dur": 849000, "relative_dur": 0.08299120234604106, "relative_gap_to_previous": 0.00019550342130987292, "parent_is_longest": true, "runtime_str": "849 us"}}}, "is_backward_op": true, "id": "oq0UvMCs4o8rEhzI", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.392.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 849000, "runtime_str": "849 us", "start_timestamp": "23:14:19.340.340722", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059340797000, "dur": 36000, "relative_dur": 0.0035190615835777126, "relative_gap_to_previous": 0.0004887585532746823, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1678058059603071000, "dur": 81000, "relative_dur": 0.007917888563049853, "relative_gap_to_previous": 9.775171065493646e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "OjNFmNrhgwKT4m8s", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.393.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "23:14:19.340.340797", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059340837000, "dur": 212000, "relative_dur": 0.02072336265884653, "relative_gap_to_previous": 0.00039100684261974585, "parent_is_longest": true, "runtime_str": "212 us"}}, "gpu3": {"time": {"ts": 1678058059603154000, "dur": 1813000, "relative_dur": 0.1772238514173998, "relative_gap_to_previous": 0.00019550342130987292, "parent_is_longest": true, "runtime_str": "1.81 ms"}}}, "is_backward_op": true, "id": "Fb5AfHHObJCdlF1T", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.394.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1813000, "runtime_str": "1.81 ms", "start_timestamp": "23:14:19.340.340837", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059341055000, "dur": 180000, "relative_dur": 0.017595307917888565, "relative_gap_to_previous": 0.0005865102639296188, "parent_is_longest": true, "runtime_str": "180 us"}}, "gpu3": {"time": {"ts": 1678058059604968000, "dur": 1853000, "relative_dur": 0.18113391984359725, "relative_gap_to_previous": 9.775171065493646e-05, "parent_is_longest": true, "runtime_str": "1.85 ms"}}}, "is_backward_op": true, "id": "30N2TySaz4PWKnky", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.395.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1853000, "runtime_str": "1.85 ms", "start_timestamp": "23:14:19.341.341055", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059341241000, "dur": 174000, "relative_dur": 0.017008797653958945, "relative_gap_to_previous": 0.0005865102639296188, "parent_is_longest": true, "runtime_str": "174 us"}}, "gpu3": {"time": {"ts": 1678058059606822000, "dur": 1785000, "relative_dur": 0.1744868035190616, "relative_gap_to_previous": 9.775171065493646e-05, "parent_is_longest": true, "runtime_str": "1.78 ms"}}}, "is_backward_op": true, "id": "mmw5m2BzyEVtdrcW", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.396.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1785000, "runtime_str": "1.78 ms", "start_timestamp": "23:14:19.341.341241", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059340355000, "dur": 1060000, "relative_dur": 0.03070861579465786, "relative_gap_to_previous": 0.00011588156903644475, "parent_is_longest": true, "runtime_str": "1.06 ms"}}, "gpu3": {"time": {"ts": 1678058059598377000, "dur": 10230000, "relative_dur": 0.29636711281070743, "relative_gap_to_previous": 2.8970392259111188e-05, "parent_is_longest": true, "runtime_str": "10.2 ms"}}}, "is_backward_op": true, "id": "eR9iMiqYMZVITmSh", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.387.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10230000, "runtime_str": "10.2 ms", "start_timestamp": "23:14:19.340.340355", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu4": {"time": {"ts": 1678058059341421000, "dur": 165000, "relative_dur": 0.004780114722753346, "relative_gap_to_previous": 0.00017382235355466714, "parent_is_longest": true, "runtime_str": "165 us"}}, "gpu3": {"time": {"ts": 1678058059608609000, "dur": 515000, "relative_dur": 0.014919752013442263, "relative_gap_to_previous": 5.7940784518222376e-05, "parent_is_longest": true, "runtime_str": "515 us"}}}, "is_backward_op": true, "id": "lCT3lYY8DTLwLu6h", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.397.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 515000, "runtime_str": "515 us", "start_timestamp": "23:14:19.341.341421", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu4": {"time": {"ts": 1678058059341592000, "dur": 83000, "relative_dur": 0.0024045425575062286, "relative_gap_to_previous": 0.00017382235355466714, "parent_is_longest": true, "runtime_str": "83 us"}}, "gpu3": {"time": {"ts": 1678058059609125000, "dur": 96000, "relative_dur": 0.0027811576568746743, "relative_gap_to_previous": 2.8970392259111188e-05, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "ZMFQI9niNTzrMTVt", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.398.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "23:14:19.341.341592", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059341679000, "dur": 160000, "relative_dur": 0.01565251418509098, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "160 us"}}, "gpu3": {"time": {"ts": 1678058059609223000, "dur": 1726000, "relative_dur": 0.16885149677166894, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.73 ms"}}}, "is_backward_op": true, "id": "uhwu9zV6TGVzo1Lk", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.400.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1726000, "runtime_str": "1.73 ms", "start_timestamp": "23:14:19.341.341679", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059341845000, "dur": 139000, "relative_dur": 0.013598121698297789, "relative_gap_to_previous": 0.0005869692819409117, "parent_is_longest": true, "runtime_str": "139 us"}}, "gpu3": {"time": {"ts": 1678058059610950000, "dur": 799000, "relative_dur": 0.07816474271179809, "relative_gap_to_previous": 9.782821365681862e-05, "parent_is_longest": true, "runtime_str": "799 us"}}}, "is_backward_op": true, "id": "KXSnidgmzhsw6i6f", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.401.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 799000, "runtime_str": "799 us", "start_timestamp": "23:14:19.341.341845", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059341989000, "dur": 28000, "relative_dur": 0.0027391899823909216, "relative_gap_to_previous": 0.0004891410682840931, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1678058059611751000, "dur": 360000, "relative_dur": 0.035218156916454704, "relative_gap_to_previous": 0.00019565642731363725, "parent_is_longest": true, "runtime_str": "360 us"}}}, "is_backward_op": true, "id": "8ju2eIf86h4BS7Oy", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.402.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "23:14:19.341.341989", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059342022000, "dur": 38000, "relative_dur": 0.0037174721189591076, "relative_gap_to_previous": 0.0004891410682840931, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1678058059612112000, "dur": 951000, "relative_dur": 0.09303463118763451, "relative_gap_to_previous": 9.782821365681862e-05, "parent_is_longest": true, "runtime_str": "951 us"}}}, "is_backward_op": true, "id": "0C9X5xoLK31fg9sq", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.403.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 951000, "runtime_str": "951 us", "start_timestamp": "23:14:19.342.342022", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059342064000, "dur": 70000, "relative_dur": 0.006847974955977304, "relative_gap_to_previous": 0.0003913128546272745, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1678058059613065000, "dur": 850000, "relative_dur": 0.08315398160829583, "relative_gap_to_previous": 0.00019565642731363725, "parent_is_longest": true, "runtime_str": "850 us"}}}, "is_backward_op": true, "id": "NoF3H3LYZKO5dv9r", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.404.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 850000, "runtime_str": "850 us", "start_timestamp": "23:14:19.342.342064", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059342140000, "dur": 35000, "relative_dur": 0.003423987477988652, "relative_gap_to_previous": 0.0005869692819409117, "parent_is_longest": true, "runtime_str": "35 us"}}, "gpu3": {"time": {"ts": 1678058059613916000, "dur": 81000, "relative_dur": 0.00792408530620231, "relative_gap_to_previous": 9.782821365681862e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "xfVsYuno1IrUhEdi", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.405.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "23:14:19.342.342140", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059342179000, "dur": 209000, "relative_dur": 0.020446096654275093, "relative_gap_to_previous": 0.0003913128546272745, "parent_is_longest": true, "runtime_str": "209 us"}}, "gpu3": {"time": {"ts": 1678058059613999000, "dur": 1813000, "relative_dur": 0.17736255135981216, "relative_gap_to_previous": 0.00019565642731363725, "parent_is_longest": true, "runtime_str": "1.81 ms"}}}, "is_backward_op": true, "id": "N12sq3PKnuFDbFJY", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.406.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1813000, "runtime_str": "1.81 ms", "start_timestamp": "23:14:19.342.342179", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059342394000, "dur": 178000, "relative_dur": 0.017413422030913714, "relative_gap_to_previous": 0.0005869692819409117, "parent_is_longest": true, "runtime_str": "178 us"}}, "gpu3": {"time": {"ts": 1678058059615812000, "dur": 1849000, "relative_dur": 0.18088436705145763, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "1.85 ms"}}}, "is_backward_op": true, "id": "yyFuBHuk3xaZWnc1", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.407.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1849000, "runtime_str": "1.85 ms", "start_timestamp": "23:14:19.342.342394", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059342578000, "dur": 173000, "relative_dur": 0.01692428096262962, "relative_gap_to_previous": 0.0005869692819409117, "parent_is_longest": true, "runtime_str": "173 us"}}, "gpu3": {"time": {"ts": 1678058059617662000, "dur": 1783000, "relative_dur": 0.17442770495010762, "relative_gap_to_previous": 9.782821365681862e-05, "parent_is_longest": true, "runtime_str": "1.78 ms"}}}, "is_backward_op": true, "id": "N7bRheclgIIj7O0c", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.408.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1783000, "runtime_str": "1.78 ms", "start_timestamp": "23:14:19.342.342578", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059341679000, "dur": 1072000, "relative_dur": 0.031056260501767192, "relative_gap_to_previous": 0.00011588156903644475, "parent_is_longest": true, "runtime_str": "1.07 ms"}}, "gpu3": {"time": {"ts": 1678058059609223000, "dur": 10222000, "relative_dur": 0.29613534967263455, "relative_gap_to_previous": 5.7940784518222376e-05, "parent_is_longest": true, "runtime_str": "10.2 ms"}}}, "is_backward_op": true, "id": "PwwEgedTxYghmGRM", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.399.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10222000, "runtime_str": "10.2 ms", "start_timestamp": "23:14:19.341.341679", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}], "resources": {"cpu4": {"time": {"ts": 1678058059339425000, "dur": 3326000, "relative_dur": 0.016057315830891265, "relative_gap_to_previous": 2.896689566606963e-05, "parent_is_longest": true, "runtime_str": "3.33 ms"}}, "gpu3": {"time": {"ts": 1678058059584927000, "dur": 34518000, "relative_dur": 0.16664655076689855, "relative_gap_to_previous": 4.827815944344938e-06, "parent_is_longest": true, "runtime_str": "34.5 ms"}}}, "is_backward_op": true, "id": "Fj2O6Y0H1u3wYvef", "pretty_name": "Layer5", "trace_file": "/results/Transformer/Transformer.378.pt.trace.json", "trace_disk_size": "116.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 34518000, "runtime_str": "34.5 ms", "start_timestamp": "23:14:19.339.339425", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 885}, {"name": "Layer4", "type": "Layer4", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu4": {"time": {"ts": 1678058059342757000, "dur": 263000, "relative_dur": 0.020321434090557873, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "263 us"}}, "gpu3": {"time": {"ts": 1678058059619446000, "dur": 701000, "relative_dur": 0.05416473497141091, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "701 us"}}}, "is_backward_op": true, "id": "M00xcpcxNleH53rr", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.411.pt.trace.json", "trace_disk_size": "9.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 701000, "runtime_str": "701 us", "start_timestamp": "23:14:19.342.342757", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 62}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu4": {"time": {"ts": 1678058059343024000, "dur": 139000, "relative_dur": 0.010740225622005873, "relative_gap_to_previous": 0.0003090712409210323, "parent_is_longest": true, "runtime_str": "139 us"}}, "gpu3": {"time": {"ts": 1678058059620149000, "dur": 5555000, "relative_dur": 0.42922268582908363, "relative_gap_to_previous": 0.00015453562046051616, "parent_is_longest": true, "runtime_str": "5.55 ms"}}}, "is_backward_op": true, "id": "UYCKY5sGLGenhqMZ", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.412.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5555000, "runtime_str": "5.55 ms", "start_timestamp": "23:14:19.343.343024", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu4": {"time": {"ts": 1678058059343170000, "dur": 98000, "relative_dur": 0.007572245402565291, "relative_gap_to_previous": 0.0005408746716118065, "parent_is_longest": true, "runtime_str": "98 us"}}, "gpu3": {"time": {"ts": 1678058059625706000, "dur": 421000, "relative_dur": 0.03252974810693865, "relative_gap_to_previous": 0.00015453562046051616, "parent_is_longest": true, "runtime_str": "421 us"}}}, "is_backward_op": true, "id": "uzUpgb90E0MsIS18", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.413.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 421000, "runtime_str": "421 us", "start_timestamp": "23:14:19.343.343170", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu4": {"time": {"ts": 1678058059343272000, "dur": 23000, "relative_dur": 0.0017771596352959358, "relative_gap_to_previous": 0.0003090712409210323, "parent_is_longest": true, "runtime_str": "23 us"}}, "gpu3": {"time": {"ts": 1678058059626129000, "dur": 476000, "relative_dur": 0.03677947766960284, "relative_gap_to_previous": 0.00015453562046051616, "parent_is_longest": true, "runtime_str": "476 us"}}}, "is_backward_op": true, "id": "bAyCU7XJdINZs6Dr", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.414.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 476000, "runtime_str": "476 us", "start_timestamp": "23:14:19.343.343272", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu4": {"time": {"ts": 1678058059343300000, "dur": 146000, "relative_dur": 0.01128110029361768, "relative_gap_to_previous": 0.00038633905115129037, "parent_is_longest": true, "runtime_str": "146 us"}}, "gpu3": {"time": {"ts": 1678058059626606000, "dur": 5782000, "relative_dur": 0.44676247875135217, "relative_gap_to_previous": 7.726781023025808e-05, "parent_is_longest": true, "runtime_str": "5.78 ms"}}}, "is_backward_op": true, "id": "Shhs8XTQ3ClKIK0q", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.415.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5782000, "runtime_str": "5.78 ms", "start_timestamp": "23:14:19.343.343300", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1678058059342757000, "dur": 689000, "relative_dur": 0.01988226467363075, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "689 us"}}, "gpu3": {"time": {"ts": 1678058059619446000, "dur": 12942000, "relative_dur": 0.37346338085069547, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12.9 ms"}}}, "is_backward_op": true, "id": "iRCQnr8VCiyh72VE", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.410.pt.trace.json", "trace_disk_size": "23.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12942000, "runtime_str": "12.9 ms", "start_timestamp": "23:14:19.342.342757", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 177}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu4": {"time": {"ts": 1678058059343453000, "dur": 138000, "relative_dur": 0.003982224274254055, "relative_gap_to_previous": 0.00020199688347665492, "parent_is_longest": true, "runtime_str": "138 us"}}, "gpu3": {"time": {"ts": 1678058059632388000, "dur": 438000, "relative_dur": 0.012639233566110694, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "438 us"}}}, "is_backward_op": true, "id": "UXNguWEkGdzlatT1", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.416.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 438000, "runtime_str": "438 us", "start_timestamp": "23:14:19.343.343453", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu4": {"time": {"ts": 1678058059343596000, "dur": 79000, "relative_dur": 0.0022796791135222486, "relative_gap_to_previous": 0.00014428348819761066, "parent_is_longest": true, "runtime_str": "79 us"}}, "gpu3": {"time": {"ts": 1678058059632828000, "dur": 95000, "relative_dur": 0.0027413862757546025, "relative_gap_to_previous": 5.7713395279044266e-05, "parent_is_longest": true, "runtime_str": "95 us"}}}, "is_backward_op": true, "id": "8s8rWwDJ8kOhK3iT", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.417.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 95000, "runtime_str": "95 us", "start_timestamp": "23:14:19.343.343596", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059343679000, "dur": 140000, "relative_dur": 0.01359487279083317, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1678058059632925000, "dur": 1742000, "relative_dur": 0.16915906001165273, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.74 ms"}}}, "is_backward_op": true, "id": "9qPego7WK8iJHS9n", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.419.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1742000, "runtime_str": "1.74 ms", "start_timestamp": "23:14:19.343.343679", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059343825000, "dur": 140000, "relative_dur": 0.01359487279083317, "relative_gap_to_previous": 0.0005826374053214216, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1678058059634668000, "dur": 802000, "relative_dur": 0.07787919984463003, "relative_gap_to_previous": 9.710623422023694e-05, "parent_is_longest": true, "runtime_str": "802 us"}}}, "is_backward_op": true, "id": "R18ConIpKN5VpCO2", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.420.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 802000, "runtime_str": "802 us", "start_timestamp": "23:14:19.343.343825", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059343971000, "dur": 28000, "relative_dur": 0.0027189745581666343, "relative_gap_to_previous": 0.0005826374053214216, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1678058059635471000, "dur": 359000, "relative_dur": 0.03486113808506506, "relative_gap_to_previous": 9.710623422023694e-05, "parent_is_longest": true, "runtime_str": "359 us"}}}, "is_backward_op": true, "id": "yuGF3FmXPZ5r6W9F", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.421.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "23:14:19.343.343971", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059344003000, "dur": 38000, "relative_dur": 0.0036900369003690036, "relative_gap_to_previous": 0.0003884249368809478, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1678058059635832000, "dur": 951000, "relative_dur": 0.09234802874344533, "relative_gap_to_previous": 0.0001942124684404739, "parent_is_longest": true, "runtime_str": "951 us"}}}, "is_backward_op": true, "id": "bm8m0PAZW134A7wH", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.422.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 951000, "runtime_str": "951 us", "start_timestamp": "23:14:19.344.344003", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059344046000, "dur": 70000, "relative_dur": 0.006797436395416585, "relative_gap_to_previous": 0.0004855311711011847, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1678058059636784000, "dur": 858000, "relative_dur": 0.08331714896096329, "relative_gap_to_previous": 9.710623422023694e-05, "parent_is_longest": true, "runtime_str": "858 us"}}}, "is_backward_op": true, "id": "8xXJ8Q3XQF0qHf90", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.423.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 858000, "runtime_str": "858 us", "start_timestamp": "23:14:19.344.344046", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059344121000, "dur": 36000, "relative_dur": 0.0034958244319285296, "relative_gap_to_previous": 0.0004855311711011847, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1678058059637643000, "dur": 81000, "relative_dur": 0.007865604971839192, "relative_gap_to_previous": 9.710623422023694e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "hPd1QV0ioI3kPTSA", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.424.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "23:14:19.344.344121", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059344161000, "dur": 212000, "relative_dur": 0.020586521654690233, "relative_gap_to_previous": 0.0003884249368809478, "parent_is_longest": true, "runtime_str": "212 us"}}, "gpu3": {"time": {"ts": 1678058059637725000, "dur": 1828000, "relative_dur": 0.17751019615459313, "relative_gap_to_previous": 9.710623422023694e-05, "parent_is_longest": true, "runtime_str": "1.83 ms"}}}, "is_backward_op": true, "id": "erb5KWKD3P79tv2a", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.425.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1828000, "runtime_str": "1.83 ms", "start_timestamp": "23:14:19.344.344161", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059344379000, "dur": 179000, "relative_dur": 0.017382015925422413, "relative_gap_to_previous": 0.0005826374053214216, "parent_is_longest": true, "runtime_str": "179 us"}}, "gpu3": {"time": {"ts": 1678058059639554000, "dur": 1869000, "relative_dur": 0.18149155175762283, "relative_gap_to_previous": 9.710623422023694e-05, "parent_is_longest": true, "runtime_str": "1.87 ms"}}}, "is_backward_op": true, "id": "XiabWyzIWymS3VqI", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.426.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1869000, "runtime_str": "1.87 ms", "start_timestamp": "23:14:19.344.344379", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059344564000, "dur": 173000, "relative_dur": 0.01679937852010099, "relative_gap_to_previous": 0.0005826374053214216, "parent_is_longest": true, "runtime_str": "173 us"}}, "gpu3": {"time": {"ts": 1678058059641424000, "dur": 1799000, "relative_dur": 0.17469411536220625, "relative_gap_to_previous": 9.710623422023694e-05, "parent_is_longest": true, "runtime_str": "1.80 ms"}}}, "is_backward_op": true, "id": "QVzR8Ly3wrpuLDge", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.427.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1799000, "runtime_str": "1.80 ms", "start_timestamp": "23:14:19.344.344564", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059343679000, "dur": 1058000, "relative_dur": 0.030530386102614417, "relative_gap_to_previous": 0.00011542679055808853, "parent_is_longest": true, "runtime_str": "1.06 ms"}}, "gpu3": {"time": {"ts": 1678058059632925000, "dur": 10298000, "relative_dur": 0.2971662722917989, "relative_gap_to_previous": 5.7713395279044266e-05, "parent_is_longest": true, "runtime_str": "10.3 ms"}}}, "is_backward_op": true, "id": "Rp8PyJ9wjVDdXOMH", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.418.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10298000, "runtime_str": "10.3 ms", "start_timestamp": "23:14:19.343.343679", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu4": {"time": {"ts": 1678058059344743000, "dur": 185000, "relative_dur": 0.005338489063311595, "relative_gap_to_previous": 0.0001731401858371328, "parent_is_longest": true, "runtime_str": "185 us"}}, "gpu3": {"time": {"ts": 1678058059643224000, "dur": 511000, "relative_dur": 0.01474577249379581, "relative_gap_to_previous": 2.8856697639522133e-05, "parent_is_longest": true, "runtime_str": "511 us"}}}, "is_backward_op": true, "id": "aG8GXqusPNoHtxyW", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.428.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 511000, "runtime_str": "511 us", "start_timestamp": "23:14:19.344.344743", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu4": {"time": {"ts": 1678058059344934000, "dur": 77000, "relative_dur": 0.0022219657182432043, "relative_gap_to_previous": 0.0001731401858371328, "parent_is_longest": true, "runtime_str": "77 us"}}, "gpu3": {"time": {"ts": 1678058059643736000, "dur": 95000, "relative_dur": 0.0027413862757546025, "relative_gap_to_previous": 2.8856697639522133e-05, "parent_is_longest": true, "runtime_str": "95 us"}}}, "is_backward_op": true, "id": "IOndMHn4Y3xSQwAW", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.429.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 95000, "runtime_str": "95 us", "start_timestamp": "23:14:19.344.344934", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059345016000, "dur": 138000, "relative_dur": 0.01344112204149216, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "138 us"}}, "gpu3": {"time": {"ts": 1678058059643833000, "dur": 1740000, "relative_dur": 0.16947501704490114, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.74 ms"}}}, "is_backward_op": true, "id": "hlheP6TlyD399vFF", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.431.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1740000, "runtime_str": "1.74 ms", "start_timestamp": "23:14:19.345.345016", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059345161000, "dur": 138000, "relative_dur": 0.01344112204149216, "relative_gap_to_previous": 0.0006817960455829356, "parent_is_longest": true, "runtime_str": "138 us"}}, "gpu3": {"time": {"ts": 1678058059645575000, "dur": 804000, "relative_dur": 0.07830914580695432, "relative_gap_to_previous": 0.00019479887016655303, "parent_is_longest": true, "runtime_str": "804 us"}}}, "is_backward_op": true, "id": "uZpCKFa5EMu00Bha", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.432.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 804000, "runtime_str": "804 us", "start_timestamp": "23:14:19.345.345161", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059345304000, "dur": 28000, "relative_dur": 0.0027271841823317423, "relative_gap_to_previous": 0.00048699717541638256, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1678058059646380000, "dur": 359000, "relative_dur": 0.03496639719489627, "relative_gap_to_previous": 9.739943508327652e-05, "parent_is_longest": true, "runtime_str": "359 us"}}}, "is_backward_op": true, "id": "uOHGxJ3g6jXrDLZI", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.433.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "23:14:19.345.345304", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059345337000, "dur": 37000, "relative_dur": 0.003603779098081231, "relative_gap_to_previous": 0.00048699717541638256, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1678058059646740000, "dur": 951000, "relative_dur": 0.09262686276419596, "relative_gap_to_previous": 9.739943508327652e-05, "parent_is_longest": true, "runtime_str": "951 us"}}}, "is_backward_op": true, "id": "nSEav3hU4StdTbWW", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.434.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 951000, "runtime_str": "951 us", "start_timestamp": "23:14:19.345.345337", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059345379000, "dur": 69000, "relative_dur": 0.00672056102074608, "relative_gap_to_previous": 0.00048699717541638256, "parent_is_longest": true, "runtime_str": "69 us"}}, "gpu3": {"time": {"ts": 1678058059647693000, "dur": 857000, "relative_dur": 0.08347131586636798, "relative_gap_to_previous": 0.00019479887016655303, "parent_is_longest": true, "runtime_str": "857 us"}}}, "is_backward_op": true, "id": "lxkQPVm4bhE9qHp6", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.435.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 857000, "runtime_str": "857 us", "start_timestamp": "23:14:19.345.345379", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059345453000, "dur": 37000, "relative_dur": 0.003603779098081231, "relative_gap_to_previous": 0.00048699717541638256, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1678058059648552000, "dur": 81000, "relative_dur": 0.007889354241745397, "relative_gap_to_previous": 0.00019479887016655303, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "MEx0bTa8OIkozGHp", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.436.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "23:14:19.345.345453", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059345494000, "dur": 212000, "relative_dur": 0.02064868023765462, "relative_gap_to_previous": 0.00038959774033310606, "parent_is_longest": true, "runtime_str": "212 us"}}, "gpu3": {"time": {"ts": 1678058059648634000, "dur": 1825000, "relative_dur": 0.17775396902697965, "relative_gap_to_previous": 9.739943508327652e-05, "parent_is_longest": true, "runtime_str": "1.82 ms"}}}, "is_backward_op": true, "id": "Ir2ncghWmERF1xZq", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.437.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1825000, "runtime_str": "1.82 ms", "start_timestamp": "23:14:19.345.345494", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059345712000, "dur": 194000, "relative_dur": 0.018895490406155645, "relative_gap_to_previous": 0.0005843966104996591, "parent_is_longest": true, "runtime_str": "194 us"}}, "gpu3": {"time": {"ts": 1678058059650460000, "dur": 1855000, "relative_dur": 0.18067595207947795, "relative_gap_to_previous": 9.739943508327652e-05, "parent_is_longest": true, "runtime_str": "1.85 ms"}}}, "is_backward_op": true, "id": "CyK6S8Dwlr5vEoBG", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.438.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1855000, "runtime_str": "1.85 ms", "start_timestamp": "23:14:19.345.345712", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059345911000, "dur": 173000, "relative_dur": 0.016850102269406836, "relative_gap_to_previous": 0.00048699717541638256, "parent_is_longest": true, "runtime_str": "173 us"}}, "gpu3": {"time": {"ts": 1678058059652316000, "dur": 1784000, "relative_dur": 0.1737605921885653, "relative_gap_to_previous": 9.739943508327652e-05, "parent_is_longest": true, "runtime_str": "1.78 ms"}}}, "is_backward_op": true, "id": "47QCfbd9xDXz35xT", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.439.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1784000, "runtime_str": "1.78 ms", "start_timestamp": "23:14:19.345.345911", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059345016000, "dur": 1068000, "relative_dur": 0.03081895307900964, "relative_gap_to_previous": 0.00014428348819761066, "parent_is_longest": true, "runtime_str": "1.07 ms"}}, "gpu3": {"time": {"ts": 1678058059643833000, "dur": 10267000, "relative_dur": 0.29627171466497376, "relative_gap_to_previous": 5.7713395279044266e-05, "parent_is_longest": true, "runtime_str": "10.3 ms"}}}, "is_backward_op": true, "id": "NOniBflowVIamYWD", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.430.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10267000, "runtime_str": "10.3 ms", "start_timestamp": "23:14:19.345.345016", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}], "resources": {"cpu4": {"time": {"ts": 1678058059342757000, "dur": 3327000, "relative_dur": 0.016062143646835607, "relative_gap_to_previous": 2.896689566606963e-05, "parent_is_longest": true, "runtime_str": "3.33 ms"}}, "gpu3": {"time": {"ts": 1678058059619446000, "dur": 34654000, "relative_dur": 0.16730313373532948, "relative_gap_to_previous": 4.827815944344938e-06, "parent_is_longest": true, "runtime_str": "34.6 ms"}}}, "is_backward_op": true, "id": "raJEwAKVIChdzG7P", "pretty_name": "Layer4", "trace_file": "/results/Transformer/Transformer.409.pt.trace.json", "trace_disk_size": "116.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 34654000, "runtime_str": "34.6 ms", "start_timestamp": "23:14:19.342.342757", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 885}, {"name": "Layer3", "type": "Layer3", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu4": {"time": {"ts": 1678058059346090000, "dur": 275000, "relative_dur": 0.02126507887411073, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "275 us"}}, "gpu3": {"time": {"ts": 1678058059654101000, "dur": 700000, "relative_dur": 0.054129291679554596, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "700 us"}}}, "is_backward_op": true, "id": "3Kt63EOLv44YWUBh", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.442.pt.trace.json", "trace_disk_size": "9.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 700000, "runtime_str": "700 us", "start_timestamp": "23:14:19.346.346090", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 62}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu4": {"time": {"ts": 1678058059346369000, "dur": 144000, "relative_dur": 0.011135168574079802, "relative_gap_to_previous": 0.0003093102381688834, "parent_is_longest": true, "runtime_str": "144 us"}}, "gpu3": {"time": {"ts": 1678058059654802000, "dur": 5560000, "relative_dur": 0.4299412310547479, "relative_gap_to_previous": 7.732755954222085e-05, "parent_is_longest": true, "runtime_str": "5.56 ms"}}}, "is_backward_op": true, "id": "5mOD9zH2iRq2naTO", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.443.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5560000, "runtime_str": "5.56 ms", "start_timestamp": "23:14:19.346.346369", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu4": {"time": {"ts": 1678058059346520000, "dur": 109000, "relative_dur": 0.008428703990102072, "relative_gap_to_previous": 0.0005412929167955459, "parent_is_longest": true, "runtime_str": "109 us"}}, "gpu3": {"time": {"ts": 1678058059660364000, "dur": 422000, "relative_dur": 0.0326322301268172, "relative_gap_to_previous": 0.0001546551190844417, "parent_is_longest": true, "runtime_str": "422 us"}}}, "is_backward_op": true, "id": "xVK3TT2I8QxYjpSE", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.444.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 422000, "runtime_str": "422 us", "start_timestamp": "23:14:19.346.346520", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu4": {"time": {"ts": 1678058059346633000, "dur": 24000, "relative_dur": 0.0018558614290133002, "relative_gap_to_previous": 0.0003093102381688834, "parent_is_longest": true, "runtime_str": "24 us"}}, "gpu3": {"time": {"ts": 1678058059660787000, "dur": 477000, "relative_dur": 0.036885245901639344, "relative_gap_to_previous": 7.732755954222085e-05, "parent_is_longest": true, "runtime_str": "477 us"}}}, "is_backward_op": true, "id": "zSPBmT68deJxx0qj", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.445.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 477000, "runtime_str": "477 us", "start_timestamp": "23:14:19.346.346633", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu4": {"time": {"ts": 1678058059346661000, "dur": 148000, "relative_dur": 0.011444478812248686, "relative_gap_to_previous": 0.0003093102381688834, "parent_is_longest": true, "runtime_str": "148 us"}}, "gpu3": {"time": {"ts": 1678058059661266000, "dur": 5767000, "relative_dur": 0.4459480358799876, "relative_gap_to_previous": 0.0001546551190844417, "parent_is_longest": true, "runtime_str": "5.77 ms"}}}, "is_backward_op": true, "id": "WxGobLJjFOVlh77f", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.446.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5767000, "runtime_str": "5.77 ms", "start_timestamp": "23:14:19.346.346661", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1678058059346090000, "dur": 719000, "relative_dur": 0.020814034275127374, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "719 us"}}, "gpu3": {"time": {"ts": 1678058059654101000, "dur": 12932000, "relative_dur": 0.3743631310792033, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12.9 ms"}}}, "is_backward_op": true, "id": "vTkgoQo2uuQxokP4", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.441.pt.trace.json", "trace_disk_size": "23.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12932000, "runtime_str": "12.9 ms", "start_timestamp": "23:14:19.346.346090", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 177}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu4": {"time": {"ts": 1678058059346817000, "dur": 137000, "relative_dur": 0.003965956461324687, "relative_gap_to_previous": 0.0002315886984715146, "parent_is_longest": true, "runtime_str": "137 us"}}, "gpu3": {"time": {"ts": 1678058059667035000, "dur": 439000, "relative_dur": 0.012708429828624364, "relative_gap_to_previous": 5.789717461787865e-05, "parent_is_longest": true, "runtime_str": "439 us"}}}, "is_backward_op": true, "id": "awsZS2ZKMeKcMTx7", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.447.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 439000, "runtime_str": "439 us", "start_timestamp": "23:14:19.346.346817", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu4": {"time": {"ts": 1678058059346959000, "dur": 77000, "relative_dur": 0.002229041222788328, "relative_gap_to_previous": 0.00014474293654469662, "parent_is_longest": true, "runtime_str": "77 us"}}, "gpu3": {"time": {"ts": 1678058059667475000, "dur": 96000, "relative_dur": 0.002779064381658175, "relative_gap_to_previous": 2.8948587308939323e-05, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "SUysBM8FGT423zDP", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.448.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "23:14:19.346.346959", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059347041000, "dur": 151000, "relative_dur": 0.014767726161369192, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "151 us"}}, "gpu3": {"time": {"ts": 1678058059667572000, "dur": 1727000, "relative_dur": 0.1688997555012225, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.73 ms"}}}, "is_backward_op": true, "id": "lEOgpIczTeYZz65Q", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.450.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1727000, "runtime_str": "1.73 ms", "start_timestamp": "23:14:19.347.347041", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059347198000, "dur": 140000, "relative_dur": 0.013691931540342298, "relative_gap_to_previous": 0.00058679706601467, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1678058059669300000, "dur": 800000, "relative_dur": 0.07823960880195599, "relative_gap_to_previous": 9.779951100244499e-05, "parent_is_longest": true, "runtime_str": "800 us"}}}, "is_backward_op": true, "id": "5wrgA4Z4qTsVEjwD", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.451.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 800000, "runtime_str": "800 us", "start_timestamp": "23:14:19.347.347198", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059347343000, "dur": 28000, "relative_dur": 0.00273838630806846, "relative_gap_to_previous": 0.0004889975550122249, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1678058059670101000, "dur": 360000, "relative_dur": 0.035207823960880194, "relative_gap_to_previous": 9.779951100244499e-05, "parent_is_longest": true, "runtime_str": "360 us"}}}, "is_backward_op": true, "id": "pI3HAsF4ph7rsZoM", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.452.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "23:14:19.347.347343", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059347375000, "dur": 38000, "relative_dur": 0.0037163814180929096, "relative_gap_to_previous": 0.00039119804400977997, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1678058059670463000, "dur": 951000, "relative_dur": 0.09300733496332518, "relative_gap_to_previous": 0.00019559902200488998, "parent_is_longest": true, "runtime_str": "951 us"}}}, "is_backward_op": true, "id": "7B7UCeoPCN7CBhSv", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.453.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 951000, "runtime_str": "951 us", "start_timestamp": "23:14:19.347.347375", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059347417000, "dur": 70000, "relative_dur": 0.006845965770171149, "relative_gap_to_previous": 0.00039119804400977997, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1678058059671415000, "dur": 851000, "relative_dur": 0.08322738386308068, "relative_gap_to_previous": 9.779951100244499e-05, "parent_is_longest": true, "runtime_str": "851 us"}}}, "is_backward_op": true, "id": "IUNrGQFAlBH6iv7P", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.454.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 851000, "runtime_str": "851 us", "start_timestamp": "23:14:19.347.347417", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059347492000, "dur": 46000, "relative_dur": 0.0044987775061124695, "relative_gap_to_previous": 0.0004889975550122249, "parent_is_longest": true, "runtime_str": "46 us"}}, "gpu3": {"time": {"ts": 1678058059672267000, "dur": 81000, "relative_dur": 0.007921760391198044, "relative_gap_to_previous": 9.779951100244499e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "yRfqeSo2cqkjRIOm", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.455.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "23:14:19.347.347492", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059347543000, "dur": 213000, "relative_dur": 0.02083129584352078, "relative_gap_to_previous": 0.0004889975550122249, "parent_is_longest": true, "runtime_str": "213 us"}}, "gpu3": {"time": {"ts": 1678058059672350000, "dur": 1812000, "relative_dur": 0.17721271393643032, "relative_gap_to_previous": 0.00019559902200488998, "parent_is_longest": true, "runtime_str": "1.81 ms"}}}, "is_backward_op": true, "id": "ALiHrDz6Z8RAHaVv", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.456.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1812000, "runtime_str": "1.81 ms", "start_timestamp": "23:14:19.347.347543", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059347762000, "dur": 207000, "relative_dur": 0.02024449877750611, "relative_gap_to_previous": 0.00058679706601467, "parent_is_longest": true, "runtime_str": "207 us"}}, "gpu3": {"time": {"ts": 1678058059674163000, "dur": 1849000, "relative_dur": 0.18083129584352078, "relative_gap_to_previous": 9.779951100244499e-05, "parent_is_longest": true, "runtime_str": "1.85 ms"}}}, "is_backward_op": true, "id": "3jbXbgF29tSPBpqJ", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.457.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1849000, "runtime_str": "1.85 ms", "start_timestamp": "23:14:19.347.347762", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059347975000, "dur": 174000, "relative_dur": 0.017017114914425428, "relative_gap_to_previous": 0.00058679706601467, "parent_is_longest": true, "runtime_str": "174 us"}}, "gpu3": {"time": {"ts": 1678058059676014000, "dur": 1783000, "relative_dur": 0.1743765281173594, "relative_gap_to_previous": 0.00019559902200488998, "parent_is_longest": true, "runtime_str": "1.78 ms"}}}, "is_backward_op": true, "id": "8j0nwZVHmX76F5eJ", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.458.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1783000, "runtime_str": "1.78 ms", "start_timestamp": "23:14:19.347.347975", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059347041000, "dur": 1108000, "relative_dur": 0.032075034738304774, "relative_gap_to_previous": 0.00014474293654469662, "parent_is_longest": true, "runtime_str": "1.11 ms"}}, "gpu3": {"time": {"ts": 1678058059667572000, "dur": 10225000, "relative_dur": 0.2959993052339046, "relative_gap_to_previous": 2.8948587308939323e-05, "parent_is_longest": true, "runtime_str": "10.2 ms"}}}, "is_backward_op": true, "id": "VXr7vuaP32iZuJIe", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.449.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10225000, "runtime_str": "10.2 ms", "start_timestamp": "23:14:19.347.347041", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu4": {"time": {"ts": 1678058059348155000, "dur": 175000, "relative_dur": 0.005066002779064382, "relative_gap_to_previous": 0.00017369152385363594, "parent_is_longest": true, "runtime_str": "175 us"}}, "gpu3": {"time": {"ts": 1678058059677798000, "dur": 517000, "relative_dur": 0.014966419638721631, "relative_gap_to_previous": 2.8948587308939323e-05, "parent_is_longest": true, "runtime_str": "517 us"}}}, "is_backward_op": true, "id": "PkLeehK3A2N8mLmq", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.459.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 517000, "runtime_str": "517 us", "start_timestamp": "23:14:19.348.348155", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu4": {"time": {"ts": 1678058059348336000, "dur": 78000, "relative_dur": 0.0022579898100972672, "relative_gap_to_previous": 0.00017369152385363594, "parent_is_longest": true, "runtime_str": "78 us"}}, "gpu3": {"time": {"ts": 1678058059678316000, "dur": 96000, "relative_dur": 0.002779064381658175, "relative_gap_to_previous": 2.8948587308939323e-05, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "np4zgj3nKJZqipaB", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.460.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "23:14:19.348.348336", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059348418000, "dur": 138000, "relative_dur": 0.013487099296325255, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "138 us"}}, "gpu3": {"time": {"ts": 1678058059678413000, "dur": 1727000, "relative_dur": 0.1687842064112588, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.73 ms"}}}, "is_backward_op": true, "id": "1YYFPs2rWAxmaS2h", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.462.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1727000, "runtime_str": "1.73 ms", "start_timestamp": "23:14:19.348.348418", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059348562000, "dur": 149000, "relative_dur": 0.014562157935887412, "relative_gap_to_previous": 0.0005863956215793589, "parent_is_longest": true, "runtime_str": "149 us"}}, "gpu3": {"time": {"ts": 1678058059680141000, "dur": 806000, "relative_dur": 0.07877247849882721, "relative_gap_to_previous": 9.773260359655981e-05, "parent_is_longest": true, "runtime_str": "806 us"}}}, "is_backward_op": true, "id": "B0oiB2z23PI8GhJ5", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.463.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 806000, "runtime_str": "806 us", "start_timestamp": "23:14:19.348.348562", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059348716000, "dur": 28000, "relative_dur": 0.002736512900703675, "relative_gap_to_previous": 0.000488663017982799, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1678058059680948000, "dur": 359000, "relative_dur": 0.03508600469116497, "relative_gap_to_previous": 9.773260359655981e-05, "parent_is_longest": true, "runtime_str": "359 us"}}}, "is_backward_op": true, "id": "SpGm43DjYqgRioqM", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.464.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "23:14:19.348.348716", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059348748000, "dur": 38000, "relative_dur": 0.003713838936669273, "relative_gap_to_previous": 0.00039093041438623924, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1678058059681308000, "dur": 951000, "relative_dur": 0.09294370602032838, "relative_gap_to_previous": 9.773260359655981e-05, "parent_is_longest": true, "runtime_str": "951 us"}}}, "is_backward_op": true, "id": "OIYsy5sw4gfuxe4d", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.465.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 951000, "runtime_str": "951 us", "start_timestamp": "23:14:19.348.348748", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059348791000, "dur": 71000, "relative_dur": 0.006939014855355747, "relative_gap_to_previous": 0.000488663017982799, "parent_is_longest": true, "runtime_str": "71 us"}}, "gpu3": {"time": {"ts": 1678058059682260000, "dur": 850000, "relative_dur": 0.08307271305707584, "relative_gap_to_previous": 9.773260359655981e-05, "parent_is_longest": true, "runtime_str": "850 us"}}}, "is_backward_op": true, "id": "dLw2N22p8aFp55BC", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.466.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 850000, "runtime_str": "850 us", "start_timestamp": "23:14:19.348.348791", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059348867000, "dur": 36000, "relative_dur": 0.003518373729476153, "relative_gap_to_previous": 0.000488663017982799, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1678058059683112000, "dur": 81000, "relative_dur": 0.007916340891321344, "relative_gap_to_previous": 0.00019546520719311962, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "i82Ci3ioksDHtlA6", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.467.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "23:14:19.348.348867", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059348907000, "dur": 223000, "relative_dur": 0.021794370602032837, "relative_gap_to_previous": 0.00039093041438623924, "parent_is_longest": true, "runtime_str": "223 us"}}, "gpu3": {"time": {"ts": 1678058059683194000, "dur": 1811000, "relative_dur": 0.17699374511336982, "relative_gap_to_previous": 9.773260359655981e-05, "parent_is_longest": true, "runtime_str": "1.81 ms"}}}, "is_backward_op": true, "id": "9ausF9dBRjWwRZjz", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.468.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1811000, "runtime_str": "1.81 ms", "start_timestamp": "23:14:19.348.348907", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059349136000, "dur": 179000, "relative_dur": 0.017494136043784206, "relative_gap_to_previous": 0.0005863956215793589, "parent_is_longest": true, "runtime_str": "179 us"}}, "gpu3": {"time": {"ts": 1678058059685006000, "dur": 1854000, "relative_dur": 0.1811962470680219, "relative_gap_to_previous": 9.773260359655981e-05, "parent_is_longest": true, "runtime_str": "1.85 ms"}}}, "is_backward_op": true, "id": "NJyqOMnHUUNZcr93", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.469.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1854000, "runtime_str": "1.85 ms", "start_timestamp": "23:14:19.349.349136", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059349321000, "dur": 191000, "relative_dur": 0.018666927286942924, "relative_gap_to_previous": 0.0005863956215793589, "parent_is_longest": true, "runtime_str": "191 us"}}, "gpu3": {"time": {"ts": 1678058059686862000, "dur": 1783000, "relative_dur": 0.17425723221266615, "relative_gap_to_previous": 0.00019546520719311962, "parent_is_longest": true, "runtime_str": "1.78 ms"}}}, "is_backward_op": true, "id": "wmDqvAGBEb9Np2Mf", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.470.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1783000, "runtime_str": "1.78 ms", "start_timestamp": "23:14:19.349.349321", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059348418000, "dur": 1094000, "relative_dur": 0.03166975451597962, "relative_gap_to_previous": 0.0001157943492357573, "parent_is_longest": true, "runtime_str": "1.09 ms"}}, "gpu3": {"time": {"ts": 1678058059678413000, "dur": 10232000, "relative_dur": 0.29620194534506716, "relative_gap_to_previous": 2.8948587308939323e-05, "parent_is_longest": true, "runtime_str": "10.2 ms"}}}, "is_backward_op": true, "id": "ho5h2Ni3Ela9wHSH", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.461.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10232000, "runtime_str": "10.2 ms", "start_timestamp": "23:14:19.348.348418", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}], "resources": {"cpu4": {"time": {"ts": 1678058059346090000, "dur": 3422000, "relative_dur": 0.016520786161548378, "relative_gap_to_previous": 2.896689566606963e-05, "parent_is_longest": true, "runtime_str": "3.42 ms"}}, "gpu3": {"time": {"ts": 1678058059654101000, "dur": 34544000, "relative_dur": 0.16677207398145152, "relative_gap_to_previous": 4.827815944344938e-06, "parent_is_longest": true, "runtime_str": "34.5 ms"}}}, "is_backward_op": true, "id": "0Xg8laXAS41diXh6", "pretty_name": "Layer3", "trace_file": "/results/Transformer/Transformer.440.pt.trace.json", "trace_disk_size": "116.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 34544000, "runtime_str": "34.5 ms", "start_timestamp": "23:14:19.346.346090", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 885}, {"name": "Layer2", "type": "Layer2", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu4": {"time": {"ts": 1678058059349518000, "dur": 258000, "relative_dur": 0.019973678098629714, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "258 us"}}, "gpu3": {"time": {"ts": 1678058059688646000, "dur": 693000, "relative_dur": 0.05365022838120306, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "693 us"}}}, "is_backward_op": true, "id": "mCKn38kPuNcSxJI4", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.473.pt.trace.json", "trace_disk_size": "9.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 693000, "runtime_str": "693 us", "start_timestamp": "23:14:19.349.349518", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 62}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu4": {"time": {"ts": 1678058059349780000, "dur": 153000, "relative_dur": 0.011844855616629249, "relative_gap_to_previous": 0.000309669427885732, "parent_is_longest": true, "runtime_str": "153 us"}}, "gpu3": {"time": {"ts": 1678058059689340000, "dur": 5560000, "relative_dur": 0.43044050476116746, "relative_gap_to_previous": 7.7417356971433e-05, "parent_is_longest": true, "runtime_str": "5.56 ms"}}}, "is_backward_op": true, "id": "jEJFyLBsEb7SgABb", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.474.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5560000, "runtime_str": "5.56 ms", "start_timestamp": "23:14:19.349.349780", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu4": {"time": {"ts": 1678058059349941000, "dur": 99000, "relative_dur": 0.007664318340171866, "relative_gap_to_previous": 0.000619338855771464, "parent_is_longest": true, "runtime_str": "99 us"}}, "gpu3": {"time": {"ts": 1678058059694901000, "dur": 423000, "relative_dur": 0.032747541998916156, "relative_gap_to_previous": 7.7417356971433e-05, "parent_is_longest": true, "runtime_str": "423 us"}}}, "is_backward_op": true, "id": "Zrsy6krWpBZWCtSH", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.475.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 423000, "runtime_str": "423 us", "start_timestamp": "23:14:19.349.349941", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu4": {"time": {"ts": 1678058059350044000, "dur": 24000, "relative_dur": 0.001858016567314392, "relative_gap_to_previous": 0.000309669427885732, "parent_is_longest": true, "runtime_str": "24 us"}}, "gpu3": {"time": {"ts": 1678058059695326000, "dur": 474000, "relative_dur": 0.03669582720445924, "relative_gap_to_previous": 0.000154834713942866, "parent_is_longest": true, "runtime_str": "474 us"}}}, "is_backward_op": true, "id": "2UeBlrZs87cKKSZ8", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.476.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 474000, "runtime_str": "474 us", "start_timestamp": "23:14:19.350.350044", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu4": {"time": {"ts": 1678058059350072000, "dur": 147000, "relative_dur": 0.01138035147480065, "relative_gap_to_previous": 0.000309669427885732, "parent_is_longest": true, "runtime_str": "147 us"}}, "gpu3": {"time": {"ts": 1678058059695801000, "dur": 5762000, "relative_dur": 0.44607881086939694, "relative_gap_to_previous": 7.7417356971433e-05, "parent_is_longest": true, "runtime_str": "5.76 ms"}}}, "is_backward_op": true, "id": "FRVdRoNbhXbEveVc", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.477.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5762000, "runtime_str": "5.76 ms", "start_timestamp": "23:14:19.350.350072", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1678058059349518000, "dur": 701000, "relative_dur": 0.02031471875271684, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "701 us"}}, "gpu3": {"time": {"ts": 1678058059688646000, "dur": 12917000, "relative_dur": 0.37432984611817893, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12.9 ms"}}}, "is_backward_op": true, "id": "wvDWsBYuK2GFYCtK", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.472.pt.trace.json", "trace_disk_size": "23.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12917000, "runtime_str": "12.9 ms", "start_timestamp": "23:14:19.349.349518", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 177}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu4": {"time": {"ts": 1678058059350227000, "dur": 150000, "relative_dur": 0.004346944098298896, "relative_gap_to_previous": 0.00023183701857594112, "parent_is_longest": true, "runtime_str": "150 us"}}, "gpu3": {"time": {"ts": 1678058059701564000, "dur": 431000, "relative_dur": 0.012490219375778827, "relative_gap_to_previous": 2.897962732199264e-05, "parent_is_longest": true, "runtime_str": "431 us"}}}, "is_backward_op": true, "id": "5VAB91TuCy373Ub0", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.478.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 431000, "runtime_str": "431 us", "start_timestamp": "23:14:19.350.350227", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu4": {"time": {"ts": 1678058059350383000, "dur": 77000, "relative_dur": 0.0022314313037934334, "relative_gap_to_previous": 0.00017387776393195582, "parent_is_longest": true, "runtime_str": "77 us"}}, "gpu3": {"time": {"ts": 1678058059701996000, "dur": 96000, "relative_dur": 0.002782044222911293, "relative_gap_to_previous": 2.897962732199264e-05, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "sf5JXRqKNKmrEB28", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.479.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "23:14:19.350.350383", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059350464000, "dur": 141000, "relative_dur": 0.013789731051344744, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "141 us"}}, "gpu3": {"time": {"ts": 1678058059702094000, "dur": 1724000, "relative_dur": 0.16860635696821516, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.72 ms"}}}, "is_backward_op": true, "id": "al5OCUi93mxnWpOq", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.481.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1724000, "runtime_str": "1.72 ms", "start_timestamp": "23:14:19.350.350464", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059350611000, "dur": 151000, "relative_dur": 0.014767726161369192, "relative_gap_to_previous": 0.00058679706601467, "parent_is_longest": true, "runtime_str": "151 us"}}, "gpu3": {"time": {"ts": 1678058059703819000, "dur": 803000, "relative_dur": 0.07853300733496332, "relative_gap_to_previous": 9.779951100244499e-05, "parent_is_longest": true, "runtime_str": "803 us"}}}, "is_backward_op": true, "id": "zZjVba8ExTLP040o", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.482.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 803000, "runtime_str": "803 us", "start_timestamp": "23:14:19.350.350611", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059350768000, "dur": 28000, "relative_dur": 0.00273838630806846, "relative_gap_to_previous": 0.00058679706601467, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1678058059704623000, "dur": 360000, "relative_dur": 0.035207823960880194, "relative_gap_to_previous": 9.779951100244499e-05, "parent_is_longest": true, "runtime_str": "360 us"}}}, "is_backward_op": true, "id": "mZHM1ppg581Y1o9h", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.483.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "23:14:19.350.350768", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059350800000, "dur": 39000, "relative_dur": 0.0038141809290953544, "relative_gap_to_previous": 0.00039119804400977997, "parent_is_longest": true, "runtime_str": "39 us"}}, "gpu3": {"time": {"ts": 1678058059704985000, "dur": 953000, "relative_dur": 0.09320293398533007, "relative_gap_to_previous": 0.00019559902200488998, "parent_is_longest": true, "runtime_str": "953 us"}}}, "is_backward_op": true, "id": "7nbru2OJYSqr4L4H", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.484.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.350.350800", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059350843000, "dur": 71000, "relative_dur": 0.006943765281173594, "relative_gap_to_previous": 0.00039119804400977997, "parent_is_longest": true, "runtime_str": "71 us"}}, "gpu3": {"time": {"ts": 1678058059705939000, "dur": 850000, "relative_dur": 0.08312958435207823, "relative_gap_to_previous": 9.779951100244499e-05, "parent_is_longest": true, "runtime_str": "850 us"}}}, "is_backward_op": true, "id": "0gl4Q904qAp64Sa2", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.485.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 850000, "runtime_str": "850 us", "start_timestamp": "23:14:19.350.350843", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059350919000, "dur": 36000, "relative_dur": 0.0035207823960880197, "relative_gap_to_previous": 0.0004889975550122249, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1678058059706790000, "dur": 81000, "relative_dur": 0.007921760391198044, "relative_gap_to_previous": 9.779951100244499e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "L6GtyjOdcUcFjiCN", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.486.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "23:14:19.350.350919", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059350960000, "dur": 243000, "relative_dur": 0.02376528117359413, "relative_gap_to_previous": 0.0004889975550122249, "parent_is_longest": true, "runtime_str": "243 us"}}, "gpu3": {"time": {"ts": 1678058059706872000, "dur": 1811000, "relative_dur": 0.17711491442542787, "relative_gap_to_previous": 9.779951100244499e-05, "parent_is_longest": true, "runtime_str": "1.81 ms"}}}, "is_backward_op": true, "id": "bB9smHZrlyGXAzFV", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.487.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1811000, "runtime_str": "1.81 ms", "start_timestamp": "23:14:19.350.350960", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059351209000, "dur": 181000, "relative_dur": 0.01770171149144254, "relative_gap_to_previous": 0.00058679706601467, "parent_is_longest": true, "runtime_str": "181 us"}}, "gpu3": {"time": {"ts": 1678058059708684000, "dur": 1851000, "relative_dur": 0.18102689486552567, "relative_gap_to_previous": 9.779951100244499e-05, "parent_is_longest": true, "runtime_str": "1.85 ms"}}}, "is_backward_op": true, "id": "cXpvKZxL1cGe5eFN", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.488.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1851000, "runtime_str": "1.85 ms", "start_timestamp": "23:14:19.351.351209", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059351396000, "dur": 195000, "relative_dur": 0.019070904645476772, "relative_gap_to_previous": 0.00058679706601467, "parent_is_longest": true, "runtime_str": "195 us"}}, "gpu3": {"time": {"ts": 1678058059710537000, "dur": 1782000, "relative_dur": 0.17427872860635696, "relative_gap_to_previous": 0.00019559902200488998, "parent_is_longest": true, "runtime_str": "1.78 ms"}}}, "is_backward_op": true, "id": "YbkLIaTw9rR0DbeM", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.489.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1782000, "runtime_str": "1.78 ms", "start_timestamp": "23:14:19.351.351396", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059350464000, "dur": 1127000, "relative_dur": 0.03266003999188571, "relative_gap_to_previous": 0.00011591850928797056, "parent_is_longest": true, "runtime_str": "1.13 ms"}}, "gpu3": {"time": {"ts": 1678058059702094000, "dur": 10225000, "relative_dur": 0.2963166893673747, "relative_gap_to_previous": 5.795925464398528e-05, "parent_is_longest": true, "runtime_str": "10.2 ms"}}}, "is_backward_op": true, "id": "cUBewk56K51mmUMG", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.480.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10225000, "runtime_str": "10.2 ms", "start_timestamp": "23:14:19.350.350464", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu4": {"time": {"ts": 1678058059351597000, "dur": 165000, "relative_dur": 0.004781638508128785, "relative_gap_to_previous": 0.00017387776393195582, "parent_is_longest": true, "runtime_str": "165 us"}}, "gpu3": {"time": {"ts": 1678058059712321000, "dur": 509000, "relative_dur": 0.014750630306894254, "relative_gap_to_previous": 5.795925464398528e-05, "parent_is_longest": true, "runtime_str": "509 us"}}}, "is_backward_op": true, "id": "svvyPRCu8XJHnKAj", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.490.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 509000, "runtime_str": "509 us", "start_timestamp": "23:14:19.351.351597", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu4": {"time": {"ts": 1678058059351768000, "dur": 77000, "relative_dur": 0.0022314313037934334, "relative_gap_to_previous": 0.00017387776393195582, "parent_is_longest": true, "runtime_str": "77 us"}}, "gpu3": {"time": {"ts": 1678058059712831000, "dur": 96000, "relative_dur": 0.002782044222911293, "relative_gap_to_previous": 2.897962732199264e-05, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "GbCyrgB2P8q1M4W4", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.491.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "23:14:19.351.351768", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059351850000, "dur": 150000, "relative_dur": 0.014671361502347418, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "150 us"}}, "gpu3": {"time": {"ts": 1678058059712929000, "dur": 1725000, "relative_dur": 0.1687206572769953, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.73 ms"}}}, "is_backward_op": true, "id": "vUkQaYlpiiEnIzGM", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.493.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1725000, "runtime_str": "1.73 ms", "start_timestamp": "23:14:19.351.351850", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059352006000, "dur": 140000, "relative_dur": 0.013693270735524257, "relative_gap_to_previous": 0.0005868544600938967, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1678058059714656000, "dur": 800000, "relative_dur": 0.0782472613458529, "relative_gap_to_previous": 0.00019561815336463224, "parent_is_longest": true, "runtime_str": "800 us"}}}, "is_backward_op": true, "id": "Fp0UhUNIB1pE4AUR", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.494.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 800000, "runtime_str": "800 us", "start_timestamp": "23:14:19.352.352006", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059352151000, "dur": 29000, "relative_dur": 0.0028364632237871676, "relative_gap_to_previous": 0.0004890453834115806, "parent_is_longest": true, "runtime_str": "29 us"}}, "gpu3": {"time": {"ts": 1678058059715457000, "dur": 361000, "relative_dur": 0.03530907668231612, "relative_gap_to_previous": 9.780907668231612e-05, "parent_is_longest": true, "runtime_str": "361 us"}}}, "is_backward_op": true, "id": "QlxNbA6TIg3vmUPo", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.495.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 361000, "runtime_str": "361 us", "start_timestamp": "23:14:19.352.352151", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059352184000, "dur": 38000, "relative_dur": 0.0037167449139280124, "relative_gap_to_previous": 0.0003912363067292645, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1678058059715819000, "dur": 953000, "relative_dur": 0.09321205007824726, "relative_gap_to_previous": 9.780907668231612e-05, "parent_is_longest": true, "runtime_str": "953 us"}}}, "is_backward_op": true, "id": "N41GbCTCYgBq7p6z", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.496.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.352.352184", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059352226000, "dur": 70000, "relative_dur": 0.006846635367762128, "relative_gap_to_previous": 0.0003912363067292645, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1678058059716774000, "dur": 850000, "relative_dur": 0.08313771517996871, "relative_gap_to_previous": 0.00019561815336463224, "parent_is_longest": true, "runtime_str": "850 us"}}}, "is_backward_op": true, "id": "qLvkNOZ9VTeHjGMF", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.497.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 850000, "runtime_str": "850 us", "start_timestamp": "23:14:19.352.352226", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059352302000, "dur": 35000, "relative_dur": 0.003423317683881064, "relative_gap_to_previous": 0.0005868544600938967, "parent_is_longest": true, "runtime_str": "35 us"}}, "gpu3": {"time": {"ts": 1678058059717625000, "dur": 81000, "relative_dur": 0.007922535211267605, "relative_gap_to_previous": 9.780907668231612e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "D4nV6fTBpaxfF1vr", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.498.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "23:14:19.352.352302", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059352350000, "dur": 217000, "relative_dur": 0.0212245696400626, "relative_gap_to_previous": 0.0012715179968701095, "parent_is_longest": true, "runtime_str": "217 us"}}, "gpu3": {"time": {"ts": 1678058059717707000, "dur": 1812000, "relative_dur": 0.1772300469483568, "relative_gap_to_previous": 9.780907668231612e-05, "parent_is_longest": true, "runtime_str": "1.81 ms"}}}, "is_backward_op": true, "id": "Z59yMaHVundzP6I4", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.499.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1812000, "runtime_str": "1.81 ms", "start_timestamp": "23:14:19.352.352350", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059352573000, "dur": 193000, "relative_dur": 0.01887715179968701, "relative_gap_to_previous": 0.0005868544600938967, "parent_is_longest": true, "runtime_str": "193 us"}}, "gpu3": {"time": {"ts": 1678058059719521000, "dur": 1849000, "relative_dur": 0.1808489827856025, "relative_gap_to_previous": 0.00019561815336463224, "parent_is_longest": true, "runtime_str": "1.85 ms"}}}, "is_backward_op": true, "id": "CtggtYnKY6bzFIPI", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.500.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1849000, "runtime_str": "1.85 ms", "start_timestamp": "23:14:19.352.352573", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059352776000, "dur": 177000, "relative_dur": 0.017312206572769953, "relative_gap_to_previous": 0.0009780907668231612, "parent_is_longest": true, "runtime_str": "177 us"}}, "gpu3": {"time": {"ts": 1678058059721371000, "dur": 1782000, "relative_dur": 0.1742957746478873, "relative_gap_to_previous": 9.780907668231612e-05, "parent_is_longest": true, "runtime_str": "1.78 ms"}}}, "is_backward_op": true, "id": "FOoGhoIlk7zroP1K", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.501.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1782000, "runtime_str": "1.78 ms", "start_timestamp": "23:14:19.352.352776", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059351850000, "dur": 1103000, "relative_dur": 0.03196452893615788, "relative_gap_to_previous": 0.0001448981366099632, "parent_is_longest": true, "runtime_str": "1.10 ms"}}, "gpu3": {"time": {"ts": 1678058059712929000, "dur": 10224000, "relative_dur": 0.29628770974005275, "relative_gap_to_previous": 5.795925464398528e-05, "parent_is_longest": true, "runtime_str": "10.2 ms"}}}, "is_backward_op": true, "id": "U0dDEIcjcIDbtj7W", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.492.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10224000, "runtime_str": "10.2 ms", "start_timestamp": "23:14:19.351.351850", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}], "resources": {"cpu4": {"time": {"ts": 1678058059349518000, "dur": 3435000, "relative_dur": 0.016583547768824862, "relative_gap_to_previous": 2.896689566606963e-05, "parent_is_longest": true, "runtime_str": "3.44 ms"}}, "gpu3": {"time": {"ts": 1678058059688646000, "dur": 34507000, "relative_dur": 0.16659344479151078, "relative_gap_to_previous": 4.827815944344938e-06, "parent_is_longest": true, "runtime_str": "34.5 ms"}}}, "is_backward_op": true, "id": "4cIGf624SCwczxjP", "pretty_name": "Layer2", "trace_file": "/results/Transformer/Transformer.471.pt.trace.json", "trace_disk_size": "116.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 34507000, "runtime_str": "34.5 ms", "start_timestamp": "23:14:19.349.349518", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 885}, {"name": "Layer1", "type": "Layer1", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu4": {"time": {"ts": 1678058059352959000, "dur": 266000, "relative_dur": 0.020577086717722595, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "266 us"}}, "gpu3": {"time": {"ts": 1678058059723155000, "dur": 699000, "relative_dur": 0.05407287073566953, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "699 us"}}}, "is_backward_op": true, "id": "viSyreVvnO5eAbWJ", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.504.pt.trace.json", "trace_disk_size": "9.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 699000, "runtime_str": "699 us", "start_timestamp": "23:14:19.352.352959", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 62}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu4": {"time": {"ts": 1678058059353229000, "dur": 141000, "relative_dur": 0.010907403109770248, "relative_gap_to_previous": 0.00030942987545447514, "parent_is_longest": true, "runtime_str": "141 us"}}, "gpu3": {"time": {"ts": 1678058059723855000, "dur": 5556000, "relative_dur": 0.429798097006266, "relative_gap_to_previous": 7.735746886361879e-05, "parent_is_longest": true, "runtime_str": "5.56 ms"}}}, "is_backward_op": true, "id": "b7YbUSL4IlYzQ5K7", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.505.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5556000, "runtime_str": "5.56 ms", "start_timestamp": "23:14:19.353.353229", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu4": {"time": {"ts": 1678058059353378000, "dur": 107000, "relative_dur": 0.00827724916840721, "relative_gap_to_previous": 0.0006188597509089503, "parent_is_longest": true, "runtime_str": "107 us"}}, "gpu3": {"time": {"ts": 1678058059729412000, "dur": 421000, "relative_dur": 0.032567494391583504, "relative_gap_to_previous": 7.735746886361879e-05, "parent_is_longest": true, "runtime_str": "421 us"}}}, "is_backward_op": true, "id": "CnY6YbdZnf6S7yfr", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.506.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 421000, "runtime_str": "421 us", "start_timestamp": "23:14:19.353.353378", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu4": {"time": {"ts": 1678058059353489000, "dur": 25000, "relative_dur": 0.0019339367215904695, "relative_gap_to_previous": 0.00030942987545447514, "parent_is_longest": true, "runtime_str": "25 us"}}, "gpu3": {"time": {"ts": 1678058059729835000, "dur": 476000, "relative_dur": 0.03682215517908254, "relative_gap_to_previous": 0.00015471493772723757, "parent_is_longest": true, "runtime_str": "476 us"}}}, "is_backward_op": true, "id": "6v6sacb6NvlTPind", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.507.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 476000, "runtime_str": "476 us", "start_timestamp": "23:14:19.353.353489", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu4": {"time": {"ts": 1678058059353518000, "dur": 148000, "relative_dur": 0.01144890539181558, "relative_gap_to_previous": 0.00030942987545447514, "parent_is_longest": true, "runtime_str": "148 us"}}, "gpu3": {"time": {"ts": 1678058059730312000, "dur": 5770000, "relative_dur": 0.44635259534308036, "relative_gap_to_previous": 7.735746886361879e-05, "parent_is_longest": true, "runtime_str": "5.77 ms"}}}, "is_backward_op": true, "id": "aNPPgcEs8zbuluxO", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.508.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5770000, "runtime_str": "5.77 ms", "start_timestamp": "23:14:19.353.353518", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1678058059352959000, "dur": 707000, "relative_dur": 0.020452441564452674, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "707 us"}}, "gpu3": {"time": {"ts": 1678058059723155000, "dur": 12927000, "relative_dur": 0.37395857440407315, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12.9 ms"}}}, "is_backward_op": true, "id": "pCb0KZK9xZb0XVly", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.503.pt.trace.json", "trace_disk_size": "23.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12927000, "runtime_str": "12.9 ms", "start_timestamp": "23:14:19.352.352959", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 177}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu4": {"time": {"ts": 1678058059353674000, "dur": 137000, "relative_dur": 0.003963202962277251, "relative_gap_to_previous": 0.00023142791020597085, "parent_is_longest": true, "runtime_str": "137 us"}}, "gpu3": {"time": {"ts": 1678058059736084000, "dur": 436000, "relative_dur": 0.01261282110622541, "relative_gap_to_previous": 5.785697755149271e-05, "parent_is_longest": true, "runtime_str": "436 us"}}}, "is_backward_op": true, "id": "5uiBjcvmbZIpyTvE", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.509.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 436000, "runtime_str": "436 us", "start_timestamp": "23:14:19.353.353674", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu4": {"time": {"ts": 1678058059353816000, "dur": 77000, "relative_dur": 0.0022274936357324694, "relative_gap_to_previous": 0.0001446424438787318, "parent_is_longest": true, "runtime_str": "77 us"}}, "gpu3": {"time": {"ts": 1678058059736521000, "dur": 96000, "relative_dur": 0.00277713492247165, "relative_gap_to_previous": 2.8928488775746356e-05, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "WbJW66AO3LxtYxMn", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.510.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "23:14:19.353.353816", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059353898000, "dur": 151000, "relative_dur": 0.014736020298623987, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "151 us"}}, "gpu3": {"time": {"ts": 1678058059736619000, "dur": 1731000, "relative_dur": 0.1689274909729677, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.73 ms"}}}, "is_backward_op": true, "id": "vP5hqCGwDKvZbyjP", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.512.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1731000, "runtime_str": "1.73 ms", "start_timestamp": "23:14:19.353.353898", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059354055000, "dur": 162000, "relative_dur": 0.015809505221040306, "relative_gap_to_previous": 0.0005855372304089002, "parent_is_longest": true, "runtime_str": "162 us"}}, "gpu3": {"time": {"ts": 1678058059738351000, "dur": 804000, "relative_dur": 0.07846198887479262, "relative_gap_to_previous": 9.758953840148337e-05, "parent_is_longest": true, "runtime_str": "804 us"}}}, "is_backward_op": true, "id": "RTkQ0o8GMYdnGHae", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.513.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 804000, "runtime_str": "804 us", "start_timestamp": "23:14:19.354.354055", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059354223000, "dur": 28000, "relative_dur": 0.0027325070752415343, "relative_gap_to_previous": 0.0005855372304089002, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1678058059739157000, "dur": 359000, "relative_dur": 0.035034644286132524, "relative_gap_to_previous": 0.00019517907680296673, "parent_is_longest": true, "runtime_str": "359 us"}}}, "is_backward_op": true, "id": "uyc7A381tsOZ4B6L", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.514.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "23:14:19.354.354223", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059354256000, "dur": 37000, "relative_dur": 0.0036108129208548842, "relative_gap_to_previous": 0.0004879476920074168, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1678058059739517000, "dur": 952000, "relative_dur": 0.09290524055821216, "relative_gap_to_previous": 9.758953840148337e-05, "parent_is_longest": true, "runtime_str": "952 us"}}}, "is_backward_op": true, "id": "0T0eztm0g5JvPVHo", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.515.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "23:14:19.354.354256", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059354298000, "dur": 69000, "relative_dur": 0.006733678149702352, "relative_gap_to_previous": 0.0004879476920074168, "parent_is_longest": true, "runtime_str": "69 us"}}, "gpu3": {"time": {"ts": 1678058059740471000, "dur": 850000, "relative_dur": 0.08295110764126086, "relative_gap_to_previous": 0.00019517907680296673, "parent_is_longest": true, "runtime_str": "850 us"}}}, "is_backward_op": true, "id": "xo5mXBth1TwZZdxJ", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.516.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 850000, "runtime_str": "850 us", "start_timestamp": "23:14:19.354.354298", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059354373000, "dur": 36000, "relative_dur": 0.003513223382453401, "relative_gap_to_previous": 0.0005855372304089002, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1678058059741323000, "dur": 80000, "relative_dur": 0.0078071630721186685, "relative_gap_to_previous": 0.00019517907680296673, "parent_is_longest": true, "runtime_str": "80 us"}}}, "is_backward_op": true, "id": "E5cUGtyXG6NEr2YC", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.517.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 80000, "runtime_str": "80 us", "start_timestamp": "23:14:19.354.354373", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059354414000, "dur": 224000, "relative_dur": 0.021860056601932274, "relative_gap_to_previous": 0.0004879476920074168, "parent_is_longest": true, "runtime_str": "224 us"}}, "gpu3": {"time": {"ts": 1678058059741404000, "dur": 1817000, "relative_dur": 0.17732019127549525, "relative_gap_to_previous": 9.758953840148337e-05, "parent_is_longest": true, "runtime_str": "1.82 ms"}}}, "is_backward_op": true, "id": "2Mdd7PJLFODyWV7P", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.518.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1817000, "runtime_str": "1.82 ms", "start_timestamp": "23:14:19.354.354414", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059354644000, "dur": 193000, "relative_dur": 0.018834780911486288, "relative_gap_to_previous": 0.0005855372304089002, "parent_is_longest": true, "runtime_str": "193 us"}}, "gpu3": {"time": {"ts": 1678058059743222000, "dur": 1854000, "relative_dur": 0.18093100419635016, "relative_gap_to_previous": 9.758953840148337e-05, "parent_is_longest": true, "runtime_str": "1.85 ms"}}}, "is_backward_op": true, "id": "gPJcXcZngRdVfq9d", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.519.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1854000, "runtime_str": "1.85 ms", "start_timestamp": "23:14:19.354.354644", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059354843000, "dur": 174000, "relative_dur": 0.016980579681858104, "relative_gap_to_previous": 0.0005855372304089002, "parent_is_longest": true, "runtime_str": "174 us"}}, "gpu3": {"time": {"ts": 1678058059745077000, "dur": 1789000, "relative_dur": 0.17458768420025372, "relative_gap_to_previous": 9.758953840148337e-05, "parent_is_longest": true, "runtime_str": "1.79 ms"}}}, "is_backward_op": true, "id": "InlwYvpItGvbkB2s", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.520.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1789000, "runtime_str": "1.79 ms", "start_timestamp": "23:14:19.354.354843", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059353898000, "dur": 1119000, "relative_dur": 0.03237097894006017, "relative_gap_to_previous": 0.0001446424438787318, "parent_is_longest": true, "runtime_str": "1.12 ms"}}, "gpu3": {"time": {"ts": 1678058059736619000, "dur": 10247000, "relative_dur": 0.2964302244850729, "relative_gap_to_previous": 5.785697755149271e-05, "parent_is_longest": true, "runtime_str": "10.2 ms"}}}, "is_backward_op": true, "id": "hsAqrfqrWboDw9cB", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.511.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10247000, "runtime_str": "10.2 ms", "start_timestamp": "23:14:19.353.353898", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu4": {"time": {"ts": 1678058059355023000, "dur": 177000, "relative_dur": 0.005120342513307105, "relative_gap_to_previous": 0.00017357093265447812, "parent_is_longest": true, "runtime_str": "177 us"}}, "gpu3": {"time": {"ts": 1678058059746867000, "dur": 515000, "relative_dur": 0.014898171719509372, "relative_gap_to_previous": 2.8928488775746356e-05, "parent_is_longest": true, "runtime_str": "515 us"}}}, "is_backward_op": true, "id": "EbB0wjMYMUNfe81P", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.521.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 515000, "runtime_str": "515 us", "start_timestamp": "23:14:19.355.355023", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu4": {"time": {"ts": 1678058059355205000, "dur": 79000, "relative_dur": 0.002285350613283962, "relative_gap_to_previous": 0.0001446424438787318, "parent_is_longest": true, "runtime_str": "79 us"}}, "gpu3": {"time": {"ts": 1678058059747384000, "dur": 95000, "relative_dur": 0.002748206433695904, "relative_gap_to_previous": 5.785697755149271e-05, "parent_is_longest": true, "runtime_str": "95 us"}}}, "is_backward_op": true, "id": "c5JhuDr4bqJtBN79", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.522.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 95000, "runtime_str": "95 us", "start_timestamp": "23:14:19.355.355205", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059355288000, "dur": 140000, "relative_dur": 0.013669205233352862, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1678058059747481000, "dur": 1731000, "relative_dur": 0.1690099589923843, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.73 ms"}}}, "is_backward_op": true, "id": "PI6sshe4gnYf3RKN", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.524.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1731000, "runtime_str": "1.73 ms", "start_timestamp": "23:14:19.355.355288", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059355434000, "dur": 152000, "relative_dur": 0.014840851396211677, "relative_gap_to_previous": 0.0005858230814294083, "parent_is_longest": true, "runtime_str": "152 us"}}, "gpu3": {"time": {"ts": 1678058059749213000, "dur": 799000, "relative_dur": 0.07801210701034954, "relative_gap_to_previous": 9.763718023823472e-05, "parent_is_longest": true, "runtime_str": "799 us"}}}, "is_backward_op": true, "id": "nq7tn3wXyN86avRg", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.525.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 799000, "runtime_str": "799 us", "start_timestamp": "23:14:19.355.355434", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059355592000, "dur": 28000, "relative_dur": 0.0027338410466705722, "relative_gap_to_previous": 0.0005858230814294083, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1678058059750013000, "dur": 360000, "relative_dur": 0.0351493848857645, "relative_gap_to_previous": 9.763718023823472e-05, "parent_is_longest": true, "runtime_str": "360 us"}}}, "is_backward_op": true, "id": "8PnSd3WXIaY8jPSU", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.526.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "23:14:19.355.355592", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059355624000, "dur": 38000, "relative_dur": 0.003710212849052919, "relative_gap_to_previous": 0.0003905487209529389, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1678058059750374000, "dur": 953000, "relative_dur": 0.09304823276703769, "relative_gap_to_previous": 9.763718023823472e-05, "parent_is_longest": true, "runtime_str": "953 us"}}}, "is_backward_op": true, "id": "21k9NnvSeK1nwzuU", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.527.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.355.355624", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059355666000, "dur": 70000, "relative_dur": 0.006834602616676431, "relative_gap_to_previous": 0.0003905487209529389, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1678058059751328000, "dur": 851000, "relative_dur": 0.08308924038273775, "relative_gap_to_previous": 9.763718023823472e-05, "parent_is_longest": true, "runtime_str": "851 us"}}}, "is_backward_op": true, "id": "JSAwjNvKIY2JXeyL", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.528.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 851000, "runtime_str": "851 us", "start_timestamp": "23:14:19.355.355666", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059355741000, "dur": 36000, "relative_dur": 0.0035149384885764497, "relative_gap_to_previous": 0.0004881859011911736, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1678058059752180000, "dur": 81000, "relative_dur": 0.007908611599297012, "relative_gap_to_previous": 9.763718023823472e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "U2cMyR8KjcQerrlj", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.529.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "23:14:19.355.355741", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059355782000, "dur": 222000, "relative_dur": 0.021675454012888107, "relative_gap_to_previous": 0.0004881859011911736, "parent_is_longest": true, "runtime_str": "222 us"}}, "gpu3": {"time": {"ts": 1678058059752262000, "dur": 1816000, "relative_dur": 0.17730911931263424, "relative_gap_to_previous": 9.763718023823472e-05, "parent_is_longest": true, "runtime_str": "1.82 ms"}}}, "is_backward_op": true, "id": "hFL9oLS6CKFnXV3a", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.530.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1816000, "runtime_str": "1.82 ms", "start_timestamp": "23:14:19.355.355782", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059356010000, "dur": 187000, "relative_dur": 0.018258152704549894, "relative_gap_to_previous": 0.0005858230814294083, "parent_is_longest": true, "runtime_str": "187 us"}}, "gpu3": {"time": {"ts": 1678058059754080000, "dur": 1854000, "relative_dur": 0.18101933216168717, "relative_gap_to_previous": 0.00019527436047646945, "parent_is_longest": true, "runtime_str": "1.85 ms"}}}, "is_backward_op": true, "id": "eLwzOYbZM283877D", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.531.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1854000, "runtime_str": "1.85 ms", "start_timestamp": "23:14:19.356.356010", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059356203000, "dur": 187000, "relative_dur": 0.018258152704549894, "relative_gap_to_previous": 0.0005858230814294083, "parent_is_longest": true, "runtime_str": "187 us"}}, "gpu3": {"time": {"ts": 1678058059755935000, "dur": 1788000, "relative_dur": 0.17457527826596367, "relative_gap_to_previous": 9.763718023823472e-05, "parent_is_longest": true, "runtime_str": "1.79 ms"}}}, "is_backward_op": true, "id": "ARwh1LCmFmWSRUFj", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.532.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1788000, "runtime_str": "1.79 ms", "start_timestamp": "23:14:19.356.356203", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059355288000, "dur": 1102000, "relative_dur": 0.031879194630872486, "relative_gap_to_previous": 0.00011571395510298542, "parent_is_longest": true, "runtime_str": "1.10 ms"}}, "gpu3": {"time": {"ts": 1678058059747481000, "dur": 10242000, "relative_dur": 0.29628558204119415, "relative_gap_to_previous": 5.785697755149271e-05, "parent_is_longest": true, "runtime_str": "10.2 ms"}}}, "is_backward_op": true, "id": "MAQPgZBN1BrnwhPu", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.523.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10242000, "runtime_str": "10.2 ms", "start_timestamp": "23:14:19.355.355288", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}], "resources": {"cpu4": {"time": {"ts": 1678058059352959000, "dur": 3431000, "relative_dur": 0.016564236505047483, "relative_gap_to_previous": 2.896689566606963e-05, "parent_is_longest": true, "runtime_str": "3.43 ms"}}, "gpu3": {"time": {"ts": 1678058059723155000, "dur": 34568000, "relative_dur": 0.16688794156411582, "relative_gap_to_previous": 9.655631888689876e-06, "parent_is_longest": true, "runtime_str": "34.6 ms"}}}, "is_backward_op": true, "id": "2X716h3ocJOSHvm3", "pretty_name": "Layer1", "trace_file": "/results/Transformer/Transformer.502.pt.trace.json", "trace_disk_size": "116.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 34568000, "runtime_str": "34.6 ms", "start_timestamp": "23:14:19.352.352959", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 885}], "resources": {"cpu4": {"time": {"ts": 1678058059336149000, "dur": 20241000, "relative_dur": 0.07316359062222126, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "20.2 ms"}}, "gpu3": {"time": {"ts": 1678058059550590000, "dur": 207133000, "relative_dur": 0.7487077721630628, "relative_gap_to_previous": 3.614623320103812e-06, "parent_is_longest": true, "runtime_str": "207 ms"}}}, "is_backward_op": true, "id": "nn4PBYHowy1jnngo", "pretty_name": "TransformerDecoder", "trace_file": "/results/Transformer/Transformer.345.pt.trace.json", "trace_disk_size": "694.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 207133000, "runtime_str": "207 ms", "start_timestamp": "23:14:19.336.336149", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5282}, {"name": "PositionalEncoding", "type": "PositionalEncoding", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 310, in forward\n    with hotline.annotate('PositionalEncoding'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 310, "ops": [{"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 311, in forward\n    tgt = self.pos_encoder(tgt, targets_positions, decode=decode, cache=cache) # ret float32(32, 256)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 372, in forward\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 372, "resources": {"cpu4": {"time": {"ts": 1678058059356396000, "dur": 250000, "relative_dur": 0.36075036075036077, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "250 us"}}, "gpu3": {"time": {"ts": 1678058059757724000, "dur": 693000, "relative_dur": 1.0, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "693 us"}}}, "is_backward_op": true, "id": "ple1OLrkgFMxOJV5", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.534.pt.trace.json", "trace_disk_size": "8.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 693000, "runtime_str": "693 us", "start_timestamp": "23:14:19.356.356396", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 60}], "resources": {"cpu4": {"time": {"ts": 1678058059356396000, "dur": 250000, "relative_dur": 0.000903655830025953, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "250 us"}}, "gpu3": {"time": {"ts": 1678058059757724000, "dur": 693000, "relative_dur": 0.0025049339608319418, "relative_gap_to_previous": 3.614623320103812e-06, "parent_is_longest": true, "runtime_str": "693 us"}}}, "is_backward_op": true, "id": "agxYX14QEAKmYyOq", "pretty_name": "PositionalEncoding", "trace_file": "/results/Transformer/Transformer.533.pt.trace.json", "trace_disk_size": "8.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 693000, "runtime_str": "693 us", "start_timestamp": "23:14:19.356.356396", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 60}, {"name": "Embedding", "type": "Embedding", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 301, in forward\n    with hotline.annotate('Embedding'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 301, "resources": {"cpu4": {"time": {"ts": 1678058059356651000, "dur": 411000, "relative_dur": 0.0014856101845626666, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "411 us"}}, "gpu3": {"time": {"ts": 1678058059758418000, "dur": 1116000, "relative_dur": 0.004033919625235854, "relative_gap_to_previous": 3.614623320103812e-06, "parent_is_longest": true, "runtime_str": "1.12 ms"}}}, "is_backward_op": true, "id": "fEUel85X5zajsgwy", "pretty_name": "Embedding", "trace_file": "/results/Transformer/Transformer.535.pt.trace.json", "trace_disk_size": "17.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1116000, "runtime_str": "1.12 ms", "start_timestamp": "23:14:19.356.356651", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 112}], "resources": {"cpu4": {"time": {"ts": 1678058059335228000, "dur": 21815000, "relative_dur": 0.05183717211176774, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "21.8 ms"}}, "gpu3": {"time": {"ts": 1678058059482880000, "dur": 276654000, "relative_dur": 0.6573899158106346, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "277 ms"}}}, "is_backward_op": true, "id": "iXEyUpPN9WduZXUX", "pretty_name": "Decoder", "trace_file": "/results/Transformer/Transformer.343.pt.trace.json", "trace_disk_size": "739.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 276654000, "runtime_str": "277 ms", "start_timestamp": "23:14:19.335.335228", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5589}, {"name": "Encoder", "type": "Encoder", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 205, in forward\n    with hotline.annotate('Encoder'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 205, "ops": [{"name": "TransformerEncoder", "type": "TransformerEncoder", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 260, in forward\n    with hotline.annotate('TransformerEncoder'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 260, "ops": [{"name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu4": {"time": {"ts": 1678058059758875000, "dur": 158000, "relative_dur": 0.012477296059385612, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "158 us"}}, "gpu3": {"time": {"ts": 1678058059759535000, "dur": 432000, "relative_dur": 0.03411513859275053, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "432 us"}}}, "is_backward_op": true, "id": "iSll5HALb3M1S9b5", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.540.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 432000, "runtime_str": "432 us", "start_timestamp": "23:14:19.758.758875", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 36}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu4": {"time": {"ts": 1678058059759038000, "dur": 152000, "relative_dur": 0.012003474690041854, "relative_gap_to_previous": 0.00039485114111979786, "parent_is_longest": true, "runtime_str": "152 us"}}, "gpu3": {"time": {"ts": 1678058059759968000, "dur": 5562000, "relative_dur": 0.43923240938166314, "relative_gap_to_previous": 7.897022822395957e-05, "parent_is_longest": true, "runtime_str": "5.56 ms"}}}, "is_backward_op": true, "id": "DNLYOD3L8IpExEDc", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.541.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5562000, "runtime_str": "5.56 ms", "start_timestamp": "23:14:19.759.759038", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu4": {"time": {"ts": 1678058059759197000, "dur": 120000, "relative_dur": 0.009476427386875147, "relative_gap_to_previous": 0.000552791597567717, "parent_is_longest": true, "runtime_str": "120 us"}}, "gpu3": {"time": {"ts": 1678058059765531000, "dur": 423000, "relative_dur": 0.033404406538734895, "relative_gap_to_previous": 7.897022822395957e-05, "parent_is_longest": true, "runtime_str": "423 us"}}}, "is_backward_op": true, "id": "9f7YoOMvC1olIsCM", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.542.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 423000, "runtime_str": "423 us", "start_timestamp": "23:14:19.759.759197", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu4": {"time": {"ts": 1678058059759322000, "dur": 24000, "relative_dur": 0.0018952854773750297, "relative_gap_to_previous": 0.00039485114111979786, "parent_is_longest": true, "runtime_str": "24 us"}}, "gpu3": {"time": {"ts": 1678058059765956000, "dur": 474000, "relative_dur": 0.03743188817815683, "relative_gap_to_previous": 0.00015794045644791914, "parent_is_longest": true, "runtime_str": "474 us"}}}, "is_backward_op": true, "id": "C2jfW5fjl86nyQft", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.543.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 474000, "runtime_str": "474 us", "start_timestamp": "23:14:19.759.759322", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu4": {"time": {"ts": 1678058059759350000, "dur": 167000, "relative_dur": 0.013188028113401248, "relative_gap_to_previous": 0.0003158809128958383, "parent_is_longest": true, "runtime_str": "167 us"}}, "gpu3": {"time": {"ts": 1678058059766432000, "dur": 5766000, "relative_dur": 0.45534233593935086, "relative_gap_to_previous": 0.00015794045644791914, "parent_is_longest": true, "runtime_str": "5.77 ms"}}}, "is_backward_op": true, "id": "vHIHzMDxWCyGkbBl", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.544.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5766000, "runtime_str": "5.77 ms", "start_timestamp": "23:14:19.759.759350", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1678058059758875000, "dur": 642000, "relative_dur": 0.026707712788085532, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "642 us"}}, "gpu3": {"time": {"ts": 1678058059759535000, "dur": 12663000, "relative_dur": 0.5267909143855563, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12.7 ms"}}}, "is_backward_op": true, "id": "sCQyb1Pkg2gTxJJc", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.539.pt.trace.json", "trace_disk_size": "20.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12663000, "runtime_str": "12.7 ms", "start_timestamp": "23:14:19.758.758875", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 151}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu4": {"time": {"ts": 1678058059759525000, "dur": 137000, "relative_dur": 0.005699309426740994, "relative_gap_to_previous": 0.00033280638988268576, "parent_is_longest": true, "runtime_str": "137 us"}}, "gpu3": {"time": {"ts": 1678058059772199000, "dur": 437000, "relative_dur": 0.01817954904734171, "relative_gap_to_previous": 4.160079873533572e-05, "parent_is_longest": true, "runtime_str": "437 us"}}}, "is_backward_op": true, "id": "BGcO3urjG5pdAnz4", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.545.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 437000, "runtime_str": "437 us", "start_timestamp": "23:14:19.759.759525", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu4": {"time": {"ts": 1678058059759668000, "dur": 76000, "relative_dur": 0.0031616607038855147, "relative_gap_to_previous": 0.0002496047924120143, "parent_is_longest": true, "runtime_str": "76 us"}}, "gpu3": {"time": {"ts": 1678058059772637000, "dur": 100000, "relative_dur": 0.0041600798735335716, "relative_gap_to_previous": 4.160079873533572e-05, "parent_is_longest": true, "runtime_str": "100 us"}}}, "is_backward_op": true, "id": "OF9WvVQjZzFj26Sj", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.546.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 100000, "runtime_str": "100 us", "start_timestamp": "23:14:19.759.759668", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059759749000, "dur": 151000, "relative_dur": 0.014747533938861216, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "151 us"}}, "gpu3": {"time": {"ts": 1678058059772739000, "dur": 1728000, "relative_dur": 0.16876648110167008, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.73 ms"}}}, "is_backward_op": true, "id": "7wHEc85xdyatuftc", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.548.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1728000, "runtime_str": "1.73 ms", "start_timestamp": "23:14:19.759.759749", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059759907000, "dur": 144000, "relative_dur": 0.014063873425139173, "relative_gap_to_previous": 0.0006836605137220432, "parent_is_longest": true, "runtime_str": "144 us"}}, "gpu3": {"time": {"ts": 1678058059774468000, "dur": 800000, "relative_dur": 0.07813263013966208, "relative_gap_to_previous": 9.76657876745776e-05, "parent_is_longest": true, "runtime_str": "800 us"}}}, "is_backward_op": true, "id": "L4wlM6j7nKecELoV", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.549.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 800000, "runtime_str": "800 us", "start_timestamp": "23:14:19.759.759907", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059760057000, "dur": 27000, "relative_dur": 0.002636976267213595, "relative_gap_to_previous": 0.0005859947260474656, "parent_is_longest": true, "runtime_str": "27 us"}}, "gpu3": {"time": {"ts": 1678058059775269000, "dur": 360000, "relative_dur": 0.03515968356284793, "relative_gap_to_previous": 9.76657876745776e-05, "parent_is_longest": true, "runtime_str": "360 us"}}}, "is_backward_op": true, "id": "4wtS08UnUsrMODCn", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.550.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "23:14:19.760.760057", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059760089000, "dur": 38000, "relative_dur": 0.0037112999316339487, "relative_gap_to_previous": 0.000488328938372888, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1678058059775630000, "dur": 954000, "relative_dur": 0.09317316144154703, "relative_gap_to_previous": 9.76657876745776e-05, "parent_is_longest": true, "runtime_str": "954 us"}}}, "is_backward_op": true, "id": "eBc6xjw5nqS076Z0", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.551.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 954000, "runtime_str": "954 us", "start_timestamp": "23:14:19.760.760089", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059760132000, "dur": 70000, "relative_dur": 0.006836605137220432, "relative_gap_to_previous": 0.000488328938372888, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1678058059776585000, "dur": 851000, "relative_dur": 0.08311358531106554, "relative_gap_to_previous": 9.76657876745776e-05, "parent_is_longest": true, "runtime_str": "851 us"}}}, "is_backward_op": true, "id": "P0SspfqBP3H6Oe48", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.552.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 851000, "runtime_str": "851 us", "start_timestamp": "23:14:19.760.760132", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059760207000, "dur": 36000, "relative_dur": 0.0035159683562847933, "relative_gap_to_previous": 0.000488328938372888, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1678058059777437000, "dur": 81000, "relative_dur": 0.007910928801640784, "relative_gap_to_previous": 9.76657876745776e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "GGCe4o0B9xtj0eKO", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.553.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "23:14:19.760.760207", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059760248000, "dur": 224000, "relative_dur": 0.021877136439105382, "relative_gap_to_previous": 0.000488328938372888, "parent_is_longest": true, "runtime_str": "224 us"}}, "gpu3": {"time": {"ts": 1678058059777519000, "dur": 1817000, "relative_dur": 0.17745873620470748, "relative_gap_to_previous": 9.76657876745776e-05, "parent_is_longest": true, "runtime_str": "1.82 ms"}}}, "is_backward_op": true, "id": "cvSvHjp1pxmhxwy3", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.554.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1817000, "runtime_str": "1.82 ms", "start_timestamp": "23:14:19.760.760248", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059760478000, "dur": 192000, "relative_dur": 0.0187518312335189, "relative_gap_to_previous": 0.0005859947260474656, "parent_is_longest": true, "runtime_str": "192 us"}}, "gpu3": {"time": {"ts": 1678058059779337000, "dur": 1853000, "relative_dur": 0.1809747045609923, "relative_gap_to_previous": 9.76657876745776e-05, "parent_is_longest": true, "runtime_str": "1.85 ms"}}}, "is_backward_op": true, "id": "iQo4w2AtGLRzIyjK", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.555.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1853000, "runtime_str": "1.85 ms", "start_timestamp": "23:14:19.760.760478", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059760675000, "dur": 176000, "relative_dur": 0.017189178630725655, "relative_gap_to_previous": 0.000488328938372888, "parent_is_longest": true, "runtime_str": "176 us"}}, "gpu3": {"time": {"ts": 1678058059781191000, "dur": 1787000, "relative_dur": 0.17452876257447017, "relative_gap_to_previous": 9.76657876745776e-05, "parent_is_longest": true, "runtime_str": "1.79 ms"}}}, "is_backward_op": true, "id": "5xl6qzg8SSjKdO70", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.556.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1787000, "runtime_str": "1.79 ms", "start_timestamp": "23:14:19.760.760675", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059759749000, "dur": 1102000, "relative_dur": 0.04584408020633996, "relative_gap_to_previous": 0.0002080039936766786, "parent_is_longest": true, "runtime_str": "1.10 ms"}}, "gpu3": {"time": {"ts": 1678058059772739000, "dur": 10239000, "relative_dur": 0.42595057825110244, "relative_gap_to_previous": 8.320159747067144e-05, "parent_is_longest": true, "runtime_str": "10.2 ms"}}}, "is_backward_op": true, "id": "d7itZKPFQUSuiXJs", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.547.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10239000, "runtime_str": "10.2 ms", "start_timestamp": "23:14:19.759.759749", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu4": {"time": {"ts": 1678058059760857000, "dur": 186000, "relative_dur": 0.007737748564772443, "relative_gap_to_previous": 0.0002496047924120143, "parent_is_longest": true, "runtime_str": "186 us"}}, "gpu3": {"time": {"ts": 1678058059782979000, "dur": 594000, "relative_dur": 0.024710874448789416, "relative_gap_to_previous": 4.160079873533572e-05, "parent_is_longest": true, "runtime_str": "594 us"}}}, "is_backward_op": true, "id": "5K33ZzIcLqMQg8US", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.557.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 594000, "runtime_str": "594 us", "start_timestamp": "23:14:19.760.760857", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 43}], "resources": {"cpu4": {"time": {"ts": 1678058059758875000, "dur": 2168000, "relative_dur": 0.015210726087658123, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2.17 ms"}}, "gpu3": {"time": {"ts": 1678058059759535000, "dur": 24038000, "relative_dur": 0.16865103030217987, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "24 ms"}}}, "is_backward_op": true, "id": "mMIIvzgMOTdsjqYq", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.538.pt.trace.json", "trace_disk_size": "72.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 24038000, "runtime_str": "24 ms", "start_timestamp": "23:14:19.758.758875", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 544}, {"name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu4": {"time": {"ts": 1678058059761050000, "dur": 76000, "relative_dur": 0.00616083009079118, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "76 us"}}, "gpu3": {"time": {"ts": 1678058059783575000, "dur": 98000, "relative_dur": 0.007944228274967574, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "98 us"}}}, "is_backward_op": true, "id": "4Lf4IECk0FxrdlKF", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.560.pt.trace.json", "trace_disk_size": "2.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 98000, "runtime_str": "98 us", "start_timestamp": "23:14:19.761.761050", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu4": {"time": {"ts": 1678058059761131000, "dur": 142000, "relative_dur": 0.011511024643320364, "relative_gap_to_previous": 0.0004053177691309987, "parent_is_longest": true, "runtime_str": "142 us"}}, "gpu3": {"time": {"ts": 1678058059783675000, "dur": 5561000, "relative_dur": 0.45079442282749677, "relative_gap_to_previous": 0.00016212710765239947, "parent_is_longest": true, "runtime_str": "5.56 ms"}}}, "is_backward_op": true, "id": "XgDpdSfGDYX5YRNd", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.561.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5561000, "runtime_str": "5.56 ms", "start_timestamp": "23:14:19.761.761131", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu4": {"time": {"ts": 1678058059761280000, "dur": 100000, "relative_dur": 0.008106355382619975, "relative_gap_to_previous": 0.0005674448767833982, "parent_is_longest": true, "runtime_str": "100 us"}}, "gpu3": {"time": {"ts": 1678058059789238000, "dur": 421000, "relative_dur": 0.03412775616083009, "relative_gap_to_previous": 0.00016212710765239947, "parent_is_longest": true, "runtime_str": "421 us"}}}, "is_backward_op": true, "id": "gVBl4pIladkjHdRj", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.562.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 421000, "runtime_str": "421 us", "start_timestamp": "23:14:19.761.761280", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu4": {"time": {"ts": 1678058059761384000, "dur": 24000, "relative_dur": 0.0019455252918287938, "relative_gap_to_previous": 0.00032425421530479895, "parent_is_longest": true, "runtime_str": "24 us"}}, "gpu3": {"time": {"ts": 1678058059789661000, "dur": 474000, "relative_dur": 0.03842412451361868, "relative_gap_to_previous": 0.00016212710765239947, "parent_is_longest": true, "runtime_str": "474 us"}}}, "is_backward_op": true, "id": "mvAAb7gdesLWh7Qh", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.563.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 474000, "runtime_str": "474 us", "start_timestamp": "23:14:19.761.761384", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu4": {"time": {"ts": 1678058059761412000, "dur": 161000, "relative_dur": 0.013051232166018159, "relative_gap_to_previous": 0.00032425421530479895, "parent_is_longest": true, "runtime_str": "161 us"}}, "gpu3": {"time": {"ts": 1678058059790137000, "dur": 5774000, "relative_dur": 0.4680609597924773, "relative_gap_to_previous": 0.00016212710765239947, "parent_is_longest": true, "runtime_str": "5.77 ms"}}}, "is_backward_op": true, "id": "6Gt04z2qJNCPXY5J", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.564.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5774000, "runtime_str": "5.77 ms", "start_timestamp": "23:14:19.761.761412", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1678058059761050000, "dur": 523000, "relative_dur": 0.02204983346684093, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "523 us"}}, "gpu3": {"time": {"ts": 1678058059783575000, "dur": 12336000, "relative_dur": 0.5200893798220836, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12.3 ms"}}}, "is_backward_op": true, "id": "zpGtii5Z3NVd4ayP", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.559.pt.trace.json", "trace_disk_size": "17.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12336000, "runtime_str": "12.3 ms", "start_timestamp": "23:14:19.761.761050", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 134}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu4": {"time": {"ts": 1678058059761580000, "dur": 136000, "relative_dur": 0.005733799907247355, "relative_gap_to_previous": 0.0002951220540494962, "parent_is_longest": true, "runtime_str": "136 us"}}, "gpu3": {"time": {"ts": 1678058059795912000, "dur": 437000, "relative_dur": 0.01842404823137569, "relative_gap_to_previous": 4.216029343564231e-05, "parent_is_longest": true, "runtime_str": "437 us"}}}, "is_backward_op": true, "id": "MBYwG94mfQAQiohd", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.565.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 437000, "runtime_str": "437 us", "start_timestamp": "23:14:19.761.761580", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu4": {"time": {"ts": 1678058059761722000, "dur": 76000, "relative_dur": 0.0032041823011088157, "relative_gap_to_previous": 0.00025296176061385386, "parent_is_longest": true, "runtime_str": "76 us"}}, "gpu3": {"time": {"ts": 1678058059796350000, "dur": 100000, "relative_dur": 0.004216029343564231, "relative_gap_to_previous": 4.216029343564231e-05, "parent_is_longest": true, "runtime_str": "100 us"}}}, "is_backward_op": true, "id": "5e7hDhcC2w6PwEtA", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.566.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 100000, "runtime_str": "100 us", "start_timestamp": "23:14:19.761.761722", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059761803000, "dur": 152000, "relative_dur": 0.014830715191726022, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "152 us"}}, "gpu3": {"time": {"ts": 1678058059796451000, "dur": 1732000, "relative_dur": 0.1689920967899307, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.73 ms"}}}, "is_backward_op": true, "id": "obqWjtmiNQk0QnVU", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.568.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1732000, "runtime_str": "1.73 ms", "start_timestamp": "23:14:19.761.761803", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059761961000, "dur": 140000, "relative_dur": 0.013659869255537126, "relative_gap_to_previous": 0.0005854229680944482, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1678058059798184000, "dur": 799000, "relative_dur": 0.07795882525124402, "relative_gap_to_previous": 9.757049468240804e-05, "parent_is_longest": true, "runtime_str": "799 us"}}}, "is_backward_op": true, "id": "8XnLs9POzeRTfXSB", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.569.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 799000, "runtime_str": "799 us", "start_timestamp": "23:14:19.761.761961", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059762107000, "dur": 28000, "relative_dur": 0.002731973851107425, "relative_gap_to_previous": 0.0005854229680944482, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1678058059798984000, "dur": 359000, "relative_dur": 0.035027807590984485, "relative_gap_to_previous": 9.757049468240804e-05, "parent_is_longest": true, "runtime_str": "359 us"}}}, "is_backward_op": true, "id": "XuRXgdBuK1CDL3zP", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.570.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "23:14:19.762.762107", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059762139000, "dur": 38000, "relative_dur": 0.0037076787979315054, "relative_gap_to_previous": 0.00039028197872963214, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1678058059799345000, "dur": 953000, "relative_dur": 0.09298468143233486, "relative_gap_to_previous": 0.00019514098936481607, "parent_is_longest": true, "runtime_str": "953 us"}}}, "is_backward_op": true, "id": "VZsTGwPF1SQ4VfbN", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.571.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "23:14:19.762.762139", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059762182000, "dur": 69000, "relative_dur": 0.006732364133086155, "relative_gap_to_previous": 0.0004878524734120402, "parent_is_longest": true, "runtime_str": "69 us"}}, "gpu3": {"time": {"ts": 1678058059800299000, "dur": 851000, "relative_dur": 0.08303249097472924, "relative_gap_to_previous": 9.757049468240804e-05, "parent_is_longest": true, "runtime_str": "851 us"}}}, "is_backward_op": true, "id": "UcjotbwIXSuQQ6uZ", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.572.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 851000, "runtime_str": "851 us", "start_timestamp": "23:14:19.762.762182", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059762256000, "dur": 47000, "relative_dur": 0.004585813250073178, "relative_gap_to_previous": 0.0004878524734120402, "parent_is_longest": true, "runtime_str": "47 us"}}, "gpu3": {"time": {"ts": 1678058059801152000, "dur": 80000, "relative_dur": 0.007805639574592643, "relative_gap_to_previous": 0.00019514098936481607, "parent_is_longest": true, "runtime_str": "80 us"}}}, "is_backward_op": true, "id": "gMEIVVGXaM0WXmcx", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.573.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 80000, "runtime_str": "80 us", "start_timestamp": "23:14:19.762.762256", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059762307000, "dur": 237000, "relative_dur": 0.023124207239730705, "relative_gap_to_previous": 0.00039028197872963214, "parent_is_longest": true, "runtime_str": "237 us"}}, "gpu3": {"time": {"ts": 1678058059801234000, "dur": 1817000, "relative_dur": 0.1772855888379354, "relative_gap_to_previous": 0.00019514098936481607, "parent_is_longest": true, "runtime_str": "1.82 ms"}}}, "is_backward_op": true, "id": "A5p8byyFprBjxYmd", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.574.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1817000, "runtime_str": "1.82 ms", "start_timestamp": "23:14:19.762.762307", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059762550000, "dur": 190000, "relative_dur": 0.018538393989657526, "relative_gap_to_previous": 0.0005854229680944482, "parent_is_longest": true, "runtime_str": "190 us"}}, "gpu3": {"time": {"ts": 1678058059803052000, "dur": 1859000, "relative_dur": 0.18138354961459655, "relative_gap_to_previous": 9.757049468240804e-05, "parent_is_longest": true, "runtime_str": "1.86 ms"}}}, "is_backward_op": true, "id": "VrkutgBTADtaNKFD", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.575.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1859000, "runtime_str": "1.86 ms", "start_timestamp": "23:14:19.762.762550", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059762746000, "dur": 181000, "relative_dur": 0.017660259537515856, "relative_gap_to_previous": 0.0005854229680944482, "parent_is_longest": true, "runtime_str": "181 us"}}, "gpu3": {"time": {"ts": 1678058059804912000, "dur": 1788000, "relative_dur": 0.17445604449214558, "relative_gap_to_previous": 9.757049468240804e-05, "parent_is_longest": true, "runtime_str": "1.79 ms"}}}, "is_backward_op": true, "id": "8yJ40DgaCqacpbHV", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.576.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1788000, "runtime_str": "1.79 ms", "start_timestamp": "23:14:19.762.762746", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059761803000, "dur": 1124000, "relative_dur": 0.04738816982166196, "relative_gap_to_previous": 0.00021080146717821155, "parent_is_longest": true, "runtime_str": "1.12 ms"}}, "gpu3": {"time": {"ts": 1678058059796451000, "dur": 10249000, "relative_dur": 0.4321008474218981, "relative_gap_to_previous": 4.216029343564231e-05, "parent_is_longest": true, "runtime_str": "10.2 ms"}}}, "is_backward_op": true, "id": "DbcKc7klfPGEXdZL", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.567.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10249000, "runtime_str": "10.2 ms", "start_timestamp": "23:14:19.761.761803", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu4": {"time": {"ts": 1678058059762933000, "dur": 188000, "relative_dur": 0.007926135165900754, "relative_gap_to_previous": 0.00025296176061385386, "parent_is_longest": true, "runtime_str": "188 us"}}, "gpu3": {"time": {"ts": 1678058059806701000, "dur": 593000, "relative_dur": 0.02500105400733589, "relative_gap_to_previous": 4.216029343564231e-05, "parent_is_longest": true, "runtime_str": "593 us"}}}, "is_backward_op": true, "id": "KSrJoBnYMmQF2qcl", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.577.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 593000, "runtime_str": "593 us", "start_timestamp": "23:14:19.762.762933", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 43}], "resources": {"cpu4": {"time": {"ts": 1678058059761050000, "dur": 2071000, "relative_dur": 0.014530172383551648, "relative_gap_to_previous": 4.9112122976755935e-05, "parent_is_longest": true, "runtime_str": "2.07 ms"}}, "gpu3": {"time": {"ts": 1678058059783575000, "dur": 23719000, "relative_dur": 0.16641292069795344, "relative_gap_to_previous": 1.403203513621598e-05, "parent_is_longest": true, "runtime_str": "23.7 ms"}}}, "is_backward_op": true, "id": "xbiEjcuowUGHxeFT", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.558.pt.trace.json", "trace_disk_size": "69.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 23719000, "runtime_str": "23.7 ms", "start_timestamp": "23:14:19.761.761050", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 527}, {"name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu4": {"time": {"ts": 1678058059763127000, "dur": 75000, "relative_dur": 0.006081245439065921, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "75 us"}}, "gpu3": {"time": {"ts": 1678058059807295000, "dur": 98000, "relative_dur": 0.007946160707046136, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "98 us"}}}, "is_backward_op": true, "id": "diledCrT5RB2M3gg", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.580.pt.trace.json", "trace_disk_size": "2.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 98000, "runtime_str": "98 us", "start_timestamp": "23:14:19.763.763127", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu4": {"time": {"ts": 1678058059763207000, "dur": 141000, "relative_dur": 0.011432741425443932, "relative_gap_to_previous": 0.0004054163626043947, "parent_is_longest": true, "runtime_str": "141 us"}}, "gpu3": {"time": {"ts": 1678058059807396000, "dur": 5557000, "relative_dur": 0.4505797453985243, "relative_gap_to_previous": 0.00024324981756263683, "parent_is_longest": true, "runtime_str": "5.56 ms"}}}, "is_backward_op": true, "id": "gsBApzg4DoxWhGPd", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.581.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5557000, "runtime_str": "5.56 ms", "start_timestamp": "23:14:19.763.763207", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu4": {"time": {"ts": 1678058059763356000, "dur": 109000, "relative_dur": 0.008838076704775804, "relative_gap_to_previous": 0.0006486661801670316, "parent_is_longest": true, "runtime_str": "109 us"}}, "gpu3": {"time": {"ts": 1678058059812954000, "dur": 423000, "relative_dur": 0.03429822427633179, "relative_gap_to_previous": 8.108327252087895e-05, "parent_is_longest": true, "runtime_str": "423 us"}}}, "is_backward_op": true, "id": "Bs6BvV19fYRvGkeJ", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.582.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 423000, "runtime_str": "423 us", "start_timestamp": "23:14:19.763.763356", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu4": {"time": {"ts": 1678058059763469000, "dur": 23000, "relative_dur": 0.0018649152679802157, "relative_gap_to_previous": 0.0003243330900835158, "parent_is_longest": true, "runtime_str": "23 us"}}, "gpu3": {"time": {"ts": 1678058059813378000, "dur": 475000, "relative_dur": 0.0385145544474175, "relative_gap_to_previous": 8.108327252087895e-05, "parent_is_longest": true, "runtime_str": "475 us"}}}, "is_backward_op": true, "id": "lFCNTfo1wyPbupvd", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.583.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 475000, "runtime_str": "475 us", "start_timestamp": "23:14:19.763.763469", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu4": {"time": {"ts": 1678058059763496000, "dur": 148000, "relative_dur": 0.012000324333090083, "relative_gap_to_previous": 0.0003243330900835158, "parent_is_longest": true, "runtime_str": "148 us"}}, "gpu3": {"time": {"ts": 1678058059813855000, "dur": 5773000, "relative_dur": 0.46809373226303413, "relative_gap_to_previous": 0.0001621665450417579, "parent_is_longest": true, "runtime_str": "5.77 ms"}}}, "is_backward_op": true, "id": "U1JKDQN0nzewHJw3", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.584.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5773000, "runtime_str": "5.77 ms", "start_timestamp": "23:14:19.763.763496", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1678058059763127000, "dur": 517000, "relative_dur": 0.021812505273816556, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "517 us"}}, "gpu3": {"time": {"ts": 1678058059807295000, "dur": 12333000, "relative_dur": 0.5203358366382583, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12.3 ms"}}}, "is_backward_op": true, "id": "oJXDnw99WLHwe1Wm", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.579.pt.trace.json", "trace_disk_size": "17.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12333000, "runtime_str": "12.3 ms", "start_timestamp": "23:14:19.763.763127", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 134}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu4": {"time": {"ts": 1678058059763653000, "dur": 139000, "relative_dur": 0.005864484009788204, "relative_gap_to_previous": 0.0003797147920006751, "parent_is_longest": true, "runtime_str": "139 us"}}, "gpu3": {"time": {"ts": 1678058059819629000, "dur": 432000, "relative_dur": 0.018226310016032404, "relative_gap_to_previous": 4.219053244451945e-05, "parent_is_longest": true, "runtime_str": "432 us"}}}, "is_backward_op": true, "id": "vFocEQftUzk697nA", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.585.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 432000, "runtime_str": "432 us", "start_timestamp": "23:14:19.763.763653", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu4": {"time": {"ts": 1678058059763798000, "dur": 87000, "relative_dur": 0.003670576322673192, "relative_gap_to_previous": 0.0002531431946671167, "parent_is_longest": true, "runtime_str": "87 us"}}, "gpu3": {"time": {"ts": 1678058059820062000, "dur": 100000, "relative_dur": 0.004219053244451945, "relative_gap_to_previous": 4.219053244451945e-05, "parent_is_longest": true, "runtime_str": "100 us"}}}, "is_backward_op": true, "id": "GvPusN5WAxvmFXFR", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.586.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 100000, "runtime_str": "100 us", "start_timestamp": "23:14:19.763.763798", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059763890000, "dur": 142000, "relative_dur": 0.01386447959382933, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "142 us"}}, "gpu3": {"time": {"ts": 1678058059820162000, "dur": 1730000, "relative_dur": 0.16891232181214608, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.73 ms"}}}, "is_backward_op": true, "id": "dkJKYeVJhZ7Y6zDo", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.588.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1730000, "runtime_str": "1.73 ms", "start_timestamp": "23:14:19.763.763890", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059764038000, "dur": 141000, "relative_dur": 0.013766842413591095, "relative_gap_to_previous": 0.0005858230814294083, "parent_is_longest": true, "runtime_str": "141 us"}}, "gpu3": {"time": {"ts": 1678058059821894000, "dur": 801000, "relative_dur": 0.07820738137082602, "relative_gap_to_previous": 0.00019527436047646945, "parent_is_longest": true, "runtime_str": "801 us"}}}, "is_backward_op": true, "id": "TzsODTdG8eQizgHg", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.589.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 801000, "runtime_str": "801 us", "start_timestamp": "23:14:19.764.764038", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059764184000, "dur": 27000, "relative_dur": 0.0026362038664323375, "relative_gap_to_previous": 0.0004881859011911736, "parent_is_longest": true, "runtime_str": "27 us"}}, "gpu3": {"time": {"ts": 1678058059822696000, "dur": 360000, "relative_dur": 0.0351493848857645, "relative_gap_to_previous": 9.763718023823472e-05, "parent_is_longest": true, "runtime_str": "360 us"}}}, "is_backward_op": true, "id": "mV4JgbFielH92My8", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.590.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "23:14:19.764.764184", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059764215000, "dur": 38000, "relative_dur": 0.003710212849052919, "relative_gap_to_previous": 0.0003905487209529389, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1678058059823057000, "dur": 951000, "relative_dur": 0.09285295840656121, "relative_gap_to_previous": 9.763718023823472e-05, "parent_is_longest": true, "runtime_str": "951 us"}}}, "is_backward_op": true, "id": "hNhBX5TQcVEbHbbH", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.591.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 951000, "runtime_str": "951 us", "start_timestamp": "23:14:19.764.764215", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059764263000, "dur": 75000, "relative_dur": 0.007322788517867604, "relative_gap_to_previous": 0.0009763718023823472, "parent_is_longest": true, "runtime_str": "75 us"}}, "gpu3": {"time": {"ts": 1678058059824010000, "dur": 852000, "relative_dur": 0.08318687756297598, "relative_gap_to_previous": 0.00019527436047646945, "parent_is_longest": true, "runtime_str": "852 us"}}}, "is_backward_op": true, "id": "7XXwUhopFiF4XpnY", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.592.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 852000, "runtime_str": "852 us", "start_timestamp": "23:14:19.764.764263", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059764344000, "dur": 37000, "relative_dur": 0.0036125756688146844, "relative_gap_to_previous": 0.0005858230814294083, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1678058059824863000, "dur": 81000, "relative_dur": 0.007908611599297012, "relative_gap_to_previous": 9.763718023823472e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "pT1bpgznmPtRzR2f", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.593.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "23:14:19.764.764344", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059764385000, "dur": 225000, "relative_dur": 0.021968365553602813, "relative_gap_to_previous": 0.0003905487209529389, "parent_is_longest": true, "runtime_str": "225 us"}}, "gpu3": {"time": {"ts": 1678058059824944000, "dur": 1817000, "relative_dur": 0.1774067564928725, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "1.82 ms"}}}, "is_backward_op": true, "id": "EG8k1knuQ9OfUzBm", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.594.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1817000, "runtime_str": "1.82 ms", "start_timestamp": "23:14:19.764.764385", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059764616000, "dur": 184000, "relative_dur": 0.01796524116383519, "relative_gap_to_previous": 0.0005858230814294083, "parent_is_longest": true, "runtime_str": "184 us"}}, "gpu3": {"time": {"ts": 1678058059826762000, "dur": 1856000, "relative_dur": 0.18121460652216365, "relative_gap_to_previous": 9.763718023823472e-05, "parent_is_longest": true, "runtime_str": "1.86 ms"}}}, "is_backward_op": true, "id": "mhVnQyl1L5VYHpjJ", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.595.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1856000, "runtime_str": "1.86 ms", "start_timestamp": "23:14:19.764.764616", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059764806000, "dur": 187000, "relative_dur": 0.018258152704549894, "relative_gap_to_previous": 0.0005858230814294083, "parent_is_longest": true, "runtime_str": "187 us"}}, "gpu3": {"time": {"ts": 1678058059828620000, "dur": 1784000, "relative_dur": 0.17418472954501074, "relative_gap_to_previous": 0.00019527436047646945, "parent_is_longest": true, "runtime_str": "1.78 ms"}}}, "is_backward_op": true, "id": "YalO5u5UfxhGOnsQ", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.596.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1784000, "runtime_str": "1.78 ms", "start_timestamp": "23:14:19.764.764806", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059763890000, "dur": 1103000, "relative_dur": 0.046536157286304954, "relative_gap_to_previous": 0.00021095266222259724, "parent_is_longest": true, "runtime_str": "1.10 ms"}}, "gpu3": {"time": {"ts": 1678058059820162000, "dur": 10242000, "relative_dur": 0.4321154332967682, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "10.2 ms"}}}, "is_backward_op": true, "id": "BxrNyTllKokEZ0hU", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.587.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10242000, "runtime_str": "10.2 ms", "start_timestamp": "23:14:19.763.763890", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu4": {"time": {"ts": 1678058059765000000, "dur": 178000, "relative_dur": 0.007509914775124462, "relative_gap_to_previous": 0.00029533372711163615, "parent_is_longest": true, "runtime_str": "178 us"}}, "gpu3": {"time": {"ts": 1678058059830405000, "dur": 592000, "relative_dur": 0.024976795207155515, "relative_gap_to_previous": 4.219053244451945e-05, "parent_is_longest": true, "runtime_str": "592 us"}}}, "is_backward_op": true, "id": "0b4BqCYLyzIYccMx", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.597.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 592000, "runtime_str": "592 us", "start_timestamp": "23:14:19.765.765000", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 43}], "resources": {"cpu4": {"time": {"ts": 1678058059763127000, "dur": 2051000, "relative_dur": 0.014389852032189489, "relative_gap_to_previous": 4.209610540864794e-05, "parent_is_longest": true, "runtime_str": "2.05 ms"}}, "gpu3": {"time": {"ts": 1678058059807295000, "dur": 23702000, "relative_dur": 0.16629364839929558, "relative_gap_to_previous": 7.01601756810799e-06, "parent_is_longest": true, "runtime_str": "23.7 ms"}}}, "is_backward_op": true, "id": "SHUZyEMWRDoByxmQ", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.578.pt.trace.json", "trace_disk_size": "69.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 23702000, "runtime_str": "23.7 ms", "start_timestamp": "23:14:19.763.763127", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 527}, {"name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu4": {"time": {"ts": 1678058059765184000, "dur": 76000, "relative_dur": 0.006164828033744322, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "76 us"}}, "gpu3": {"time": {"ts": 1678058059830998000, "dur": 99000, "relative_dur": 0.008030499675535367, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "99 us"}}}, "is_backward_op": true, "id": "oH5q2hx75Y3QrbYh", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.600.pt.trace.json", "trace_disk_size": "2.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "23:14:19.765.765184", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu4": {"time": {"ts": 1678058059765265000, "dur": 155000, "relative_dur": 0.012573004542504866, "relative_gap_to_previous": 0.0004055807916937054, "parent_is_longest": true, "runtime_str": "155 us"}}, "gpu3": {"time": {"ts": 1678058059831098000, "dur": 5556000, "relative_dur": 0.4506813757300454, "relative_gap_to_previous": 8.111615833874107e-05, "parent_is_longest": true, "runtime_str": "5.56 ms"}}}, "is_backward_op": true, "id": "3JkFjI9Tt9CfDRdr", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.601.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5556000, "runtime_str": "5.56 ms", "start_timestamp": "23:14:19.765.765265", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu4": {"time": {"ts": 1678058059765428000, "dur": 100000, "relative_dur": 0.008111615833874108, "relative_gap_to_previous": 0.0006489292667099286, "parent_is_longest": true, "runtime_str": "100 us"}}, "gpu3": {"time": {"ts": 1678058059836655000, "dur": 423000, "relative_dur": 0.034312134977287474, "relative_gap_to_previous": 8.111615833874107e-05, "parent_is_longest": true, "runtime_str": "423 us"}}}, "is_backward_op": true, "id": "jKbuwrrPVcYnt1ou", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.602.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 423000, "runtime_str": "423 us", "start_timestamp": "23:14:19.765.765428", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu4": {"time": {"ts": 1678058059765532000, "dur": 24000, "relative_dur": 0.001946787800129786, "relative_gap_to_previous": 0.0003244646333549643, "parent_is_longest": true, "runtime_str": "24 us"}}, "gpu3": {"time": {"ts": 1678058059837079000, "dur": 475000, "relative_dur": 0.03853017521090201, "relative_gap_to_previous": 8.111615833874107e-05, "parent_is_longest": true, "runtime_str": "475 us"}}}, "is_backward_op": true, "id": "hKdEBHdUw42Ek7w9", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.603.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 475000, "runtime_str": "475 us", "start_timestamp": "23:14:19.765.765532", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu4": {"time": {"ts": 1678058059765560000, "dur": 173000, "relative_dur": 0.014033095392602207, "relative_gap_to_previous": 0.0003244646333549643, "parent_is_longest": true, "runtime_str": "173 us"}}, "gpu3": {"time": {"ts": 1678058059837557000, "dur": 5769000, "relative_dur": 0.4679591174561973, "relative_gap_to_previous": 0.00024334847501622324, "parent_is_longest": true, "runtime_str": "5.77 ms"}}}, "is_backward_op": true, "id": "WMYwM19ZnVa2oPfF", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.604.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5769000, "runtime_str": "5.77 ms", "start_timestamp": "23:14:19.765.765560", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1678058059765184000, "dur": 549000, "relative_dur": 0.023164556962025316, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "549 us"}}, "gpu3": {"time": {"ts": 1678058059830998000, "dur": 12328000, "relative_dur": 0.520168776371308, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12.3 ms"}}}, "is_backward_op": true, "id": "19AB1Xj9YMZnt9yn", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.599.pt.trace.json", "trace_disk_size": "17.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12328000, "runtime_str": "12.3 ms", "start_timestamp": "23:14:19.765.765184", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 134}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu4": {"time": {"ts": 1678058059765741000, "dur": 150000, "relative_dur": 0.006329113924050633, "relative_gap_to_previous": 0.00033755274261603374, "parent_is_longest": true, "runtime_str": "150 us"}}, "gpu3": {"time": {"ts": 1678058059843327000, "dur": 435000, "relative_dur": 0.018354430379746836, "relative_gap_to_previous": 4.219409282700422e-05, "parent_is_longest": true, "runtime_str": "435 us"}}}, "is_backward_op": true, "id": "LLSdkAv6zUlRlz26", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.605.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 435000, "runtime_str": "435 us", "start_timestamp": "23:14:19.765.765741", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu4": {"time": {"ts": 1678058059765897000, "dur": 76000, "relative_dur": 0.0032067510548523205, "relative_gap_to_previous": 0.00025316455696202533, "parent_is_longest": true, "runtime_str": "76 us"}}, "gpu3": {"time": {"ts": 1678058059843763000, "dur": 99000, "relative_dur": 0.004177215189873418, "relative_gap_to_previous": 4.219409282700422e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "is_backward_op": true, "id": "HbQWuZUVpyUAeVXj", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.606.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "23:14:19.765.765897", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059765977000, "dur": 149000, "relative_dur": 0.014570702131820849, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "149 us"}}, "gpu3": {"time": {"ts": 1678058059843863000, "dur": 1726000, "relative_dur": 0.1687854488558576, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.73 ms"}}}, "is_backward_op": true, "id": "EPnV894nBL11dtRY", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.608.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1726000, "runtime_str": "1.73 ms", "start_timestamp": "23:14:19.765.765977", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059766132000, "dur": 154000, "relative_dur": 0.015059651867787991, "relative_gap_to_previous": 0.000586739683160571, "parent_is_longest": true, "runtime_str": "154 us"}}, "gpu3": {"time": {"ts": 1678058059845591000, "dur": 800000, "relative_dur": 0.07823195775474281, "relative_gap_to_previous": 0.00019557989438685703, "parent_is_longest": true, "runtime_str": "800 us"}}}, "is_backward_op": true, "id": "CF7CvaK28YpPcY2V", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.609.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 800000, "runtime_str": "800 us", "start_timestamp": "23:14:19.766.766132", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059766292000, "dur": 29000, "relative_dur": 0.002835908468609427, "relative_gap_to_previous": 0.000586739683160571, "parent_is_longest": true, "runtime_str": "29 us"}}, "gpu3": {"time": {"ts": 1678058059846393000, "dur": 360000, "relative_dur": 0.035204380989634264, "relative_gap_to_previous": 0.00019557989438685703, "parent_is_longest": true, "runtime_str": "360 us"}}}, "is_backward_op": true, "id": "fWT4vgLwpAmg5mQZ", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.610.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "23:14:19.766.766292", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059766326000, "dur": 38000, "relative_dur": 0.0037160179933502834, "relative_gap_to_previous": 0.0004889497359671426, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1678058059846754000, "dur": 952000, "relative_dur": 0.09309602972814394, "relative_gap_to_previous": 9.778994719342852e-05, "parent_is_longest": true, "runtime_str": "952 us"}}}, "is_backward_op": true, "id": "gjwKVve9BSRzgigg", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.611.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "23:14:19.766.766326", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059766369000, "dur": 71000, "relative_dur": 0.006943086250733425, "relative_gap_to_previous": 0.0004889497359671426, "parent_is_longest": true, "runtime_str": "71 us"}}, "gpu3": {"time": {"ts": 1678058059847707000, "dur": 850000, "relative_dur": 0.08312145511441424, "relative_gap_to_previous": 9.778994719342852e-05, "parent_is_longest": true, "runtime_str": "850 us"}}}, "is_backward_op": true, "id": "oEPI84kxe6FBvdt4", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.612.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 850000, "runtime_str": "850 us", "start_timestamp": "23:14:19.766.766369", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059766445000, "dur": 37000, "relative_dur": 0.003618228046156855, "relative_gap_to_previous": 0.0004889497359671426, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1678058059848558000, "dur": 82000, "relative_dur": 0.008018775669861138, "relative_gap_to_previous": 9.778994719342852e-05, "parent_is_longest": true, "runtime_str": "82 us"}}}, "is_backward_op": true, "id": "iCc0vQP0sOTKlNZI", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.613.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 82000, "runtime_str": "82 us", "start_timestamp": "23:14:19.766.766445", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059766486000, "dur": 228000, "relative_dur": 0.022296107960101702, "relative_gap_to_previous": 0.00039115978877371407, "parent_is_longest": true, "runtime_str": "228 us"}}, "gpu3": {"time": {"ts": 1678058059848641000, "dur": 1812000, "relative_dur": 0.17719538431449247, "relative_gap_to_previous": 9.778994719342852e-05, "parent_is_longest": true, "runtime_str": "1.81 ms"}}}, "is_backward_op": true, "id": "ekMHCxM5vwsh2goC", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.614.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1812000, "runtime_str": "1.81 ms", "start_timestamp": "23:14:19.766.766486", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059766720000, "dur": 181000, "relative_dur": 0.01769998044201056, "relative_gap_to_previous": 0.000586739683160571, "parent_is_longest": true, "runtime_str": "181 us"}}, "gpu3": {"time": {"ts": 1678058059850454000, "dur": 1851000, "relative_dur": 0.18100919225503617, "relative_gap_to_previous": 9.778994719342852e-05, "parent_is_longest": true, "runtime_str": "1.85 ms"}}}, "is_backward_op": true, "id": "dLIpNLWKywSXbXKt", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.615.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1851000, "runtime_str": "1.85 ms", "start_timestamp": "23:14:19.766.766720", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059766907000, "dur": 192000, "relative_dur": 0.018775669861138274, "relative_gap_to_previous": 0.000586739683160571, "parent_is_longest": true, "runtime_str": "192 us"}}, "gpu3": {"time": {"ts": 1678058059852306000, "dur": 1783000, "relative_dur": 0.17435947584588304, "relative_gap_to_previous": 9.778994719342852e-05, "parent_is_longest": true, "runtime_str": "1.78 ms"}}}, "is_backward_op": true, "id": "SzRkkvlTzLAAHCy0", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.616.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1783000, "runtime_str": "1.78 ms", "start_timestamp": "23:14:19.766.766907", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059765977000, "dur": 1122000, "relative_dur": 0.047341772151898734, "relative_gap_to_previous": 0.00016877637130801687, "parent_is_longest": true, "runtime_str": "1.12 ms"}}, "gpu3": {"time": {"ts": 1678058059843863000, "dur": 10226000, "relative_dur": 0.43147679324894517, "relative_gap_to_previous": 4.219409282700422e-05, "parent_is_longest": true, "runtime_str": "10.2 ms"}}}, "is_backward_op": true, "id": "VnqEHG1mLRw4LxWI", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.607.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10226000, "runtime_str": "10.2 ms", "start_timestamp": "23:14:19.765.765977", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu4": {"time": {"ts": 1678058059767105000, "dur": 176000, "relative_dur": 0.007426160337552743, "relative_gap_to_previous": 0.00025316455696202533, "parent_is_longest": true, "runtime_str": "176 us"}}, "gpu3": {"time": {"ts": 1678058059854090000, "dur": 608000, "relative_dur": 0.025654008438818564, "relative_gap_to_previous": 4.219409282700422e-05, "parent_is_longest": true, "runtime_str": "608 us"}}}, "is_backward_op": true, "id": "nuz2d8tzelQGjxZ3", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.617.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 608000, "runtime_str": "608 us", "start_timestamp": "23:14:19.767.767105", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 43}], "resources": {"cpu4": {"time": {"ts": 1678058059765184000, "dur": 2097000, "relative_dur": 0.014712588840322456, "relative_gap_to_previous": 4.209610540864794e-05, "parent_is_longest": true, "runtime_str": "2.10 ms"}}, "gpu3": {"time": {"ts": 1678058059830998000, "dur": 23700000, "relative_dur": 0.1662796163641594, "relative_gap_to_previous": 7.01601756810799e-06, "parent_is_longest": true, "runtime_str": "23.7 ms"}}}, "is_backward_op": true, "id": "Ljb2mc10ikl5xExT", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.598.pt.trace.json", "trace_disk_size": "69.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 23700000, "runtime_str": "23.7 ms", "start_timestamp": "23:14:19.765.765184", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 527}, {"name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu4": {"time": {"ts": 1678058059767287000, "dur": 77000, "relative_dur": 0.006246450880181715, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "77 us"}}, "gpu3": {"time": {"ts": 1678058059854699000, "dur": 98000, "relative_dur": 0.007950028392958546, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "98 us"}}}, "is_backward_op": true, "id": "qyuk9HeS0mnZE72B", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.620.pt.trace.json", "trace_disk_size": "2.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 98000, "runtime_str": "98 us", "start_timestamp": "23:14:19.767.767287", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu4": {"time": {"ts": 1678058059767368000, "dur": 155000, "relative_dur": 0.01257402449906709, "relative_gap_to_previous": 0.00032449095481463456, "parent_is_longest": true, "runtime_str": "155 us"}}, "gpu3": {"time": {"ts": 1678058059854798000, "dur": 5557000, "relative_dur": 0.450799058976231, "relative_gap_to_previous": 8.112273870365864e-05, "parent_is_longest": true, "runtime_str": "5.56 ms"}}}, "is_backward_op": true, "id": "WPDkR7xRcoI5w83X", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.621.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5557000, "runtime_str": "5.56 ms", "start_timestamp": "23:14:19.767.767368", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu4": {"time": {"ts": 1678058059767531000, "dur": 98000, "relative_dur": 0.007950028392958546, "relative_gap_to_previous": 0.0006489819096292691, "parent_is_longest": true, "runtime_str": "98 us"}}, "gpu3": {"time": {"ts": 1678058059860357000, "dur": 423000, "relative_dur": 0.0343149184716476, "relative_gap_to_previous": 0.00016224547740731728, "parent_is_longest": true, "runtime_str": "423 us"}}}, "is_backward_op": true, "id": "h3MYMphaT81nN2Zx", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.622.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 423000, "runtime_str": "423 us", "start_timestamp": "23:14:19.767.767531", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu4": {"time": {"ts": 1678058059767634000, "dur": 23000, "relative_dur": 0.0018658229901841487, "relative_gap_to_previous": 0.0004056136935182932, "parent_is_longest": true, "runtime_str": "23 us"}}, "gpu3": {"time": {"ts": 1678058059860782000, "dur": 474000, "relative_dur": 0.03845217814553419, "relative_gap_to_previous": 0.00016224547740731728, "parent_is_longest": true, "runtime_str": "474 us"}}}, "is_backward_op": true, "id": "CMPmkCMYsYyAaKZm", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.623.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 474000, "runtime_str": "474 us", "start_timestamp": "23:14:19.767.767634", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu4": {"time": {"ts": 1678058059767662000, "dur": 147000, "relative_dur": 0.01192504258943782, "relative_gap_to_previous": 0.0004056136935182932, "parent_is_longest": true, "runtime_str": "147 us"}}, "gpu3": {"time": {"ts": 1678058059861258000, "dur": 5768000, "relative_dur": 0.467915956842703, "relative_gap_to_previous": 0.00016224547740731728, "parent_is_longest": true, "runtime_str": "5.77 ms"}}}, "is_backward_op": true, "id": "mtidhKYqWbjdwvHz", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.624.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5768000, "runtime_str": "5.77 ms", "start_timestamp": "23:14:19.767.767662", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1678058059767287000, "dur": 522000, "relative_dur": 0.022052300283046767, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "522 us"}}, "gpu3": {"time": {"ts": 1678058059854699000, "dur": 12327000, "relative_dur": 0.5207638038105699, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12.3 ms"}}}, "is_backward_op": true, "id": "1YniJIAHS5BKLC8j", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.619.pt.trace.json", "trace_disk_size": "17.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12327000, "runtime_str": "12.3 ms", "start_timestamp": "23:14:19.767.767287", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 134}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu4": {"time": {"ts": 1678058059767816000, "dur": 151000, "relative_dur": 0.00637911368341008, "relative_gap_to_previous": 0.0002957205018799375, "parent_is_longest": true, "runtime_str": "151 us"}}, "gpu3": {"time": {"ts": 1678058059867028000, "dur": 432000, "relative_dur": 0.018250179544590427, "relative_gap_to_previous": 8.449157196569642e-05, "parent_is_longest": true, "runtime_str": "432 us"}}}, "is_backward_op": true, "id": "0wJQfXhayE5SIK8s", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.625.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 432000, "runtime_str": "432 us", "start_timestamp": "23:14:19.767.767816", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu4": {"time": {"ts": 1678058059767973000, "dur": 77000, "relative_dur": 0.0032529255206793124, "relative_gap_to_previous": 0.0002534747158970893, "parent_is_longest": true, "runtime_str": "77 us"}}, "gpu3": {"time": {"ts": 1678058059867461000, "dur": 99000, "relative_dur": 0.004182332812301973, "relative_gap_to_previous": 4.224578598284821e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "is_backward_op": true, "id": "CcrKuFjgEc1NXhvm", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.626.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "23:14:19.767.767973", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059768054000, "dur": 142000, "relative_dur": 0.013897044431395576, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "142 us"}}, "gpu3": {"time": {"ts": 1678058059867562000, "dur": 1726000, "relative_dur": 0.16891759639851242, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.73 ms"}}}, "is_backward_op": true, "id": "mjZG5HW3f6pekEP2", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.628.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1726000, "runtime_str": "1.73 ms", "start_timestamp": "23:14:19.768.768054", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059768202000, "dur": 154000, "relative_dur": 0.015071442552358583, "relative_gap_to_previous": 0.0005871990604815032, "parent_is_longest": true, "runtime_str": "154 us"}}, "gpu3": {"time": {"ts": 1678058059869289000, "dur": 799000, "relative_dur": 0.07819534155412018, "relative_gap_to_previous": 9.786651008025054e-05, "parent_is_longest": true, "runtime_str": "799 us"}}}, "is_backward_op": true, "id": "8TwBzv3jMgKM3AIL", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.629.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 799000, "runtime_str": "799 us", "start_timestamp": "23:14:19.768.768202", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059768362000, "dur": 28000, "relative_dur": 0.002740262282247015, "relative_gap_to_previous": 0.0005871990604815032, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1678058059870089000, "dur": 359000, "relative_dur": 0.03513407711880994, "relative_gap_to_previous": 9.786651008025054e-05, "parent_is_longest": true, "runtime_str": "359 us"}}}, "is_backward_op": true, "id": "kVlsQPdNaEynJ20m", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.630.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "23:14:19.768.768362", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059768395000, "dur": 38000, "relative_dur": 0.0037189273830495204, "relative_gap_to_previous": 0.0004893325504012527, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1678058059870450000, "dur": 950000, "relative_dur": 0.09297318457623802, "relative_gap_to_previous": 0.00019573302016050108, "parent_is_longest": true, "runtime_str": "950 us"}}}, "is_backward_op": true, "id": "0KnKz9ySMlDCq3AL", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.631.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 950000, "runtime_str": "950 us", "start_timestamp": "23:14:19.768.768395", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059768437000, "dur": 71000, "relative_dur": 0.006948522215697788, "relative_gap_to_previous": 0.00039146604032100216, "parent_is_longest": true, "runtime_str": "71 us"}}, "gpu3": {"time": {"ts": 1678058059871402000, "dur": 850000, "relative_dur": 0.08318653356821296, "relative_gap_to_previous": 0.00019573302016050108, "parent_is_longest": true, "runtime_str": "850 us"}}}, "is_backward_op": true, "id": "SuqJkae98n8aMrcY", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.632.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 850000, "runtime_str": "850 us", "start_timestamp": "23:14:19.768.768437", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059768514000, "dur": 36000, "relative_dur": 0.0035231943628890195, "relative_gap_to_previous": 0.0005871990604815032, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1678058059872253000, "dur": 81000, "relative_dur": 0.007927187316500293, "relative_gap_to_previous": 9.786651008025054e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "ne0P9ccfpgDVrUWC", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.633.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "23:14:19.768.768514", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059768555000, "dur": 229000, "relative_dur": 0.02241143080837737, "relative_gap_to_previous": 0.0004893325504012527, "parent_is_longest": true, "runtime_str": "229 us"}}, "gpu3": {"time": {"ts": 1678058059872335000, "dur": 1812000, "relative_dur": 0.17733411626541398, "relative_gap_to_previous": 9.786651008025054e-05, "parent_is_longest": true, "runtime_str": "1.81 ms"}}}, "is_backward_op": true, "id": "twY7aRZrWDaMBEer", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.634.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1812000, "runtime_str": "1.81 ms", "start_timestamp": "23:14:19.768.768555", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059768790000, "dur": 207000, "relative_dur": 0.020258367586611863, "relative_gap_to_previous": 0.0005871990604815032, "parent_is_longest": true, "runtime_str": "207 us"}}, "gpu3": {"time": {"ts": 1678058059874148000, "dur": 1848000, "relative_dur": 0.180857310628303, "relative_gap_to_previous": 9.786651008025054e-05, "parent_is_longest": true, "runtime_str": "1.85 ms"}}}, "is_backward_op": true, "id": "nmCjGQwHMfEeWT00", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.635.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1848000, "runtime_str": "1.85 ms", "start_timestamp": "23:14:19.768.768790", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059769003000, "dur": 186000, "relative_dur": 0.0182031708749266, "relative_gap_to_previous": 0.0005871990604815032, "parent_is_longest": true, "runtime_str": "186 us"}}, "gpu3": {"time": {"ts": 1678058059875997000, "dur": 1783000, "relative_dur": 0.1744959874730867, "relative_gap_to_previous": 9.786651008025054e-05, "parent_is_longest": true, "runtime_str": "1.78 ms"}}}, "is_backward_op": true, "id": "FMxkRPCDfhlsvK9n", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.636.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1783000, "runtime_str": "1.78 ms", "start_timestamp": "23:14:19.769.769003", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059768054000, "dur": 1135000, "relative_dur": 0.04794896709053272, "relative_gap_to_previous": 0.00016898314393139284, "parent_is_longest": true, "runtime_str": "1.14 ms"}}, "gpu3": {"time": {"ts": 1678058059867562000, "dur": 10218000, "relative_dur": 0.431667441172743, "relative_gap_to_previous": 8.449157196569642e-05, "parent_is_longest": true, "runtime_str": "10.2 ms"}}}, "is_backward_op": true, "id": "9eEjsQqHgcvV2FlH", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.627.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10218000, "runtime_str": "10.2 ms", "start_timestamp": "23:14:19.768.768054", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu4": {"time": {"ts": 1678058059769195000, "dur": 186000, "relative_dur": 0.007857716192809767, "relative_gap_to_previous": 0.0002534747158970893, "parent_is_longest": true, "runtime_str": "186 us"}}, "gpu3": {"time": {"ts": 1678058059877781000, "dur": 589000, "relative_dur": 0.024882767943897596, "relative_gap_to_previous": 4.224578598284821e-05, "parent_is_longest": true, "runtime_str": "589 us"}}}, "is_backward_op": true, "id": "Cp6WUfOq0eQaDckS", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.637.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 589000, "runtime_str": "589 us", "start_timestamp": "23:14:19.769.769195", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 43}], "resources": {"cpu4": {"time": {"ts": 1678058059767287000, "dur": 2094000, "relative_dur": 0.014691540787618132, "relative_gap_to_previous": 4.209610540864794e-05, "parent_is_longest": true, "runtime_str": "2.09 ms"}}, "gpu3": {"time": {"ts": 1678058059854699000, "dur": 23671000, "relative_dur": 0.16607615185468425, "relative_gap_to_previous": 7.01601756810799e-06, "parent_is_longest": true, "runtime_str": "23.7 ms"}}}, "is_backward_op": true, "id": "a1V1KV45SO0ip5zQ", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.618.pt.trace.json", "trace_disk_size": "69.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 23671000, "runtime_str": "23.7 ms", "start_timestamp": "23:14:19.767.767287", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 527}, {"name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu4": {"time": {"ts": 1678058059769388000, "dur": 82000, "relative_dur": 0.0066542238091373855, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "82 us"}}, "gpu3": {"time": {"ts": 1678058059878371000, "dur": 98000, "relative_dur": 0.007952608942627606, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "98 us"}}}, "is_backward_op": true, "id": "tPenuFRjp0Df4eUy", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.640.pt.trace.json", "trace_disk_size": "2.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 98000, "runtime_str": "98 us", "start_timestamp": "23:14:19.769.769388", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu4": {"time": {"ts": 1678058059769475000, "dur": 146000, "relative_dur": 0.011847764343098272, "relative_gap_to_previous": 0.0004057453542156942, "parent_is_longest": true, "runtime_str": "146 us"}}, "gpu3": {"time": {"ts": 1678058059878471000, "dur": 5558000, "relative_dur": 0.4510265357461657, "relative_gap_to_previous": 0.00016229814168627768, "parent_is_longest": true, "runtime_str": "5.56 ms"}}}, "is_backward_op": true, "id": "kAP4OYEzXDmFhHSF", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.641.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5558000, "runtime_str": "5.56 ms", "start_timestamp": "23:14:19.769.769475", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu4": {"time": {"ts": 1678058059769628000, "dur": 99000, "relative_dur": 0.008033758013470746, "relative_gap_to_previous": 0.0005680434959019719, "parent_is_longest": true, "runtime_str": "99 us"}}, "gpu3": {"time": {"ts": 1678058059884030000, "dur": 424000, "relative_dur": 0.034407206037490874, "relative_gap_to_previous": 8.114907084313884e-05, "parent_is_longest": true, "runtime_str": "424 us"}}}, "is_backward_op": true, "id": "H7t6XqUz4gkuJhFO", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.642.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 424000, "runtime_str": "424 us", "start_timestamp": "23:14:19.769.769628", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu4": {"time": {"ts": 1678058059769731000, "dur": 25000, "relative_dur": 0.0020287267710784713, "relative_gap_to_previous": 0.00032459628337255537, "parent_is_longest": true, "runtime_str": "25 us"}}, "gpu3": {"time": {"ts": 1678058059884455000, "dur": 475000, "relative_dur": 0.038545808650490954, "relative_gap_to_previous": 8.114907084313884e-05, "parent_is_longest": true, "runtime_str": "475 us"}}}, "is_backward_op": true, "id": "mg6L0Gx6ECkGwakd", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.643.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 475000, "runtime_str": "475 us", "start_timestamp": "23:14:19.769.769731", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu4": {"time": {"ts": 1678058059769760000, "dur": 161000, "relative_dur": 0.013065000405745354, "relative_gap_to_previous": 0.00032459628337255537, "parent_is_longest": true, "runtime_str": "161 us"}}, "gpu3": {"time": {"ts": 1678058059884931000, "dur": 5763000, "relative_dur": 0.46766209526900915, "relative_gap_to_previous": 8.114907084313884e-05, "parent_is_longest": true, "runtime_str": "5.76 ms"}}}, "is_backward_op": true, "id": "jRTJYpWLceZqMHQH", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.644.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5763000, "runtime_str": "5.76 ms", "start_timestamp": "23:14:19.769.769760", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1678058059769388000, "dur": 533000, "relative_dur": 0.02249419708799325, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "533 us"}}, "gpu3": {"time": {"ts": 1678058059878371000, "dur": 12323000, "relative_dur": 0.5200675247942604, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12.3 ms"}}}, "is_backward_op": true, "id": "vkyXSZtC0pt30Y7P", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.639.pt.trace.json", "trace_disk_size": "17.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12323000, "runtime_str": "12.3 ms", "start_timestamp": "23:14:19.769.769388", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 134}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu4": {"time": {"ts": 1678058059769929000, "dur": 139000, "relative_dur": 0.0058662165013715975, "relative_gap_to_previous": 0.0003376239713019624, "parent_is_longest": true, "runtime_str": "139 us"}}, "gpu3": {"time": {"ts": 1678058059890695000, "dur": 442000, "relative_dur": 0.018653724414433425, "relative_gap_to_previous": 4.22029964127453e-05, "parent_is_longest": true, "runtime_str": "442 us"}}}, "is_backward_op": true, "id": "iE6Fz7pRyU451VxX", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.645.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 442000, "runtime_str": "442 us", "start_timestamp": "23:14:19.769.769929", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu4": {"time": {"ts": 1678058059770074000, "dur": 76000, "relative_dur": 0.0032074277273686433, "relative_gap_to_previous": 0.00025321797847647186, "parent_is_longest": true, "runtime_str": "76 us"}}, "gpu3": {"time": {"ts": 1678058059891139000, "dur": 99000, "relative_dur": 0.0041780966448617856, "relative_gap_to_previous": 8.44059928254906e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "is_backward_op": true, "id": "6ARNn2yQCO6XWRY2", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.646.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "23:14:19.770.770074", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1678058059770155000, "dur": 153000, "relative_dur": 0.014961861920594562, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "153 us"}}, "gpu3": {"time": {"ts": 1678058059891239000, "dur": 1727000, "relative_dur": 0.16888323880305103, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1.73 ms"}}}, "is_backward_op": true, "id": "4ghq7GnTjmSo0ywz", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.648.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1727000, "runtime_str": "1.73 ms", "start_timestamp": "23:14:19.770.770155", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1678058059770315000, "dur": 141000, "relative_dur": 0.01378838255427342, "relative_gap_to_previous": 0.0006845296303539996, "parent_is_longest": true, "runtime_str": "141 us"}}, "gpu3": {"time": {"ts": 1678058059892967000, "dur": 800000, "relative_dur": 0.07823195775474281, "relative_gap_to_previous": 9.778994719342852e-05, "parent_is_longest": true, "runtime_str": "800 us"}}}, "is_backward_op": true, "id": "aJjWLS1foe4eT3we", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.649.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 800000, "runtime_str": "800 us", "start_timestamp": "23:14:19.770.770315", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1678058059770461000, "dur": 28000, "relative_dur": 0.0027381185214159984, "relative_gap_to_previous": 0.0004889497359671426, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1678058059893768000, "dur": 359000, "relative_dur": 0.035106591042440835, "relative_gap_to_previous": 9.778994719342852e-05, "parent_is_longest": true, "runtime_str": "359 us"}}}, "is_backward_op": true, "id": "I7L4vJ6K0j7aPgEo", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.650.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "23:14:19.770.770461", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1678058059770493000, "dur": 38000, "relative_dur": 0.0037160179933502834, "relative_gap_to_previous": 0.00039115978877371407, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1678058059894128000, "dur": 951000, "relative_dur": 0.09299823978095052, "relative_gap_to_previous": 9.778994719342852e-05, "parent_is_longest": true, "runtime_str": "951 us"}}}, "is_backward_op": true, "id": "0lFlKtOiqmIpWBQz", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.651.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 951000, "runtime_str": "951 us", "start_timestamp": "23:14:19.770.770493", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1678058059770535000, "dur": 70000, "relative_dur": 0.006845296303539996, "relative_gap_to_previous": 0.00039115978877371407, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1678058059895081000, "dur": 850000, "relative_dur": 0.08312145511441424, "relative_gap_to_previous": 0.00019557989438685703, "parent_is_longest": true, "runtime_str": "850 us"}}}, "is_backward_op": true, "id": "KBQpr5xpwbX5wGrS", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.652.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 850000, "runtime_str": "850 us", "start_timestamp": "23:14:19.770.770535", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1678058059770610000, "dur": 36000, "relative_dur": 0.0035204380989634267, "relative_gap_to_previous": 0.0004889497359671426, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1678058059895932000, "dur": 81000, "relative_dur": 0.00792098572266771, "relative_gap_to_previous": 9.778994719342852e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "QJFTmqrrmY959KGs", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.653.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "23:14:19.770.770610", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1678058059770651000, "dur": 227000, "relative_dur": 0.022198318012908273, "relative_gap_to_previous": 0.0004889497359671426, "parent_is_longest": true, "runtime_str": "227 us"}}, "gpu3": {"time": {"ts": 1678058059896014000, "dur": 1814000, "relative_dur": 0.17739096420887931, "relative_gap_to_previous": 9.778994719342852e-05, "parent_is_longest": true, "runtime_str": "1.81 ms"}}}, "is_backward_op": true, "id": "yFvghCOjZiBh4GHs", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.654.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1814000, "runtime_str": "1.81 ms", "start_timestamp": "23:14:19.770.770651", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1678058059770884000, "dur": 192000, "relative_dur": 0.018775669861138274, "relative_gap_to_previous": 0.000586739683160571, "parent_is_longest": true, "runtime_str": "192 us"}}, "gpu3": {"time": {"ts": 1678058059897829000, "dur": 1851000, "relative_dur": 0.18100919225503617, "relative_gap_to_previous": 9.778994719342852e-05, "parent_is_longest": true, "runtime_str": "1.85 ms"}}}, "is_backward_op": true, "id": "8ovsrqDSiHO3exUS", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.655.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1851000, "runtime_str": "1.85 ms", "start_timestamp": "23:14:19.770.770884", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1678058059771082000, "dur": 782000, "relative_dur": 0.0764717387052611, "relative_gap_to_previous": 0.000586739683160571, "parent_is_longest": true, "runtime_str": "782 us"}}, "gpu3": {"time": {"ts": 1678058059899681000, "dur": 1784000, "relative_dur": 0.17445726579307647, "relative_gap_to_previous": 9.778994719342852e-05, "parent_is_longest": true, "runtime_str": "1.78 ms"}}}, "is_backward_op": true, "id": "n0F99g2G2sVOjwf1", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.656.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1784000, "runtime_str": "1.78 ms", "start_timestamp": "23:14:19.771.771082", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1678058059770155000, "dur": 1709000, "relative_dur": 0.07212492086938173, "relative_gap_to_previous": 0.00021101498206372652, "parent_is_longest": true, "runtime_str": "1.71 ms"}}, "gpu3": {"time": {"ts": 1678058059891239000, "dur": 10226000, "relative_dur": 0.4315678413167335, "relative_gap_to_previous": 4.22029964127453e-05, "parent_is_longest": true, "runtime_str": "10.2 ms"}}}, "is_backward_op": true, "id": "bSK9mWnb9lZJ1xMW", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.647.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10226000, "runtime_str": "10.2 ms", "start_timestamp": "23:14:19.770.770155", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu4": {"time": {"ts": 1678058059771870000, "dur": 194000, "relative_dur": 0.008187381304072589, "relative_gap_to_previous": 0.00025321797847647186, "parent_is_longest": true, "runtime_str": "194 us"}}, "gpu3": {"time": {"ts": 1678058059901466000, "dur": 600000, "relative_dur": 0.02532179784764718, "relative_gap_to_previous": 4.22029964127453e-05, "parent_is_longest": true, "runtime_str": "600 us"}}}, "is_backward_op": true, "id": "E78fLIP7B1Hc559e", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.657.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 600000, "runtime_str": "600 us", "start_timestamp": "23:14:19.771.771870", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 43}], "resources": {"cpu4": {"time": {"ts": 1678058059769388000, "dur": 2676000, "relative_dur": 0.018774863012256983, "relative_gap_to_previous": 4.9112122976755935e-05, "parent_is_longest": true, "runtime_str": "2.68 ms"}}, "gpu3": {"time": {"ts": 1678058059878371000, "dur": 23695000, "relative_dur": 0.16624453627631883, "relative_gap_to_previous": 7.01601756810799e-06, "parent_is_longest": true, "runtime_str": "23.7 ms"}}}, "is_backward_op": true, "id": "pVrRiDa4iPj3JPwC", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.638.pt.trace.json", "trace_disk_size": "69.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 23695000, "runtime_str": "23.7 ms", "start_timestamp": "23:14:19.769.769388", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 527}], "resources": {"cpu4": {"time": {"ts": 1678058059758875000, "dur": 13189000, "relative_dur": 0.09177127111803836, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "13.2 ms"}}, "gpu3": {"time": {"ts": 1678058059759535000, "dur": 142531000, "relative_dur": 0.9917545715160455, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "143 ms"}}}, "is_backward_op": true, "id": "QyxZntFhC2ly6m0c", "pretty_name": "TransformerEncoder", "trace_file": "/results/Transformer/Transformer.537.pt.trace.json", "trace_disk_size": "421.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 142531000, "runtime_str": "143 ms", "start_timestamp": "23:14:19.758.758875", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 3179}, {"name": "PositionalEncoding", "type": "PositionalEncoding", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 258, in forward\n    with hotline.annotate('PositionalEncoding'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 258, "ops": [{"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 259, in forward\n    src = self.pos_encoder(src, inputs_positions)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 372, in forward\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 372, "resources": {"cpu4": {"time": {"ts": 1678058059772071000, "dur": 71000, "relative_dur": 0.7171717171717171, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "71 us"}}, "gpu3": {"time": {"ts": 1678058059902067000, "dur": 99000, "relative_dur": 1.0, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "99 us"}}}, "is_backward_op": true, "id": "cSPOijG6oMRaCYnB", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.659.pt.trace.json", "trace_disk_size": "2.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "23:14:19.772.772071", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 17}], "resources": {"cpu4": {"time": {"ts": 1678058059772071000, "dur": 71000, "relative_dur": 0.0004940298922875672, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "71 us"}}, "gpu3": {"time": {"ts": 1678058059902067000, "dur": 99000, "relative_dur": 0.0006888585822037908, "relative_gap_to_previous": 6.958167497007988e-06, "parent_is_longest": true, "runtime_str": "99 us"}}}, "is_backward_op": true, "id": "d3COqjBpYihvabPJ", "pretty_name": "PositionalEncoding", "trace_file": "/results/Transformer/Transformer.658.pt.trace.json", "trace_disk_size": "2.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "23:14:19.772.772071", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 17}, {"name": "Embedding", "type": "Embedding", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 254, in forward\n    with hotline.annotate('Embedding'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 254, "resources": {"cpu4": {"time": {"ts": 1678058059772147000, "dur": 369000, "relative_dur": 0.0025675638063959475, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "369 us"}}, "gpu3": {"time": {"ts": 1678058059902168000, "dur": 1083000, "relative_dur": 0.007535695399259651, "relative_gap_to_previous": 1.3916334994015976e-05, "parent_is_longest": true, "runtime_str": "1.08 ms"}}}, "is_backward_op": true, "id": "oW3DK64zSjP9F5nX", "pretty_name": "Embedding", "trace_file": "/results/Transformer/Transformer.660.pt.trace.json", "trace_disk_size": "15.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1083000, "runtime_str": "1.08 ms", "start_timestamp": "23:14:19.772.772147", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 105}], "resources": {"cpu4": {"time": {"ts": 1678058059758875000, "dur": 13641000, "relative_dur": 0.03241397500695044, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "13.6 ms"}}, "gpu3": {"time": {"ts": 1678058059759535000, "dur": 143716000, "relative_dur": 0.34150039088768336, "relative_gap_to_previous": 2.376216920090201e-06, "parent_is_longest": true, "runtime_str": "144 ms"}}}, "is_backward_op": true, "id": "6GdoKE5e8lKnFnbF", "pretty_name": "Encoder", "trace_file": "/results/Transformer/Transformer.536.pt.trace.json", "trace_disk_size": "439.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 143716000, "runtime_str": "144 ms", "start_timestamp": "23:14:19.758.758875", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 3301}], "is_model_pass": "Backward", "idx": 324, "id": "QT3oOWGrc3Fh5Utf", "pretty_name": "Backward", "trace_file": "/results/Transformer/Transformer.342.pt.trace.json", "trace_disk_size": "1.2 MB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "runtime": 420837000, "runtime_str": "421 ms", "start_timestamp": "23:14:19.335.335228", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8902}, {"name": "Optimizer", "type": "training loop", "instances": 1, "resources": {"cpu2": {"time": {"ts": 1678058059903232000, "dur": 28246000, "relative_dur": 0.038583267652807965, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "28.2 ms"}}, "gpu3": {"time": {"ts": 1678058059904029000, "dur": 27431000, "relative_dur": 0.037469999822423536, "relative_gap_to_previous": 0.0004261835129815225, "parent_is_longest": true, "runtime_str": "27.4 ms"}}}, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 160, in update_params\n    with hotline.annotate('Optimizer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "source_file_num": 160, "ops": [{"name": "aten::sqrt(13%) and 9 others\u2026", "type": "generated", "generated_depth": 1, "instances": 1916, "resources": {"cpu2": {"time": {"ts": 1678058059903233000, "dur": 28226000, "relative_dur": 0.9992919351412589, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "28.2 ms"}, "res_name": "aten::sqrt(13%) and 9 others\u2026"}, "gpu3": {"time": {"ts": 1678058059904029000, "dur": 27431000, "relative_dur": 0.9711463570063018, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "27.4 ms"}, "res_name": "vectorized_elementwise_kernel<4 addcdiv_cuda_kernelTensorIteratorBase& c10::Scalar const&::{lambda#2}::operator const::{lambda#14}::operator const::{lambda  #1} Array<char 4> addcdiv_cuda_kernelTensorIteratorBase& c10::Scalar const&::{lambda#2}::operator const::{lambda#14}::operator const::{lambda  #1} Array<char 4>(20%) and 6 others\u2026"}}, "id": "wwGGUf9BkJpOrzet", "pretty_name": "aten::sqrt(13%) and 9 others\u2026", "trace_file": "/results/Transformer/Transformer.662.pt.trace.json", "trace_disk_size": "1.0 MB", "runtime": 28226000, "runtime_str": "28.2 ms", "start_timestamp": "23:14:19.903.903233", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 5737}], "idx": 325, "id": "7tTZFGVTprDd1tWZ", "pretty_name": "Optimizer", "trace_file": "/results/Transformer/Transformer.661.pt.trace.json", "trace_disk_size": "1.0 MB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "runtime": 28246000, "runtime_str": "28.2 ms", "start_timestamp": "23:14:19.903.903232", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 5738}], "resources": {"cpu2": {"time": {"ts": 1678058059199273000, "dur": 676148000, "parent_is_longest": true, "runtime_str": "676 ms"}}, "cpu4": {"time": {"ts": 1678058059335228000, "dur": 35498000, "parent_is_longest": false, "runtime_str": "35.5 ms"}}, "gpu3": {"time": {"ts": 1678058059199381000, "dur": 732079000, "parent_is_longest": true, "runtime_str": "732 ms"}}}, "source_file_name": "/home/ubuntu/algorithmic-efficiency/submission_runner.py", "source_file_num": 324, "id": "1zXSeB3wrbRxCjSm", "pretty_name": "Transformer Training Iteration", "total_accuracy_str": "98.57%", "trace_file": "/results/Transformer/Transformer.1.pt.trace.json", "trace_disk_size": "6.7 MB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/submission_runner.py", "config": {"trace_filepath": "/home/ubuntu/cpath/results/Transformer.worker0.pt.trace.json", "output_dir": "/home/ubuntu/cpath/results", "ui_traces_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/Transformer", "ui_model_path": "/home/ubuntu/cpath/results/ui/src/results/Transformer.js", "results_summary_csv_filepath": "/home/ubuntu/cpath/results/results_summary.csv", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer", "run_name": "Transformer", "model_name": "Transformer", "backend": "torch", "test": false, "is_test_accuracy": true, "view_manual_annotations": false, "source_file_name": "/home/ubuntu/algorithmic-efficiency/submission_runner.py", "source_file_num": 324, "num_gpus": 1, "slice_idx": 0, "drop_flow_view": "", "last_found_was_fused": false, "count_sum_greater_than_1": 0, "count_sum_less_than_1": 0, "tiny_op_threshold": 0.05, "max_generated_depth": 2, "remove_slice_args": true, "write_model_ops_to_file": true, "op_idx": 1}, "metadata": {}, "metadata.model": "Transformer", "metadata.dataset": "WMT17", "metadata.batch_size": 32, "metadata.optimizer": "Adam", "trace_event_count": 21293, "pytorch_version": "1.13.1+cu117", "gpu_model": "NVIDIA GeForce RTX 3090", "gpu_cuda_version": "11.7", "hotline_traces_trace_disk_size": "17.0 MB", "hotline_annotation_count": "662", "processed_datetime": "05/03/2023 23:14:31", "runtime_without_profiling": "718 ms \u00b10.1%", "runtime_with_profiling": "729 ms \u00b10.1%", "runtime_profiling_overhead_factor": "0.02\u00d7 slower", "hotline_analysis_time": "4.53 s", "runtime": 732079000, "runtime_str": "732 ms", "start_timestamp": "23:14:19.199.199273", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3"}]