export const model =
[{"name": "Transformer Training Iteration", "type": "root", "ops": [{"name": "Load Data", "type": "training loop", "instances": 27, "resources": {"cpu2": {"time": {"ts": 1677893021833875000, "dur": 2062000, "relative_dur": 0.0028281908356741574, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893021833992000, "dur": 449000, "relative_dur": 0.0006158378686797753, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "449 us"}}}, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 345, in train_once\n    with hotline.annotate('Load Data'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/submission_runner.py", "source_file_num": 345, "ops": [{"name": "aten::lift_fresh", "type": "generated", "generated_depth": 1, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1677893021833875000, "dur": 3000, "relative_dur": 0.001454898157129001, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 us"}, "res_name": "aten::lift_fresh"}}, "id": "2wq6bAwr2BmKN67G", "pretty_name": "aten::lift_fresh", "trace_file": "/results/Transformer/Transformer.3.pt.trace.json", "trace_disk_size": "99 Bytes", "runtime": 3000, "runtime_str": "3 us", "start_timestamp": "01:23:41.833.833875", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 1}, {"name": "aten::to", "type": "generated", "generated_depth": 1, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1677893021833890000, "dur": 80000, "relative_dur": 0.038797284190106696, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "80 us"}}, "gpu3": {"time": {"ts": 1677893021833992000, "dur": 5000, "relative_dur": 0.0024248302618816685, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "5 us"}}}, "ops": [{"name": "aten::empty_strided", "type": "generated", "generated_depth": 2, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1677893021833895000, "dur": 18000, "relative_dur": 0.225, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "18 us"}, "res_name": "aten::empty_strided"}}, "id": "zAbESpfoRkdCxBrs", "pretty_name": "aten::empty_strided", "trace_file": "/results/Transformer/Transformer.5.pt.trace.json", "trace_disk_size": "103 Bytes", "runtime": 18000, "runtime_str": "18 us", "start_timestamp": "01:23:41.833.833895", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 1}, {"name": "aten::copy_", "type": "generated", "generated_depth": 2, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1677893021833916000, "dur": 53000, "relative_dur": 0.6625, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "53 us"}}, "gpu3": {"time": {"ts": 1677893021833992000, "dur": 5000, "relative_dur": 0.0625, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "5 us"}}}, "ops": [{"name": "aten::to", "type": "generated", "generated_depth": 3, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1677893021833924000, "dur": 13000, "relative_dur": 0.24528301886792453, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "13 us"}, "res_name": "aten::to"}}, "id": "P85ckSbhjE5q4NX6", "pretty_name": "aten::to", "trace_file": "/results/Transformer/Transformer.7.pt.trace.json", "trace_disk_size": "383 Bytes", "runtime": 13000, "runtime_str": "13 us", "start_timestamp": "01:23:41.833.833924", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 4}, {"name": "aten::expand_as", "type": "generated", "generated_depth": 3, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1677893021833940000, "dur": 14000, "relative_dur": 0.2641509433962264, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "14 us"}, "res_name": "aten::expand_as"}}, "id": "62L0sLyzjRZDoTlc", "pretty_name": "aten::expand_as", "trace_file": "/results/Transformer/Transformer.8.pt.trace.json", "trace_disk_size": "291 Bytes", "runtime": 14000, "runtime_str": "14 us", "start_timestamp": "01:23:41.833.833940", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 3}], "id": "xW2NkQjqcln0n5Gs", "pretty_name": "aten::copy_", "trace_file": "/results/Transformer/Transformer.6.pt.trace.json", "trace_disk_size": "1.2 kB", "runtime": 53000, "runtime_str": "53 us", "start_timestamp": "01:23:41.833.833916", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 12}], "id": "0IiE3vcBtpcTUJeU", "pretty_name": "aten::to", "trace_file": "/results/Transformer/Transformer.4.pt.trace.json", "trace_disk_size": "1.5 kB", "runtime": 80000, "runtime_str": "80 us", "start_timestamp": "01:23:41.833.833890", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 15}, {"name": "aten::to(87%) and 4 others\u2026", "type": "generated", "generated_depth": 1, "instances": 19, "resources": {"cpu2": {"time": {"ts": 1677893021834026000, "dur": 1948000, "relative_dur": 0.944713870029098, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}, "res_name": "aten::to(87%) and 4 others\u2026"}, "gpu3": {"time": {"ts": 1677893021834101000, "dur": 340000, "relative_dur": 0.16488845780795344, "relative_gap_to_previous": 0.0504364694471387, "parent_is_longest": true, "runtime_str": "340 us"}, "res_name": "Memcpy HtoD Pageable -> Device"}}, "id": "pLsHBrfDE6OuTK9S", "pretty_name": "aten::to(87%) and 4 others\u2026", "trace_file": "/results/Transformer/Transformer.9.pt.trace.json", "trace_disk_size": "9.1 kB", "runtime": 1948000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.834.834026", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 93}], "idx": 1, "id": "m4zA3j2SlbtJ6cRb", "pretty_name": "Load Data", "trace_file": "/results/Transformer/Transformer.2.pt.trace.json", "trace_disk_size": "10.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/submission_runner.py", "runtime": 2062000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.833.833875", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 109}, {"name": "Forward", "type": "training loop", "instances": 6, "resources": {"cpu2": {"time": {"ts": 1677893021836082000, "dur": 76217000, "relative_dur": 0.10453744952598315, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "76 ms"}}, "gpu3": {"time": {"ts": 1677893021837451000, "dur": 236906000, "relative_dur": 0.32493471295646065, "relative_gap_to_previous": 0.004128445400280899, "parent_is_longest": true, "runtime_str": "237 ms"}}}, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 137, in update_params\n    with hotline.annotate('Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "source_file_num": 137, "ops": [{"idx": 4, "name": "Encoder", "type": "Encoder", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 205, in forward\n    with hotline.annotate('Encoder'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 205, "ops": [{"idx": 5, "name": "Embedding", "type": "Embedding", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 254, in forward\n    with hotline.annotate('Embedding'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 254, "resources": {"cpu2": {"time": {"ts": 1677893021836083000, "dur": 2294000, "relative_dur": 0.027106227106227107, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893021837451000, "dur": 1711000, "relative_dur": 0.02021741699161054, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "id": "gguTaDcISgM6ZFsb", "pretty_name": "Embedding", "trace_file": "/results/Transformer/Transformer.12.pt.trace.json", "trace_disk_size": "26.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 2294000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.836.836083", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 177}, {"idx": 6, "name": "PositionalEncoding", "type": "PositionalEncoding", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 258, in forward\n    with hotline.annotate('PositionalEncoding'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 258, "ops": [{"idx": 7, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 259, in forward\n    src = self.pos_encoder(src, inputs_positions)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 372, in forward\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 372, "resources": {"cpu2": {"time": {"ts": 1677893021838572000, "dur": 360000, "relative_dur": 1.0, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "360 us"}}, "gpu3": {"time": {"ts": 1677893021839164000, "dur": 275000, "relative_dur": 0.7638888888888888, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "275 us"}}}, "id": "f7A3pcBTVRi0mW7L", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.14.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "01:23:41.838.838572", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021838572000, "dur": 360000, "relative_dur": 0.004253810705423609, "relative_gap_to_previous": 0.0006971523100555359, "parent_is_longest": true, "runtime_str": "360 us"}}, "gpu3": {"time": {"ts": 1677893021839164000, "dur": 275000, "relative_dur": 0.003249438733309701, "relative_gap_to_previous": 2.3632281696797826e-05, "parent_is_longest": true, "runtime_str": "275 us"}}}, "id": "eXwZmkhAhhpOzjzN", "pretty_name": "PositionalEncoding", "trace_file": "/results/Transformer/Transformer.13.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "01:23:41.838.838572", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 30}, {"idx": 8, "name": "TransformerEncoder", "type": "TransformerEncoder", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 260, in forward\n    with hotline.annotate('TransformerEncoder'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 260, "ops": [{"idx": 9, "name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"idx": 10, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu2": {"time": {"ts": 1677893021839058000, "dur": 506000, "relative_dur": 0.037752741923449976, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "506 us"}}, "gpu3": {"time": {"ts": 1677893021839551000, "dur": 105000, "relative_dur": 0.00783406699992539, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "105 us"}}}, "id": "Yojv3vtX3XEGIpgO", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.17.pt.trace.json", "trace_disk_size": "2.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 506000, "runtime_str": "506 us", "start_timestamp": "01:23:41.839.839058", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 21}, {"idx": 11, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"idx": 12, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021839750000, "dur": 475000, "relative_dur": 0.08601955813111192, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "475 us"}}, "gpu3": {"time": {"ts": 1677893021840071000, "dur": 928000, "relative_dur": 0.1680550525172039, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "928 us"}}}, "id": "MqEV0GynNfUlun5P", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.19.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 928000, "runtime_str": "928 us", "start_timestamp": "01:23:41.839.839750", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 35}, {"idx": 13, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021840435000, "dur": 168000, "relative_dur": 0.030423759507424847, "relative_gap_to_previous": 0.03386454183266932, "parent_is_longest": true, "runtime_str": "168 us"}}, "gpu3": {"time": {"ts": 1677893021841000000, "dur": 877000, "relative_dur": 0.15881926838102137, "relative_gap_to_previous": 0.00018109380659181456, "parent_is_longest": true, "runtime_str": "877 us"}}}, "id": "BcpxrzkOVKoFBoi9", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.20.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 877000, "runtime_str": "877 us", "start_timestamp": "01:23:41.840.840435", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 14, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021840758000, "dur": 163000, "relative_dur": 0.029518290474465775, "relative_gap_to_previous": 0.02390438247011952, "parent_is_longest": true, "runtime_str": "163 us"}}, "gpu3": {"time": {"ts": 1677893021841880000, "dur": 876000, "relative_dur": 0.15863817457442955, "relative_gap_to_previous": 0.0005432814197754436, "parent_is_longest": true, "runtime_str": "876 us"}}}, "id": "TPFsDOMlzz2eofis", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.21.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 876000, "runtime_str": "876 us", "start_timestamp": "01:23:41.840.840758", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 15, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021840950000, "dur": 316000, "relative_dur": 0.0572256428830134, "relative_gap_to_previous": 0.0010865628395508873, "parent_is_longest": true, "runtime_str": "316 us"}}, "gpu3": {"time": {"ts": 1677893021842758000, "dur": 82000, "relative_dur": 0.014849692140528795, "relative_gap_to_previous": 0.0003621876131836291, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "PA9H6sBLBMOPSNaa", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.22.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 316000, "runtime_str": "316 us", "start_timestamp": "01:23:41.840.840950", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 16, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021841433000, "dur": 111000, "relative_dur": 0.020101412531691416, "relative_gap_to_previous": 0.026077508149221298, "parent_is_longest": true, "runtime_str": "111 us"}}, "gpu3": {"time": {"ts": 1677893021842841000, "dur": 781000, "relative_dur": 0.14143426294820718, "relative_gap_to_previous": 0.00018109380659181456, "parent_is_longest": true, "runtime_str": "781 us"}}}, "id": "ms25ofWaN1b5StO8", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.23.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 781000, "runtime_str": "781 us", "start_timestamp": "01:23:41.841.841433", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 17, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021841777000, "dur": 79000, "relative_dur": 0.01430641072075335, "relative_gap_to_previous": 0.03802969938428106, "parent_is_longest": true, "runtime_str": "79 us"}}, "gpu3": {"time": {"ts": 1677893021843623000, "dur": 320000, "relative_dur": 0.057950018109380656, "relative_gap_to_previous": 0.00018109380659181456, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "5tuU1nuPy0fMx1xR", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.24.pt.trace.json", "trace_disk_size": "902 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "01:23:41.841.841777", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 18, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021842023000, "dur": 74000, "relative_dur": 0.013400941687794278, "relative_gap_to_previous": 0.026077508149221298, "parent_is_longest": true, "runtime_str": "74 us"}}, "gpu3": {"time": {"ts": 1677893021843945000, "dur": 380000, "relative_dur": 0.06881564650488953, "relative_gap_to_previous": 0.0003621876131836291, "parent_is_longest": true, "runtime_str": "380 us"}}}, "id": "5vKM1ynJJji8HCSi", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.25.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 380000, "runtime_str": "380 us", "start_timestamp": "01:23:41.842.842023", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 19, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021842282000, "dur": 84000, "relative_dur": 0.015211879753712423, "relative_gap_to_previous": 0.02933719666787396, "parent_is_longest": true, "runtime_str": "84 us"}}, "gpu3": {"time": {"ts": 1677893021844326000, "dur": 402000, "relative_dur": 0.07279971024990946, "relative_gap_to_previous": 0.00018109380659181456, "parent_is_longest": true, "runtime_str": "402 us"}}}, "id": "XkZtw6h8gW6TKGWI", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.26.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 402000, "runtime_str": "402 us", "start_timestamp": "01:23:41.842.842282", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 20, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021842542000, "dur": 194000, "relative_dur": 0.03513219847881202, "relative_gap_to_previous": 0.027707352408547627, "parent_is_longest": true, "runtime_str": "194 us"}}, "gpu3": {"time": {"ts": 1677893021844729000, "dur": 864000, "relative_dur": 0.15646504889532778, "relative_gap_to_previous": 0.00018109380659181456, "parent_is_longest": true, "runtime_str": "864 us"}}}, "id": "W9hjH1CKDqaBfuE7", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.27.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 864000, "runtime_str": "864 us", "start_timestamp": "01:23:41.842.842542", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021839750000, "dur": 2963000, "relative_dur": 0.2210699097217041, "relative_gap_to_previous": 0.012161456390360367, "parent_is_longest": true, "runtime_str": "3 ms"}}, "gpu3": {"time": {"ts": 1677893021840071000, "dur": 5522000, "relative_dur": 0.41199731403417145, "relative_gap_to_previous": 0.030963217190181302, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "KYTSVicVk8DJXWXu", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.18.pt.trace.json", "trace_disk_size": "23.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5522000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.839.839750", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 191}, {"idx": 21, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu2": {"time": {"ts": 1677893021842744000, "dur": 313000, "relative_dur": 0.023352980675968065, "relative_gap_to_previous": 0.0005968812952324106, "parent_is_longest": true, "runtime_str": "313 us"}}, "gpu3": {"time": {"ts": 1677893021845595000, "dur": 97000, "relative_dur": 0.0072371857046929795, "relative_gap_to_previous": 0.00014922032380810266, "parent_is_longest": true, "runtime_str": "97 us"}}}, "id": "deMZprkA2Stkz5qV", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.28.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 313000, "runtime_str": "313 us", "start_timestamp": "01:23:41.842.842744", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 22, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu2": {"time": {"ts": 1677893021843077000, "dur": 304000, "relative_dur": 0.022681489218831605, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "304 us"}}, "gpu3": {"time": {"ts": 1677893021845693000, "dur": 231000, "relative_dur": 0.01723494739983586, "relative_gap_to_previous": 7.461016190405133e-05, "parent_is_longest": true, "runtime_str": "231 us"}}}, "id": "W6QnjW7NSQTvvqUV", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.29.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 304000, "runtime_str": "304 us", "start_timestamp": "01:23:41.843.843077", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 23, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"idx": 24, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu2": {"time": {"ts": 1677893021843542000, "dur": 308000, "relative_dur": 0.04381846635367762, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "308 us"}}, "gpu3": {"time": {"ts": 1677893021845925000, "dur": 3170000, "relative_dur": 0.4509887608479158, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "qRC4MsC85YHBLSRd", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.31.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3170000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.843.843542", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 25, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu2": {"time": {"ts": 1677893021843990000, "dur": 79000, "relative_dur": 0.011239152084222506, "relative_gap_to_previous": 0.0199174847062171, "parent_is_longest": true, "runtime_str": "79 us"}}, "gpu3": {"time": {"ts": 1677893021849097000, "dur": 320000, "relative_dur": 0.04552567932849623, "relative_gap_to_previous": 0.0002845354958031014, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "2RbaUfb6Gq1fOV0V", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.32.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "01:23:41.843.843990", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 26, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu2": {"time": {"ts": 1677893021844206000, "dur": 73000, "relative_dur": 0.010385545596813203, "relative_gap_to_previous": 0.019490681462512448, "parent_is_longest": true, "runtime_str": "73 us"}}, "gpu3": {"time": {"ts": 1677893021849418000, "dur": 377000, "relative_dur": 0.053634940958884623, "relative_gap_to_previous": 0.0001422677479015507, "parent_is_longest": true, "runtime_str": "377 us"}}}, "id": "C3p4xMF3d1NxT8KU", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.33.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 377000, "runtime_str": "377 us", "start_timestamp": "01:23:41.844.844206", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 27, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu2": {"time": {"ts": 1677893021844409000, "dur": 111000, "relative_dur": 0.01579172001707213, "relative_gap_to_previous": 0.018494807227201593, "parent_is_longest": true, "runtime_str": "111 us"}}, "gpu3": {"time": {"ts": 1677893021849796000, "dur": 3060000, "relative_dur": 0.4353393085787452, "relative_gap_to_previous": 0.0001422677479015507, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "72SaoI7cwKzYwpsB", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.34.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3060000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.844.844409", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 28, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu2": {"time": {"ts": 1677893021844655000, "dur": 71000, "relative_dur": 0.010101010101010102, "relative_gap_to_previous": 0.019206145966709345, "parent_is_longest": true, "runtime_str": "71 us"}}, "gpu3": {"time": {"ts": 1677893021852858000, "dur": 96000, "relative_dur": 0.01365770379854887, "relative_gap_to_previous": 0.0002845354958031014, "parent_is_longest": true, "runtime_str": "96 us"}}}, "id": "4gvSoUIZrDWb5NFK", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.35.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "01:23:41.844.844655", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1677893021843542000, "dur": 1184000, "relative_dur": 0.08833843169439677, "relative_gap_to_previous": 0.010296202342759084, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893021845925000, "dur": 7029000, "relative_dur": 0.5244348280235768, "relative_gap_to_previous": 7.461016190405133e-05, "parent_is_longest": true, "runtime_str": "7 ms"}}}, "id": "gXBInCgUXgZRazYK", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.30.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7029000, "runtime_str": "7 ms", "start_timestamp": "01:23:41.843.843542", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1677893021839058000, "dur": 5645000, "relative_dur": 0.06839936992608749, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6 ms"}}, "gpu3": {"time": {"ts": 1677893021839551000, "dur": 13403000, "relative_dur": 0.16240155095116926, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "13 ms"}}}, "id": "lUbDUfwRLvrcrG2S", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.16.pt.trace.json", "trace_disk_size": "37.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 13403000, "runtime_str": "13 ms", "start_timestamp": "01:23:41.839.839058", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 315}, {"idx": 29, "name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"idx": 30, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu2": {"time": {"ts": 1677893021844741000, "dur": 356000, "relative_dur": 0.02714863112941356, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "356 us"}}, "gpu3": {"time": {"ts": 1677893021852955000, "dur": 227000, "relative_dur": 0.01731106535499123, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "227 us"}}}, "id": "AcOAEFfhnfa3qTu3", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.37.pt.trace.json", "trace_disk_size": "2.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 356000, "runtime_str": "356 us", "start_timestamp": "01:23:41.844.844741", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 20}, {"idx": 31, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"idx": 32, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021845225000, "dur": 318000, "relative_dur": 0.058790904048807546, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "318 us"}}, "gpu3": {"time": {"ts": 1677893021853184000, "dur": 866000, "relative_dur": 0.16010353115178405, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "866 us"}}}, "id": "F0RNr6S8BCTPBNwZ", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.39.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 866000, "runtime_str": "866 us", "start_timestamp": "01:23:41.845.845225", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 35}, {"idx": 33, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021845727000, "dur": 168000, "relative_dur": 0.03105934553521908, "relative_gap_to_previous": 0.031244222591976335, "parent_is_longest": true, "runtime_str": "168 us"}}, "gpu3": {"time": {"ts": 1677893021854051000, "dur": 865000, "relative_dur": 0.1599186540950268, "relative_gap_to_previous": 0.00018487705675725643, "parent_is_longest": true, "runtime_str": "865 us"}}}, "id": "KDhqZMRFD0CmMbEV", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.40.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 865000, "runtime_str": "865 us", "start_timestamp": "01:23:41.845.845727", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 34, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021846086000, "dur": 187000, "relative_dur": 0.03457200961360695, "relative_gap_to_previous": 0.03253836198927713, "parent_is_longest": true, "runtime_str": "187 us"}}, "gpu3": {"time": {"ts": 1677893021854918000, "dur": 864000, "relative_dur": 0.15973377703826955, "relative_gap_to_previous": 0.00036975411351451286, "parent_is_longest": true, "runtime_str": "864 us"}}}, "id": "IfQSHXFaPdca6WJg", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.41.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 864000, "runtime_str": "864 us", "start_timestamp": "01:23:41.846.846086", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 35, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021846298000, "dur": 289000, "relative_dur": 0.05342946940284711, "relative_gap_to_previous": 0.0018487705675725643, "parent_is_longest": true, "runtime_str": "289 us"}}, "gpu3": {"time": {"ts": 1677893021855784000, "dur": 82000, "relative_dur": 0.015159918654095026, "relative_gap_to_previous": 0.00036975411351451286, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "NzI0JJGdjuHCqz5Q", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.42.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 289000, "runtime_str": "289 us", "start_timestamp": "01:23:41.846.846298", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 36, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021846737000, "dur": 84000, "relative_dur": 0.01552967276760954, "relative_gap_to_previous": 0.024958402662229616, "parent_is_longest": true, "runtime_str": "84 us"}}, "gpu3": {"time": {"ts": 1677893021855867000, "dur": 773000, "relative_dur": 0.14290996487335922, "relative_gap_to_previous": 0.00018487705675725643, "parent_is_longest": true, "runtime_str": "773 us"}}}, "id": "CT4bPxdqNQujNMet", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.43.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 773000, "runtime_str": "773 us", "start_timestamp": "01:23:41.846.846737", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 37, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021847006000, "dur": 57000, "relative_dur": 0.010537992235163616, "relative_gap_to_previous": 0.03142909964873359, "parent_is_longest": true, "runtime_str": "57 us"}}, "gpu3": {"time": {"ts": 1677893021856641000, "dur": 321000, "relative_dur": 0.059345535219079315, "relative_gap_to_previous": 0.00018487705675725643, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "fIJmoXAOE1wcJb6m", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.44.pt.trace.json", "trace_disk_size": "902 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "01:23:41.847.847006", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 38, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021847239000, "dur": 81000, "relative_dur": 0.014975041597337771, "relative_gap_to_previous": 0.029765206137918283, "parent_is_longest": true, "runtime_str": "81 us"}}, "gpu3": {"time": {"ts": 1677893021856963000, "dur": 377000, "relative_dur": 0.06969865039748567, "relative_gap_to_previous": 0.00018487705675725643, "parent_is_longest": true, "runtime_str": "377 us"}}}, "id": "7knpBCSerbpRs0Z0", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.45.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 377000, "runtime_str": "377 us", "start_timestamp": "01:23:41.847.847239", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 39, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021847504000, "dur": 82000, "relative_dur": 0.015159918654095026, "relative_gap_to_previous": 0.031244222591976335, "parent_is_longest": true, "runtime_str": "82 us"}}, "gpu3": {"time": {"ts": 1677893021857342000, "dur": 386000, "relative_dur": 0.07136254390830098, "relative_gap_to_previous": 0.00036975411351451286, "parent_is_longest": true, "runtime_str": "386 us"}}}, "id": "0VhG56dsN8PGSNHV", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.46.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 386000, "runtime_str": "386 us", "start_timestamp": "01:23:41.847.847504", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 40, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021847754000, "dur": 173000, "relative_dur": 0.03198373081900536, "relative_gap_to_previous": 0.028286189683860232, "parent_is_longest": true, "runtime_str": "173 us"}}, "gpu3": {"time": {"ts": 1677893021857730000, "dur": 863000, "relative_dur": 0.15954889998151228, "relative_gap_to_previous": 0.00036975411351451286, "parent_is_longest": true, "runtime_str": "863 us"}}}, "id": "W6GUUXjKuDjqXcgM", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.47.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 863000, "runtime_str": "863 us", "start_timestamp": "01:23:41.847.847754", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021845225000, "dur": 2687000, "relative_dur": 0.204911156867231, "relative_gap_to_previous": 0.008617402577594753, "parent_is_longest": true, "runtime_str": "3 ms"}}, "gpu3": {"time": {"ts": 1677893021853184000, "dur": 5409000, "relative_dur": 0.4124914207275223, "relative_gap_to_previous": 0.00015252039960344697, "parent_is_longest": true, "runtime_str": "5 ms"}}}, "id": "7h1ZTIwRNLnUkIul", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.38.pt.trace.json", "trace_disk_size": "23.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5409000, "runtime_str": "5 ms", "start_timestamp": "01:23:41.845.845225", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 191}, {"idx": 41, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu2": {"time": {"ts": 1677893021847936000, "dur": 302000, "relative_dur": 0.023030580340120493, "relative_gap_to_previous": 0.0006863417982155113, "parent_is_longest": true, "runtime_str": "302 us"}}, "gpu3": {"time": {"ts": 1677893021858594000, "dur": 96000, "relative_dur": 0.0073209791809654545, "relative_gap_to_previous": 7.626019980172348e-05, "parent_is_longest": true, "runtime_str": "96 us"}}}, "id": "KiqFev58DPDm2GLz", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.48.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 302000, "runtime_str": "302 us", "start_timestamp": "01:23:41.847.847936", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 42, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu2": {"time": {"ts": 1677893021848255000, "dur": 301000, "relative_dur": 0.022954320140318767, "relative_gap_to_previous": 0.00015252039960344697, "parent_is_longest": true, "runtime_str": "301 us"}}, "gpu3": {"time": {"ts": 1677893021858692000, "dur": 227000, "relative_dur": 0.01731106535499123, "relative_gap_to_previous": 0.00015252039960344697, "parent_is_longest": true, "runtime_str": "227 us"}}}, "id": "NzrmFhSAltOYn70s", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.49.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 301000, "runtime_str": "301 us", "start_timestamp": "01:23:41.848.848255", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 43, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"idx": 44, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu2": {"time": {"ts": 1677893021848696000, "dur": 230000, "relative_dur": 0.032176832680470065, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "230 us"}}, "gpu3": {"time": {"ts": 1677893021858920000, "dur": 3173000, "relative_dur": 0.4439003917179631, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "MncfzOoTxauEUiCp", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.51.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3173000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.848.848696", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 45, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu2": {"time": {"ts": 1677893021849060000, "dur": 55000, "relative_dur": 0.007694459988808058, "relative_gap_to_previous": 0.018746502518186905, "parent_is_longest": true, "runtime_str": "55 us"}}, "gpu3": {"time": {"ts": 1677893021862095000, "dur": 318000, "relative_dur": 0.04448796866256295, "relative_gap_to_previous": 0.0002797985450475658, "parent_is_longest": true, "runtime_str": "318 us"}}}, "id": "j06e92tdwpbC6uhc", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.52.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 318000, "runtime_str": "318 us", "start_timestamp": "01:23:41.849.849060", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 46, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu2": {"time": {"ts": 1677893021849244000, "dur": 76000, "relative_dur": 0.010632344711807499, "relative_gap_to_previous": 0.01804700615556799, "parent_is_longest": true, "runtime_str": "76 us"}}, "gpu3": {"time": {"ts": 1677893021862414000, "dur": 376000, "relative_dur": 0.05260212646894236, "relative_gap_to_previous": 0.0001398992725237829, "parent_is_longest": true, "runtime_str": "376 us"}}}, "id": "gYAf0OoBafe8aBsr", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.53.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 376000, "runtime_str": "376 us", "start_timestamp": "01:23:41.849.849244", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 47, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu2": {"time": {"ts": 1677893021849450000, "dur": 108000, "relative_dur": 0.01510912143256855, "relative_gap_to_previous": 0.018186905428091774, "parent_is_longest": true, "runtime_str": "108 us"}}, "gpu3": {"time": {"ts": 1677893021862792000, "dur": 3175000, "relative_dur": 0.44418019026301064, "relative_gap_to_previous": 0.0002797985450475658, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "yo6eMDYjMy34wYCP", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.54.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3175000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.849.849450", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 48, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu2": {"time": {"ts": 1677893021849692000, "dur": 71000, "relative_dur": 0.009932848349188584, "relative_gap_to_previous": 0.018746502518186905, "parent_is_longest": true, "runtime_str": "71 us"}}, "gpu3": {"time": {"ts": 1677893021865969000, "dur": 99000, "relative_dur": 0.013850027979854504, "relative_gap_to_previous": 0.0002797985450475658, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "6JKYgliTfx6hUd6n", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.55.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "01:23:41.849.849692", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1677893021848696000, "dur": 1067000, "relative_dur": 0.08136963318843896, "relative_gap_to_previous": 0.009532524975215435, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893021858920000, "dur": 7148000, "relative_dur": 0.5451079081827195, "relative_gap_to_previous": 7.626019980172348e-05, "parent_is_longest": true, "runtime_str": "7 ms"}}}, "id": "W5V8uNWnBfOHwp8W", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.50.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7148000, "runtime_str": "7 ms", "start_timestamp": "01:23:41.848.848696", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1677893021844741000, "dur": 5007000, "relative_dur": 0.060668847691748455, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "5 ms"}}, "gpu3": {"time": {"ts": 1677893021852955000, "dur": 13113000, "relative_dur": 0.1588876772082879, "relative_gap_to_previous": 1.2116806009935782e-05, "parent_is_longest": true, "runtime_str": "13 ms"}}}, "id": "5yakAfgC246ATQz4", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.36.pt.trace.json", "trace_disk_size": "37.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 13113000, "runtime_str": "13 ms", "start_timestamp": "01:23:41.844.844741", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 314}, {"idx": 49, "name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"idx": 50, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu2": {"time": {"ts": 1677893021849778000, "dur": 356000, "relative_dur": 0.026011983048370598, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "356 us"}}, "gpu3": {"time": {"ts": 1677893021866069000, "dur": 229000, "relative_dur": 0.016732427297968727, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "229 us"}}}, "id": "C3vfwmlHxWNevMD1", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.57.pt.trace.json", "trace_disk_size": "2.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 356000, "runtime_str": "356 us", "start_timestamp": "01:23:41.849.849778", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 20}, {"idx": 51, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"idx": 52, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021850263000, "dur": 317000, "relative_dur": 0.05529391243676958, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "317 us"}}, "gpu3": {"time": {"ts": 1677893021866300000, "dur": 930000, "relative_dur": 0.16221873364730507, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "930 us"}}}, "id": "CDy0RbBhwTSwcUk0", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.59.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 930000, "runtime_str": "930 us", "start_timestamp": "01:23:41.850.850263", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 35}, {"idx": 53, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021850729000, "dur": 128000, "relative_dur": 0.02232687946973661, "relative_gap_to_previous": 0.02372230943659515, "parent_is_longest": true, "runtime_str": "128 us"}}, "gpu3": {"time": {"ts": 1677893021867231000, "dur": 929000, "relative_dur": 0.16204430490144775, "relative_gap_to_previous": 0.00017442874585731728, "parent_is_longest": true, "runtime_str": "929 us"}}}, "id": "NNRWAK3qalz17L9G", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.60.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 929000, "runtime_str": "929 us", "start_timestamp": "01:23:41.850.850729", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 54, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021851004000, "dur": 128000, "relative_dur": 0.02232687946973661, "relative_gap_to_previous": 0.023373451944880518, "parent_is_longest": true, "runtime_str": "128 us"}}, "gpu3": {"time": {"ts": 1677893021868162000, "dur": 931000, "relative_dur": 0.1623931623931624, "relative_gap_to_previous": 0.00034885749171463456, "parent_is_longest": true, "runtime_str": "931 us"}}}, "id": "GLqh97sT6qTovXSj", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.61.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 931000, "runtime_str": "931 us", "start_timestamp": "01:23:41.851.851004", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 55, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021851150000, "dur": 234000, "relative_dur": 0.04081632653061224, "relative_gap_to_previous": 0.0008721437292865864, "parent_is_longest": true, "runtime_str": "234 us"}}, "gpu3": {"time": {"ts": 1677893021869094000, "dur": 82000, "relative_dur": 0.014303157160300017, "relative_gap_to_previous": 0.00017442874585731728, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "o1EPzqA9l5JJMr1U", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.62.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 234000, "runtime_str": "234 us", "start_timestamp": "01:23:41.851.851150", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 56, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021851524000, "dur": 75000, "relative_dur": 0.013082155939298797, "relative_gap_to_previous": 0.022152450723879294, "parent_is_longest": true, "runtime_str": "75 us"}}, "gpu3": {"time": {"ts": 1677893021869178000, "dur": 776000, "relative_dur": 0.13535670678527822, "relative_gap_to_previous": 0.00034885749171463456, "parent_is_longest": true, "runtime_str": "776 us"}}}, "id": "xhnw2GkrusQCxhPJ", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.63.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 776000, "runtime_str": "776 us", "start_timestamp": "01:23:41.851.851524", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 57, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021851756000, "dur": 51000, "relative_dur": 0.008895866038723181, "relative_gap_to_previous": 0.02511773940345369, "parent_is_longest": true, "runtime_str": "51 us"}}, "gpu3": {"time": {"ts": 1677893021869956000, "dur": 321000, "relative_dur": 0.05599162742019885, "relative_gap_to_previous": 0.00034885749171463456, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "tSNfylshZtDcnFZd", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.64.pt.trace.json", "trace_disk_size": "902 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "01:23:41.851.851756", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 58, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021851947000, "dur": 64000, "relative_dur": 0.011163439734868306, "relative_gap_to_previous": 0.022152450723879294, "parent_is_longest": true, "runtime_str": "64 us"}}, "gpu3": {"time": {"ts": 1677893021870278000, "dur": 383000, "relative_dur": 0.06680620966335252, "relative_gap_to_previous": 0.00017442874585731728, "parent_is_longest": true, "runtime_str": "383 us"}}}, "id": "fw9PQUFidXIiPL4u", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.65.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 383000, "runtime_str": "383 us", "start_timestamp": "01:23:41.851.851947", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 59, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021852151000, "dur": 60000, "relative_dur": 0.010465724751439037, "relative_gap_to_previous": 0.022152450723879294, "parent_is_longest": true, "runtime_str": "60 us"}}, "gpu3": {"time": {"ts": 1677893021870662000, "dur": 440000, "relative_dur": 0.0767486481772196, "relative_gap_to_previous": 0.00017442874585731728, "parent_is_longest": true, "runtime_str": "440 us"}}}, "id": "LQ3jMN60njSWJVx1", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.66.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 440000, "runtime_str": "440 us", "start_timestamp": "01:23:41.852.852151", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 60, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021852350000, "dur": 141000, "relative_dur": 0.024594453165881738, "relative_gap_to_previous": 0.02197802197802198, "parent_is_longest": true, "runtime_str": "141 us"}}, "gpu3": {"time": {"ts": 1677893021871103000, "dur": 930000, "relative_dur": 0.16221873364730507, "relative_gap_to_previous": 0.00017442874585731728, "parent_is_longest": true, "runtime_str": "930 us"}}}, "id": "w77K15MN5V3zeihq", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.67.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 930000, "runtime_str": "930 us", "start_timestamp": "01:23:41.852.852350", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021850263000, "dur": 2215000, "relative_dur": 0.16184422037118223, "relative_gap_to_previous": 0.008475814701154464, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893021866300000, "dur": 5733000, "relative_dur": 0.4188952213941254, "relative_gap_to_previous": 0.00014613473622680112, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "UpNaTUXcrIRxOzOD", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.58.pt.trace.json", "trace_disk_size": "23.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5733000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.850.850263", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 191}, {"idx": 61, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu2": {"time": {"ts": 1677893021852496000, "dur": 247000, "relative_dur": 0.01804763992400994, "relative_gap_to_previous": 0.0003653368405670028, "parent_is_longest": true, "runtime_str": "247 us"}}, "gpu3": {"time": {"ts": 1677893021872034000, "dur": 99000, "relative_dur": 0.007233669443226655, "relative_gap_to_previous": 7.306736811340056e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "53pO1aIwTLcGmn4g", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.68.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 247000, "runtime_str": "247 us", "start_timestamp": "01:23:41.852.852496", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 62, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu2": {"time": {"ts": 1677893021852756000, "dur": 229000, "relative_dur": 0.016732427297968727, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "229 us"}}, "gpu3": {"time": {"ts": 1677893021872135000, "dur": 227000, "relative_dur": 0.016586292561741926, "relative_gap_to_previous": 0.00014613473622680112, "parent_is_longest": true, "runtime_str": "227 us"}}}, "id": "fvr7onHpIT3byUYo", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.69.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 229000, "runtime_str": "229 us", "start_timestamp": "01:23:41.852.852756", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 63, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"idx": 64, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu2": {"time": {"ts": 1677893021853117000, "dur": 223000, "relative_dur": 0.030171830604789607, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "223 us"}}, "gpu3": {"time": {"ts": 1677893021872364000, "dur": 3283000, "relative_dur": 0.44418887836557974, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "M6PqGiuB9B1iUPyU", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.71.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3283000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.853.853117", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 65, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu2": {"time": {"ts": 1677893021853473000, "dur": 54000, "relative_dur": 0.007306183195778649, "relative_gap_to_previous": 0.017994858611825194, "parent_is_longest": true, "runtime_str": "54 us"}}, "gpu3": {"time": {"ts": 1677893021875648000, "dur": 319000, "relative_dur": 0.043160600730618316, "relative_gap_to_previous": 0.00013529968881071573, "parent_is_longest": true, "runtime_str": "319 us"}}}, "id": "7ImPy9oZyegArE9h", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.72.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 319000, "runtime_str": "319 us", "start_timestamp": "01:23:41.853.853473", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 66, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu2": {"time": {"ts": 1677893021853657000, "dur": 68000, "relative_dur": 0.00920037883912867, "relative_gap_to_previous": 0.017588959545393044, "parent_is_longest": true, "runtime_str": "68 us"}}, "gpu3": {"time": {"ts": 1677893021875968000, "dur": 387000, "relative_dur": 0.05236097956974699, "relative_gap_to_previous": 0.00013529968881071573, "parent_is_longest": true, "runtime_str": "387 us"}}}, "id": "RzoYW6GWnGZ3xW6W", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.73.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 387000, "runtime_str": "387 us", "start_timestamp": "01:23:41.853.853657", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 67, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu2": {"time": {"ts": 1677893021853854000, "dur": 108000, "relative_dur": 0.014612366391557299, "relative_gap_to_previous": 0.01745365985658233, "parent_is_longest": true, "runtime_str": "108 us"}}, "gpu3": {"time": {"ts": 1677893021876357000, "dur": 3297000, "relative_dur": 0.44608307400892977, "relative_gap_to_previous": 0.00027059937762143147, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "ZsrRwsaVyDt11JAO", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.74.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3297000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.853.853854", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 68, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu2": {"time": {"ts": 1677893021854097000, "dur": 70000, "relative_dur": 0.009470978216750102, "relative_gap_to_previous": 0.018265457989446624, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1677893021879655000, "dur": 100000, "relative_dur": 0.013529968881071573, "relative_gap_to_previous": 0.00013529968881071573, "parent_is_longest": true, "runtime_str": "100 us"}}}, "id": "naBy4hdLMc6FsKnr", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.75.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 100000, "runtime_str": "100 us", "start_timestamp": "01:23:41.854.854097", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1677893021853117000, "dur": 1050000, "relative_dur": 0.07672073651907059, "relative_gap_to_previous": 0.008695016805494666, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893021872364000, "dur": 7391000, "relative_dur": 0.5400409177261435, "relative_gap_to_previous": 0.00014613473622680112, "parent_is_longest": true, "runtime_str": "7 ms"}}}, "id": "iKxInb1YX1vEUOs3", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.70.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7391000, "runtime_str": "7 ms", "start_timestamp": "01:23:41.853.853117", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1677893021849778000, "dur": 4376000, "relative_dur": 0.053023143099478975, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "4 ms"}}, "gpu3": {"time": {"ts": 1677893021866069000, "dur": 13686000, "relative_dur": 0.1658306070519811, "relative_gap_to_previous": 1.2116806009935782e-05, "parent_is_longest": true, "runtime_str": "14 ms"}}}, "id": "m98Pl0dyCSQo21Z8", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.56.pt.trace.json", "trace_disk_size": "37.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 13686000, "runtime_str": "14 ms", "start_timestamp": "01:23:41.849.849778", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 314}, {"idx": 69, "name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"idx": 70, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu2": {"time": {"ts": 1677893021854183000, "dur": 353000, "relative_dur": 0.02507992895204263, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "353 us"}}, "gpu3": {"time": {"ts": 1677893021879757000, "dur": 224000, "relative_dur": 0.01591474245115453, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "224 us"}}}, "id": "8Pwcv8K7mVELa9wp", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.77.pt.trace.json", "trace_disk_size": "2.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 353000, "runtime_str": "353 us", "start_timestamp": "01:23:41.854.854183", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 20}, {"idx": 71, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"idx": 72, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021854666000, "dur": 310000, "relative_dur": 0.05344827586206897, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "310 us"}}, "gpu3": {"time": {"ts": 1677893021879983000, "dur": 929000, "relative_dur": 0.16017241379310346, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "929 us"}}}, "id": "CY173oHW5oGdOX1H", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.79.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 929000, "runtime_str": "929 us", "start_timestamp": "01:23:41.854.854666", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 35}, {"idx": 73, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021855125000, "dur": 130000, "relative_dur": 0.022413793103448276, "relative_gap_to_previous": 0.023448275862068966, "parent_is_longest": true, "runtime_str": "130 us"}}, "gpu3": {"time": {"ts": 1677893021880913000, "dur": 931000, "relative_dur": 0.16051724137931034, "relative_gap_to_previous": 0.00017241379310344826, "parent_is_longest": true, "runtime_str": "931 us"}}}, "id": "QYfR5QMLzzv6PlTc", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.80.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 931000, "runtime_str": "931 us", "start_timestamp": "01:23:41.855.855125", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 74, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021855402000, "dur": 130000, "relative_dur": 0.022413793103448276, "relative_gap_to_previous": 0.023103448275862068, "parent_is_longest": true, "runtime_str": "130 us"}}, "gpu3": {"time": {"ts": 1677893021881846000, "dur": 930000, "relative_dur": 0.16034482758620688, "relative_gap_to_previous": 0.0003448275862068965, "parent_is_longest": true, "runtime_str": "930 us"}}}, "id": "CFCt9IUV3N1ONkWW", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.81.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 930000, "runtime_str": "930 us", "start_timestamp": "01:23:41.855.855402", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 75, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021855550000, "dur": 230000, "relative_dur": 0.039655172413793106, "relative_gap_to_previous": 0.0008620689655172414, "parent_is_longest": true, "runtime_str": "230 us"}}, "gpu3": {"time": {"ts": 1677893021882778000, "dur": 81000, "relative_dur": 0.01396551724137931, "relative_gap_to_previous": 0.0003448275862068965, "parent_is_longest": true, "runtime_str": "81 us"}}}, "id": "Nn7d6GKexcoNNCxz", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.82.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 230000, "runtime_str": "230 us", "start_timestamp": "01:23:41.855.855550", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 76, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021855921000, "dur": 84000, "relative_dur": 0.014482758620689656, "relative_gap_to_previous": 0.022068965517241378, "parent_is_longest": true, "runtime_str": "84 us"}}, "gpu3": {"time": {"ts": 1677893021882860000, "dur": 776000, "relative_dur": 0.13379310344827586, "relative_gap_to_previous": 0.00017241379310344826, "parent_is_longest": true, "runtime_str": "776 us"}}}, "id": "7ttnLz6v2mB1y3GG", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.83.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 776000, "runtime_str": "776 us", "start_timestamp": "01:23:41.855.855921", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 77, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021856166000, "dur": 50000, "relative_dur": 0.008620689655172414, "relative_gap_to_previous": 0.025517241379310347, "parent_is_longest": true, "runtime_str": "50 us"}}, "gpu3": {"time": {"ts": 1677893021883638000, "dur": 320000, "relative_dur": 0.05517241379310345, "relative_gap_to_previous": 0.0003448275862068965, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "MmuPuHgEzgmsDITk", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.84.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "01:23:41.856.856166", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 78, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021856357000, "dur": 64000, "relative_dur": 0.011034482758620689, "relative_gap_to_previous": 0.022068965517241378, "parent_is_longest": true, "runtime_str": "64 us"}}, "gpu3": {"time": {"ts": 1677893021883960000, "dur": 391000, "relative_dur": 0.06741379310344828, "relative_gap_to_previous": 0.0003448275862068965, "parent_is_longest": true, "runtime_str": "391 us"}}}, "id": "qDyDqACjhScZiTvU", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.85.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 391000, "runtime_str": "391 us", "start_timestamp": "01:23:41.856.856357", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 79, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021856562000, "dur": 58000, "relative_dur": 0.01, "relative_gap_to_previous": 0.022068965517241378, "parent_is_longest": true, "runtime_str": "58 us"}}, "gpu3": {"time": {"ts": 1677893021884352000, "dur": 459000, "relative_dur": 0.07913793103448276, "relative_gap_to_previous": 0.00017241379310344826, "parent_is_longest": true, "runtime_str": "459 us"}}}, "id": "cqCs6YzJF1rczR5O", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.86.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 459000, "runtime_str": "459 us", "start_timestamp": "01:23:41.856.856562", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 80, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021856757000, "dur": 141000, "relative_dur": 0.024310344827586206, "relative_gap_to_previous": 0.021379310344827585, "parent_is_longest": true, "runtime_str": "141 us"}}, "gpu3": {"time": {"ts": 1677893021884813000, "dur": 970000, "relative_dur": 0.16724137931034483, "relative_gap_to_previous": 0.0003448275862068965, "parent_is_longest": true, "runtime_str": "970 us"}}}, "id": "Iq7U6TgqBbgKolWH", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.87.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 970000, "runtime_str": "970 us", "start_timestamp": "01:23:41.856.856757", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021854666000, "dur": 2219000, "relative_dur": 0.15765541740674954, "relative_gap_to_previous": 0.008312611012433392, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893021879983000, "dur": 5800000, "relative_dur": 0.41207815275310833, "relative_gap_to_previous": 0.00014209591474245115, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "gHwfr6DqjtdZ3emm", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.78.pt.trace.json", "trace_disk_size": "23.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5800000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.854.854666", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 191}, {"idx": 81, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu2": {"time": {"ts": 1677893021856904000, "dur": 233000, "relative_dur": 0.01655417406749556, "relative_gap_to_previous": 0.0004262877442273535, "parent_is_longest": true, "runtime_str": "233 us"}}, "gpu3": {"time": {"ts": 1677893021885785000, "dur": 99000, "relative_dur": 0.007033747779751332, "relative_gap_to_previous": 0.00014209591474245115, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "h7AmnUIOCWp93UgW", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.88.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 233000, "runtime_str": "233 us", "start_timestamp": "01:23:41.856.856904", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 82, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu2": {"time": {"ts": 1677893021857150000, "dur": 224000, "relative_dur": 0.01591474245115453, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "224 us"}}, "gpu3": {"time": {"ts": 1677893021885885000, "dur": 231000, "relative_dur": 0.01641207815275311, "relative_gap_to_previous": 7.104795737122558e-05, "parent_is_longest": true, "runtime_str": "231 us"}}}, "id": "NwIHXln4UBWBN5W1", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.89.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 231000, "runtime_str": "231 us", "start_timestamp": "01:23:41.857.857150", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 83, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"idx": 84, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu2": {"time": {"ts": 1677893021857504000, "dur": 221000, "relative_dur": 0.02864920922997148, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "221 us"}}, "gpu3": {"time": {"ts": 1677893021886118000, "dur": 3443000, "relative_dur": 0.4463313456053928, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "5l7SyTalIymR3dI4", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.91.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3443000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.857.857504", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 85, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu2": {"time": {"ts": 1677893021857857000, "dur": 52000, "relative_dur": 0.006740990407052113, "relative_gap_to_previous": 0.01711174487943998, "parent_is_longest": true, "runtime_str": "52 us"}}, "gpu3": {"time": {"ts": 1677893021889562000, "dur": 321000, "relative_dur": 0.041612652320456314, "relative_gap_to_previous": 0.00012963443090484831, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "Z3NKwa7vMvksY0nn", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.92.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "01:23:41.857.857857", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 86, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu2": {"time": {"ts": 1677893021858038000, "dur": 66000, "relative_dur": 0.00855587243971999, "relative_gap_to_previous": 0.016722841586725434, "parent_is_longest": true, "runtime_str": "66 us"}}, "gpu3": {"time": {"ts": 1677893021889884000, "dur": 387000, "relative_dur": 0.0501685247601763, "relative_gap_to_previous": 0.00012963443090484831, "parent_is_longest": true, "runtime_str": "387 us"}}}, "id": "WM3LsUtek5nZzeWU", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.93.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 387000, "runtime_str": "387 us", "start_timestamp": "01:23:41.858.858038", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 87, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu2": {"time": {"ts": 1677893021858232000, "dur": 106000, "relative_dur": 0.013741249675913923, "relative_gap_to_previous": 0.016593207155820584, "parent_is_longest": true, "runtime_str": "106 us"}}, "gpu3": {"time": {"ts": 1677893021890274000, "dur": 3457000, "relative_dur": 0.44814622763806067, "relative_gap_to_previous": 0.000388903292714545, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "hvPiPswc8w0FtP7V", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.94.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3457000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.858.858232", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 88, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu2": {"time": {"ts": 1677893021858472000, "dur": 70000, "relative_dur": 0.009074410163339383, "relative_gap_to_previous": 0.017371013741249677, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1677893021893732000, "dur": 100000, "relative_dur": 0.012963443090484833, "relative_gap_to_previous": 0.00012963443090484831, "parent_is_longest": true, "runtime_str": "100 us"}}}, "id": "TwB3Wq545gvpY7N5", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.95.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 100000, "runtime_str": "100 us", "start_timestamp": "01:23:41.858.858472", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1677893021857504000, "dur": 1038000, "relative_dur": 0.07374777975133215, "relative_gap_to_previous": 0.008312611012433392, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893021886118000, "dur": 7714000, "relative_dur": 0.5480639431616341, "relative_gap_to_previous": 0.00014209591474245115, "parent_is_longest": true, "runtime_str": "8 ms"}}}, "id": "9FgyvhROX3PuC4z7", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.90.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7714000, "runtime_str": "8 ms", "start_timestamp": "01:23:41.857.857504", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1677893021854183000, "dur": 4346000, "relative_dur": 0.05265963891918091, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "4 ms"}}, "gpu3": {"time": {"ts": 1677893021879757000, "dur": 14075000, "relative_dur": 0.17054404458984612, "relative_gap_to_previous": 2.4233612019871563e-05, "parent_is_longest": true, "runtime_str": "14 ms"}}}, "id": "wnfGGnWlZVKQagP7", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.76.pt.trace.json", "trace_disk_size": "37.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 14075000, "runtime_str": "14 ms", "start_timestamp": "01:23:41.854.854183", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 314}, {"idx": 89, "name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"idx": 90, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu2": {"time": {"ts": 1677893021858557000, "dur": 353000, "relative_dur": 0.02492409800183577, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "353 us"}}, "gpu3": {"time": {"ts": 1677893021893833000, "dur": 225000, "relative_dur": 0.015886464732048296, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "225 us"}}}, "id": "RXDJpkNjVDW6bnGa", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.97.pt.trace.json", "trace_disk_size": "2.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 353000, "runtime_str": "353 us", "start_timestamp": "01:23:41.858.858557", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 20}, {"idx": 91, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"idx": 92, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021859041000, "dur": 318000, "relative_dur": 0.053716216216216216, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "318 us"}}, "gpu3": {"time": {"ts": 1677893021894060000, "dur": 970000, "relative_dur": 0.16385135135135134, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "970 us"}}}, "id": "4uOOWHdvWOJ0Pz7X", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.99.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 970000, "runtime_str": "970 us", "start_timestamp": "01:23:41.859.859041", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 35}, {"idx": 93, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021859511000, "dur": 131000, "relative_dur": 0.022128378378378378, "relative_gap_to_previous": 0.022804054054054054, "parent_is_longest": true, "runtime_str": "131 us"}}, "gpu3": {"time": {"ts": 1677893021895031000, "dur": 970000, "relative_dur": 0.16385135135135134, "relative_gap_to_previous": 0.00016891891891891893, "parent_is_longest": true, "runtime_str": "970 us"}}}, "id": "BMDUXsVQl44Tbwm0", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.100.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 970000, "runtime_str": "970 us", "start_timestamp": "01:23:41.859.859511", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 94, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021859789000, "dur": 130000, "relative_dur": 0.02195945945945946, "relative_gap_to_previous": 0.02195945945945946, "parent_is_longest": true, "runtime_str": "130 us"}}, "gpu3": {"time": {"ts": 1677893021896003000, "dur": 970000, "relative_dur": 0.16385135135135134, "relative_gap_to_previous": 0.00033783783783783786, "parent_is_longest": true, "runtime_str": "970 us"}}}, "id": "vrMpty50JEh5s3iw", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.101.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 970000, "runtime_str": "970 us", "start_timestamp": "01:23:41.859.859789", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 95, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021859938000, "dur": 240000, "relative_dur": 0.04054054054054054, "relative_gap_to_previous": 0.00033783783783783786, "parent_is_longest": true, "runtime_str": "240 us"}}, "gpu3": {"time": {"ts": 1677893021896975000, "dur": 82000, "relative_dur": 0.013851351351351352, "relative_gap_to_previous": 0.00033783783783783786, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "fkxG3eh2Bfplefgx", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.102.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 240000, "runtime_str": "240 us", "start_timestamp": "01:23:41.859.859938", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 96, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021860368000, "dur": 104000, "relative_dur": 0.01756756756756757, "relative_gap_to_previous": 0.029222972972972973, "parent_is_longest": true, "runtime_str": "104 us"}}, "gpu3": {"time": {"ts": 1677893021897058000, "dur": 776000, "relative_dur": 0.13108108108108107, "relative_gap_to_previous": 0.00016891891891891893, "parent_is_longest": true, "runtime_str": "776 us"}}}, "id": "RYPKSpc5uAA00rIi", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.103.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 776000, "runtime_str": "776 us", "start_timestamp": "01:23:41.860.860368", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 97, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021860700000, "dur": 73000, "relative_dur": 0.012331081081081082, "relative_gap_to_previous": 0.03564189189189189, "parent_is_longest": true, "runtime_str": "73 us"}}, "gpu3": {"time": {"ts": 1677893021897835000, "dur": 321000, "relative_dur": 0.054222972972972974, "relative_gap_to_previous": 0.00016891891891891893, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "qzfg4ke9RHAXjqlA", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.104.pt.trace.json", "trace_disk_size": "902 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "01:23:41.860.860700", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 98, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021860975000, "dur": 96000, "relative_dur": 0.016216216216216217, "relative_gap_to_previous": 0.03125, "parent_is_longest": true, "runtime_str": "96 us"}}, "gpu3": {"time": {"ts": 1677893021898158000, "dur": 389000, "relative_dur": 0.06570945945945945, "relative_gap_to_previous": 0.00033783783783783786, "parent_is_longest": true, "runtime_str": "389 us"}}}, "id": "xpTgDUXLnYO3T1GJ", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.105.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 389000, "runtime_str": "389 us", "start_timestamp": "01:23:41.860.860975", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 99, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021861231000, "dur": 66000, "relative_dur": 0.01114864864864865, "relative_gap_to_previous": 0.024155405405405405, "parent_is_longest": true, "runtime_str": "66 us"}}, "gpu3": {"time": {"ts": 1677893021898548000, "dur": 459000, "relative_dur": 0.07753378378378378, "relative_gap_to_previous": 0.00016891891891891893, "parent_is_longest": true, "runtime_str": "459 us"}}}, "id": "83l6aJYunO67xTI2", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.106.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 459000, "runtime_str": "459 us", "start_timestamp": "01:23:41.861.861231", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 100, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021861438000, "dur": 147000, "relative_dur": 0.02483108108108108, "relative_gap_to_previous": 0.020945945945945947, "parent_is_longest": true, "runtime_str": "147 us"}}, "gpu3": {"time": {"ts": 1677893021899010000, "dur": 970000, "relative_dur": 0.16385135135135134, "relative_gap_to_previous": 0.0005067567567567568, "parent_is_longest": true, "runtime_str": "970 us"}}}, "id": "tnI3JeC0wOfrXg8a", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.107.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 970000, "runtime_str": "970 us", "start_timestamp": "01:23:41.861.861438", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021859041000, "dur": 2527000, "relative_dur": 0.1784226505683824, "relative_gap_to_previous": 0.00804914213090447, "parent_is_longest": true, "runtime_str": "3 ms"}}, "gpu3": {"time": {"ts": 1677893021894060000, "dur": 5920000, "relative_dur": 0.41799053872767067, "relative_gap_to_previous": 0.0001412130198404293, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "SFGaFK4lnxiVUUmY", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.98.pt.trace.json", "trace_disk_size": "23.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5920000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.859.859041", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 191}, {"idx": 101, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu2": {"time": {"ts": 1677893021861590000, "dur": 232000, "relative_dur": 0.016380710301489798, "relative_gap_to_previous": 0.00035303254960107323, "parent_is_longest": true, "runtime_str": "232 us"}}, "gpu3": {"time": {"ts": 1677893021899982000, "dur": 98000, "relative_dur": 0.006919437972181035, "relative_gap_to_previous": 0.0001412130198404293, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "xOSAQyMnMqkb5mNq", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.108.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 232000, "runtime_str": "232 us", "start_timestamp": "01:23:41.861.861590", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 102, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu2": {"time": {"ts": 1677893021861835000, "dur": 225000, "relative_dur": 0.015886464732048296, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "225 us"}}, "gpu3": {"time": {"ts": 1677893021900082000, "dur": 230000, "relative_dur": 0.01623949728164937, "relative_gap_to_previous": 0.0001412130198404293, "parent_is_longest": true, "runtime_str": "230 us"}}}, "id": "OnwZm63mJ9wyJV8O", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.109.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 230000, "runtime_str": "230 us", "start_timestamp": "01:23:41.861.861835", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 103, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"idx": 104, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu2": {"time": {"ts": 1677893021862193000, "dur": 227000, "relative_dur": 0.029549596459255402, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "227 us"}}, "gpu3": {"time": {"ts": 1677893021900314000, "dur": 3443000, "relative_dur": 0.44819057537099716, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "FEocR9QnyYJCZudx", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.111.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3443000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.862.862193", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 105, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu2": {"time": {"ts": 1677893021862553000, "dur": 54000, "relative_dur": 0.007029419422025514, "relative_gap_to_previous": 0.017313199687581358, "parent_is_longest": true, "runtime_str": "54 us"}}, "gpu3": {"time": {"ts": 1677893021903759000, "dur": 320000, "relative_dur": 0.04165581879718823, "relative_gap_to_previous": 0.0002603488674824265, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "St9GPVdDezxngf1x", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.112.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "01:23:41.862.862553", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 106, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu2": {"time": {"ts": 1677893021862746000, "dur": 68000, "relative_dur": 0.008851861494402499, "relative_gap_to_previous": 0.018094246290028638, "parent_is_longest": true, "runtime_str": "68 us"}}, "gpu3": {"time": {"ts": 1677893021904080000, "dur": 388000, "relative_dur": 0.05050768029159073, "relative_gap_to_previous": 0.00013017443374121324, "parent_is_longest": true, "runtime_str": "388 us"}}}, "id": "gnHRhAV3bebpxwzc", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.113.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 388000, "runtime_str": "388 us", "start_timestamp": "01:23:41.862.862746", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 107, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu2": {"time": {"ts": 1677893021862943000, "dur": 108000, "relative_dur": 0.014058838844051028, "relative_gap_to_previous": 0.016792501952616504, "parent_is_longest": true, "runtime_str": "108 us"}}, "gpu3": {"time": {"ts": 1677893021904470000, "dur": 3426000, "relative_dur": 0.4459776099973965, "relative_gap_to_previous": 0.0002603488674824265, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "bW6pustqLfGFfD4s", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.114.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3426000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.862.862943", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 108, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu2": {"time": {"ts": 1677893021863185000, "dur": 69000, "relative_dur": 0.008982035928143712, "relative_gap_to_previous": 0.01744337412132257, "parent_is_longest": true, "runtime_str": "69 us"}}, "gpu3": {"time": {"ts": 1677893021907897000, "dur": 99000, "relative_dur": 0.01288726894038011, "relative_gap_to_previous": 0.00013017443374121324, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "SKMwXzh9q0CuCVH0", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.115.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "01:23:41.863.863185", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1677893021862193000, "dur": 1061000, "relative_dur": 0.07491350702534774, "relative_gap_to_previous": 0.008190355150744899, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893021900314000, "dur": 7682000, "relative_dur": 0.5423992092070888, "relative_gap_to_previous": 0.0001412130198404293, "parent_is_longest": true, "runtime_str": "8 ms"}}}, "id": "2QCpEahxk3ICMWmQ", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.110.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7682000, "runtime_str": "8 ms", "start_timestamp": "01:23:41.862.862193", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1677893021858557000, "dur": 4680000, "relative_dur": 0.05670665212649945, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "5 ms"}}, "gpu3": {"time": {"ts": 1677893021893833000, "dur": 14163000, "relative_dur": 0.17161032351872046, "relative_gap_to_previous": 1.2116806009935782e-05, "parent_is_longest": true, "runtime_str": "14 ms"}}}, "id": "MaunSrmIyLJFSiqX", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.96.pt.trace.json", "trace_disk_size": "37.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 14163000, "runtime_str": "14 ms", "start_timestamp": "01:23:41.858.858557", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 314}, {"idx": 109, "name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"idx": 110, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu2": {"time": {"ts": 1677893021863269000, "dur": 359000, "relative_dur": 0.02549172761485479, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "359 us"}}, "gpu3": {"time": {"ts": 1677893021907998000, "dur": 226000, "relative_dur": 0.016047717105730313, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "226 us"}}}, "id": "7AR4XccxiW6I1hTN", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.117.pt.trace.json", "trace_disk_size": "2.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "01:23:41.863.863269", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 20}, {"idx": 111, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"idx": 112, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021863759000, "dur": 314000, "relative_dur": 0.05342862004424026, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "314 us"}}, "gpu3": {"time": {"ts": 1677893021908225000, "dur": 961000, "relative_dur": 0.16351880210992004, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "961 us"}}}, "id": "wfeZqDLwcSpL2lRx", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.119.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 961000, "runtime_str": "961 us", "start_timestamp": "01:23:41.863.863759", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 35}, {"idx": 113, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021864222000, "dur": 127000, "relative_dur": 0.021609664794963415, "relative_gap_to_previous": 0.022970903522205207, "parent_is_longest": true, "runtime_str": "127 us"}}, "gpu3": {"time": {"ts": 1677893021909188000, "dur": 961000, "relative_dur": 0.16351880210992004, "relative_gap_to_previous": 0.0003403096818104475, "parent_is_longest": true, "runtime_str": "961 us"}}}, "id": "EEtAl1Xx2uiOeIoB", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.120.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 961000, "runtime_str": "961 us", "start_timestamp": "01:23:41.864.864222", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 114, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021864496000, "dur": 127000, "relative_dur": 0.021609664794963415, "relative_gap_to_previous": 0.022630593840394758, "parent_is_longest": true, "runtime_str": "127 us"}}, "gpu3": {"time": {"ts": 1677893021910151000, "dur": 961000, "relative_dur": 0.16351880210992004, "relative_gap_to_previous": 0.0003403096818104475, "parent_is_longest": true, "runtime_str": "961 us"}}}, "id": "BWkKWvT7ctjeA6I5", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.121.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 961000, "runtime_str": "961 us", "start_timestamp": "01:23:41.864.864496", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 115, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021864641000, "dur": 233000, "relative_dur": 0.03964607793091714, "relative_gap_to_previous": 0.000680619363620895, "parent_is_longest": true, "runtime_str": "233 us"}}, "gpu3": {"time": {"ts": 1677893021911114000, "dur": 82000, "relative_dur": 0.013952696954228348, "relative_gap_to_previous": 0.0003403096818104475, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "cPkqF3cuxb3OUHQy", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.122.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 233000, "runtime_str": "233 us", "start_timestamp": "01:23:41.864.864641", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 116, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021865013000, "dur": 76000, "relative_dur": 0.012931767908797005, "relative_gap_to_previous": 0.02126935511315297, "parent_is_longest": true, "runtime_str": "76 us"}}, "gpu3": {"time": {"ts": 1677893021911197000, "dur": 774000, "relative_dur": 0.13169984686064318, "relative_gap_to_previous": 0.00017015484090522376, "parent_is_longest": true, "runtime_str": "774 us"}}}, "id": "0Zch8fliPqiH5PIW", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.123.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 774000, "runtime_str": "774 us", "start_timestamp": "01:23:41.865.865013", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 117, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021865250000, "dur": 53000, "relative_dur": 0.009018206567976859, "relative_gap_to_previous": 0.025012761613067893, "parent_is_longest": true, "runtime_str": "53 us"}}, "gpu3": {"time": {"ts": 1677893021911973000, "dur": 321000, "relative_dur": 0.05461970393057682, "relative_gap_to_previous": 0.0003403096818104475, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "go75GWvrOEECbflO", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.124.pt.trace.json", "trace_disk_size": "902 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "01:23:41.865.865250", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 118, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021865444000, "dur": 63000, "relative_dur": 0.010719754977029096, "relative_gap_to_previous": 0.021609664794963415, "parent_is_longest": true, "runtime_str": "63 us"}}, "gpu3": {"time": {"ts": 1677893021912295000, "dur": 386000, "relative_dur": 0.06567976858941636, "relative_gap_to_previous": 0.00017015484090522376, "parent_is_longest": true, "runtime_str": "386 us"}}}, "id": "gA1PYmYLVDZVFNL6", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.125.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 386000, "runtime_str": "386 us", "start_timestamp": "01:23:41.865.865444", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 119, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021865648000, "dur": 59000, "relative_dur": 0.010039135613408202, "relative_gap_to_previous": 0.021609664794963415, "parent_is_longest": true, "runtime_str": "59 us"}}, "gpu3": {"time": {"ts": 1677893021912682000, "dur": 455000, "relative_dur": 0.07742045261187681, "relative_gap_to_previous": 0.00017015484090522376, "parent_is_longest": true, "runtime_str": "455 us"}}}, "id": "kGYxtwbpl8fHLEFx", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.126.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 455000, "runtime_str": "455 us", "start_timestamp": "01:23:41.865.865648", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 120, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021865846000, "dur": 149000, "relative_dur": 0.02535307129487834, "relative_gap_to_previous": 0.02126935511315297, "parent_is_longest": true, "runtime_str": "149 us"}}, "gpu3": {"time": {"ts": 1677893021913140000, "dur": 962000, "relative_dur": 0.16368895695082525, "relative_gap_to_previous": 0.0005104645227156713, "parent_is_longest": true, "runtime_str": "962 us"}}}, "id": "68uPYsEgDI8WkIYe", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.127.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 962000, "runtime_str": "962 us", "start_timestamp": "01:23:41.865.865846", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021863759000, "dur": 2222000, "relative_dur": 0.15777888234041043, "relative_gap_to_previous": 0.00830788894411702, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893021908225000, "dur": 5877000, "relative_dur": 0.4173116523468011, "relative_gap_to_previous": 7.100759781296599e-05, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "kbwEn0rrHz4m3CEp", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.118.pt.trace.json", "trace_disk_size": "23.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5877000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.863.863759", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 191}, {"idx": 121, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu2": {"time": {"ts": 1677893021866001000, "dur": 235000, "relative_dur": 0.016686785486047007, "relative_gap_to_previous": 0.00042604558687779593, "parent_is_longest": true, "runtime_str": "235 us"}}, "gpu3": {"time": {"ts": 1677893021914103000, "dur": 99000, "relative_dur": 0.007029752183483633, "relative_gap_to_previous": 7.100759781296599e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "7fzN6XTv27dCBmXP", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.128.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 235000, "runtime_str": "235 us", "start_timestamp": "01:23:41.866.866001", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 122, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu2": {"time": {"ts": 1677893021866249000, "dur": 227000, "relative_dur": 0.01611872470354328, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "227 us"}}, "gpu3": {"time": {"ts": 1677893021914204000, "dur": 229000, "relative_dur": 0.01626073989916921, "relative_gap_to_previous": 0.00014201519562593199, "parent_is_longest": true, "runtime_str": "229 us"}}}, "id": "lOk6OmMIrUVu78UQ", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.129.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 229000, "runtime_str": "229 us", "start_timestamp": "01:23:41.866.866249", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 123, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"idx": 124, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu2": {"time": {"ts": 1677893021866612000, "dur": 220000, "relative_dur": 0.02876945207270825, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "220 us"}}, "gpu3": {"time": {"ts": 1677893021914434000, "dur": 3410000, "relative_dur": 0.4459265071269779, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "c386rydBeRSgw6ob", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.131.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3410000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.866.866612", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 125, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu2": {"time": {"ts": 1677893021866965000, "dur": 54000, "relative_dur": 0.007061592781482934, "relative_gap_to_previous": 0.017392441480319078, "parent_is_longest": true, "runtime_str": "54 us"}}, "gpu3": {"time": {"ts": 1677893021917846000, "dur": 320000, "relative_dur": 0.041846475742121095, "relative_gap_to_previous": 0.0002615404733882568, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "v4UDgklxU4NEynS3", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.132.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "01:23:41.866.866965", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 126, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu2": {"time": {"ts": 1677893021867148000, "dur": 67000, "relative_dur": 0.008761605858506603, "relative_gap_to_previous": 0.016869360533542564, "parent_is_longest": true, "runtime_str": "67 us"}}, "gpu3": {"time": {"ts": 1677893021918167000, "dur": 387000, "relative_dur": 0.0506080816006277, "relative_gap_to_previous": 0.0001307702366941284, "parent_is_longest": true, "runtime_str": "387 us"}}}, "id": "leE6ksmtq4W5tJ0A", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.133.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 387000, "runtime_str": "387 us", "start_timestamp": "01:23:41.867.867148", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 127, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu2": {"time": {"ts": 1677893021867344000, "dur": 106000, "relative_dur": 0.013861645089577612, "relative_gap_to_previous": 0.016869360533542564, "parent_is_longest": true, "runtime_str": "106 us"}}, "gpu3": {"time": {"ts": 1677893021918556000, "dur": 3425000, "relative_dur": 0.44788806067738984, "relative_gap_to_previous": 0.0002615404733882568, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "yZotOf8mYvhwfv49", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.134.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3425000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.867.867344", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 128, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu2": {"time": {"ts": 1677893021867583000, "dur": 70000, "relative_dur": 0.00915391656858899, "relative_gap_to_previous": 0.017392441480319078, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1677893021921982000, "dur": 99000, "relative_dur": 0.012946253432718713, "relative_gap_to_previous": 0.0001307702366941284, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "kz6ixlF4tYCxGhxk", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.135.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "01:23:41.867.867583", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1677893021866612000, "dur": 1041000, "relative_dur": 0.07391890932329759, "relative_gap_to_previous": 0.00866292693318185, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893021914434000, "dur": 7647000, "relative_dur": 0.542995100475751, "relative_gap_to_previous": 7.100759781296599e-05, "parent_is_longest": true, "runtime_str": "8 ms"}}}, "id": "ywXtenY192inceXs", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.130.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7647000, "runtime_str": "8 ms", "start_timestamp": "01:23:41.866.866612", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1677893021863269000, "dur": 4370000, "relative_dur": 0.05295044226341936, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "4 ms"}}, "gpu3": {"time": {"ts": 1677893021907998000, "dur": 14083000, "relative_dur": 0.1706409790379256, "relative_gap_to_previous": 2.4233612019871563e-05, "parent_is_longest": true, "runtime_str": "14 ms"}}}, "id": "n0Z1LN2UvCYJ2LxM", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.116.pt.trace.json", "trace_disk_size": "37.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 14083000, "runtime_str": "14 ms", "start_timestamp": "01:23:41.863.863269", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 314}], "resources": {"cpu2": {"time": {"ts": 1677893021839058000, "dur": 28500000, "relative_dur": 0.336760014179369, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "28 ms"}}, "gpu3": {"time": {"ts": 1677893021839551000, "dur": 82530000, "relative_dur": 0.9751861042183623, "relative_gap_to_previous": 0.0013234077750206782, "parent_is_longest": true, "runtime_str": "83 ms"}}}, "id": "LSLp7tYh6CNa5JYY", "pretty_name": "TransformerEncoder", "trace_file": "/results/Transformer/Transformer.15.pt.trace.json", "trace_disk_size": "226.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 82530000, "runtime_str": "83 ms", "start_timestamp": "01:23:41.839.839058", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 1885}], "resources": {"cpu2": {"time": {"ts": 1677893021836083000, "dur": 31434000, "relative_dur": 0.13268553772382294, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "31 ms"}}, "gpu3": {"time": {"ts": 1677893021837451000, "dur": 84630000, "relative_dur": 0.35723029387183103, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "85 ms"}}}, "id": "My82wmuoorJwinhy", "pretty_name": "Encoder", "trace_file": "/results/Transformer/Transformer.11.pt.trace.json", "trace_disk_size": "257.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 84630000, "runtime_str": "85 ms", "start_timestamp": "01:23:41.836.836083", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 2092}, {"idx": 129, "name": "Decoder", "type": "Decoder", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 212, in forward\n    with hotline.annotate('Decoder'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 212, "ops": [{"idx": 130, "name": "Embedding", "type": "Embedding", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 301, in forward\n    with hotline.annotate('Embedding'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 301, "resources": {"cpu2": {"time": {"ts": 1677893021867669000, "dur": 1807000, "relative_dur": 0.012064683260335433, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893021922082000, "dur": 2679000, "relative_dur": 0.01788671082149343, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "9ufpRdkXvQ9xd14s", "pretty_name": "Embedding", "trace_file": "/results/Transformer/Transformer.137.pt.trace.json", "trace_disk_size": "53.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 2679000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.867.867669", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 320}, {"idx": 131, "name": "PositionalEncoding", "type": "PositionalEncoding", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 310, in forward\n    with hotline.annotate('PositionalEncoding'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 310, "ops": [{"idx": 132, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 311, in forward\n    tgt = self.pos_encoder(tgt, targets_positions, decode=decode, cache=cache) # ret float32(32, 256)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 372, in forward\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 372, "resources": {"cpu2": {"time": {"ts": 1677893021924138000, "dur": 281000, "relative_dur": 0.9894366197183099, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "281 us"}}, "gpu3": {"time": {"ts": 1677893021924762000, "dur": 284000, "relative_dur": 1.0, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "284 us"}}}, "id": "yQUcNxoTf4YmOkYT", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.139.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 284000, "runtime_str": "284 us", "start_timestamp": "01:23:41.924.924138", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021924138000, "dur": 281000, "relative_dur": 0.0018761350283089413, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "281 us"}}, "gpu3": {"time": {"ts": 1677893021924762000, "dur": 284000, "relative_dur": 0.0018961649396432006, "relative_gap_to_previous": 6.67663711141972e-06, "parent_is_longest": true, "runtime_str": "284 us"}}}, "id": "CV5TYgqt1yp8j0gz", "pretty_name": "PositionalEncoding", "trace_file": "/results/Transformer/Transformer.138.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 284000, "runtime_str": "284 us", "start_timestamp": "01:23:41.924.924138", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}, {"idx": 133, "name": "TransformerDecoder", "type": "TransformerDecoder", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 312, in forward\n    with hotline.annotate('TransformerDecoder'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 312, "ops": [{"idx": 134, "name": "Layer1", "type": "Layer1", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"idx": 135, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"idx": 136, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021924541000, "dur": 638000, "relative_dur": 0.10647530040053405, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "638 us"}}, "gpu3": {"time": {"ts": 1677893021925047000, "dur": 1072000, "relative_dur": 0.17890520694259013, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1 ms"}}}, "id": "e1JvKpVatIvViAYW", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.143.pt.trace.json", "trace_disk_size": "6.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1072000, "runtime_str": "1 ms", "start_timestamp": "01:23:41.924.924541", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"idx": 137, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021925322000, "dur": 124000, "relative_dur": 0.02069425901201602, "relative_gap_to_previous": 0.021528704939919895, "parent_is_longest": true, "runtime_str": "124 us"}}, "gpu3": {"time": {"ts": 1677893021926120000, "dur": 962000, "relative_dur": 0.16054739652870495, "relative_gap_to_previous": 0.00016688918558077436, "parent_is_longest": true, "runtime_str": "962 us"}}}, "id": "SoxF19Fzu0HQkz4T", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.144.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 962000, "runtime_str": "962 us", "start_timestamp": "01:23:41.925.925322", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 138, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021925606000, "dur": 145000, "relative_dur": 0.024198931909212282, "relative_gap_to_previous": 0.024365821094793058, "parent_is_longest": true, "runtime_str": "145 us"}}, "gpu3": {"time": {"ts": 1677893021927083000, "dur": 963000, "relative_dur": 0.16071428571428573, "relative_gap_to_previous": 0.00016688918558077436, "parent_is_longest": true, "runtime_str": "963 us"}}}, "id": "Ae0RVSfHWyNfFLlK", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.145.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 963000, "runtime_str": "963 us", "start_timestamp": "01:23:41.925.925606", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 139, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021925769000, "dur": 237000, "relative_dur": 0.03955273698264353, "relative_gap_to_previous": 0.0006675567423230974, "parent_is_longest": true, "runtime_str": "237 us"}}, "gpu3": {"time": {"ts": 1677893021928047000, "dur": 82000, "relative_dur": 0.013684913217623497, "relative_gap_to_previous": 0.00016688918558077436, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "DsqWZtUhG1Mik9xD", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.146.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 237000, "runtime_str": "237 us", "start_timestamp": "01:23:41.925.925769", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 140, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021926141000, "dur": 75000, "relative_dur": 0.012516688918558077, "relative_gap_to_previous": 0.020193591455273698, "parent_is_longest": true, "runtime_str": "75 us"}}, "gpu3": {"time": {"ts": 1677893021928131000, "dur": 780000, "relative_dur": 0.130173564753004, "relative_gap_to_previous": 0.0003337783711615487, "parent_is_longest": true, "runtime_str": "780 us"}}}, "id": "I4WAz3n8JvQy8Xt7", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.147.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 780000, "runtime_str": "780 us", "start_timestamp": "01:23:41.926.926141", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 141, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021926369000, "dur": 49000, "relative_dur": 0.008177570093457943, "relative_gap_to_previous": 0.023197596795727637, "parent_is_longest": true, "runtime_str": "49 us"}}, "gpu3": {"time": {"ts": 1677893021928912000, "dur": 321000, "relative_dur": 0.05357142857142857, "relative_gap_to_previous": 0.00016688918558077436, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "cQoJpHkwXyctnVqy", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.148.pt.trace.json", "trace_disk_size": "902 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "01:23:41.926.926369", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 142, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021926557000, "dur": 75000, "relative_dur": 0.012516688918558077, "relative_gap_to_previous": 0.020861148197596796, "parent_is_longest": true, "runtime_str": "75 us"}}, "gpu3": {"time": {"ts": 1677893021929235000, "dur": 386000, "relative_dur": 0.0644192256341789, "relative_gap_to_previous": 0.0003337783711615487, "parent_is_longest": true, "runtime_str": "386 us"}}}, "id": "xJUNlDIRk3nxE9XG", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.149.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 386000, "runtime_str": "386 us", "start_timestamp": "01:23:41.926.926557", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 143, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021926780000, "dur": 58000, "relative_dur": 0.009679572763684913, "relative_gap_to_previous": 0.022363150867823766, "parent_is_longest": true, "runtime_str": "58 us"}}, "gpu3": {"time": {"ts": 1677893021929622000, "dur": 454000, "relative_dur": 0.07576769025367157, "relative_gap_to_previous": 0.00016688918558077436, "parent_is_longest": true, "runtime_str": "454 us"}}}, "id": "xXTzOLncl79InRW5", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.150.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 454000, "runtime_str": "454 us", "start_timestamp": "01:23:41.926.926780", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 144, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021926969000, "dur": 136000, "relative_dur": 0.022696929238985315, "relative_gap_to_previous": 0.019526034712950602, "parent_is_longest": true, "runtime_str": "136 us"}}, "gpu3": {"time": {"ts": 1677893021930077000, "dur": 962000, "relative_dur": 0.16054739652870495, "relative_gap_to_previous": 0.00016688918558077436, "parent_is_longest": true, "runtime_str": "962 us"}}}, "id": "SwhWMQfwgsFSMPzZ", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.151.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 962000, "runtime_str": "962 us", "start_timestamp": "01:23:41.926.926969", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021924541000, "dur": 2550000, "relative_dur": 0.1264128494943486, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}, "gpu3": {"time": {"ts": 1677893021925047000, "dur": 5992000, "relative_dur": 0.297045409478485, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "NMnCbwwsrHy8Mn1w", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.142.pt.trace.json", "trace_disk_size": "25.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5992000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.924.924541", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 208}, {"idx": 145, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu2": {"time": {"ts": 1677893021927110000, "dur": 221000, "relative_dur": 0.010955780289510213, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "221 us"}}, "gpu3": {"time": {"ts": 1677893021931040000, "dur": 98000, "relative_dur": 0.004858219313900456, "relative_gap_to_previous": 4.9573666468372e-05, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "JLuXCXG7dXHjKw9X", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.152.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 221000, "runtime_str": "221 us", "start_timestamp": "01:23:41.927.927110", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 146, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu2": {"time": {"ts": 1677893021927344000, "dur": 231000, "relative_dur": 0.011451516954193932, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "231 us"}}, "gpu3": {"time": {"ts": 1677893021931140000, "dur": 233000, "relative_dur": 0.011550664287130676, "relative_gap_to_previous": 9.9147332936744e-05, "parent_is_longest": true, "runtime_str": "233 us"}}}, "id": "uTVe0P60c8WohEAu", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.153.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 233000, "runtime_str": "233 us", "start_timestamp": "01:23:41.927.927344", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 147, "name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"idx": 148, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021927731000, "dur": 312000, "relative_dur": 0.05302515295717199, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "312 us"}}, "gpu3": {"time": {"ts": 1677893021931375000, "dur": 962000, "relative_dur": 0.16349422161794697, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "962 us"}}}, "id": "rhsPF7OmgKmBenPs", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.155.pt.trace.json", "trace_disk_size": "4.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 962000, "runtime_str": "962 us", "start_timestamp": "01:23:41.927.927731", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 37}, {"idx": 149, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021928184000, "dur": 124000, "relative_dur": 0.02107409925220938, "relative_gap_to_previous": 0.021753908905506457, "parent_is_longest": true, "runtime_str": "124 us"}}, "gpu3": {"time": {"ts": 1677893021932339000, "dur": 962000, "relative_dur": 0.16349422161794697, "relative_gap_to_previous": 0.0003399048266485384, "parent_is_longest": true, "runtime_str": "962 us"}}}, "id": "HZJwdHaZphPrCntd", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.156.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 962000, "runtime_str": "962 us", "start_timestamp": "01:23:41.928.928184", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 150, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021928450000, "dur": 124000, "relative_dur": 0.02107409925220938, "relative_gap_to_previous": 0.02192386131883073, "parent_is_longest": true, "runtime_str": "124 us"}}, "gpu3": {"time": {"ts": 1677893021933303000, "dur": 960000, "relative_dur": 0.16315431679129844, "relative_gap_to_previous": 0.0003399048266485384, "parent_is_longest": true, "runtime_str": "960 us"}}}, "id": "Cissa6qhLAWP5HM6", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.157.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 960000, "runtime_str": "960 us", "start_timestamp": "01:23:41.928.928450", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 151, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021928592000, "dur": 223000, "relative_dur": 0.03789938817131203, "relative_gap_to_previous": 0.000849762066621346, "parent_is_longest": true, "runtime_str": "223 us"}}, "gpu3": {"time": {"ts": 1677893021934265000, "dur": 83000, "relative_dur": 0.014106050305914344, "relative_gap_to_previous": 0.0003399048266485384, "parent_is_longest": true, "runtime_str": "83 us"}}}, "id": "FWOXaqPlXGhuWHID", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.158.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 223000, "runtime_str": "223 us", "start_timestamp": "01:23:41.928.928592", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 152, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021928949000, "dur": 73000, "relative_dur": 0.012406526172671652, "relative_gap_to_previous": 0.020564242012236573, "parent_is_longest": true, "runtime_str": "73 us"}}, "gpu3": {"time": {"ts": 1677893021934349000, "dur": 780000, "relative_dur": 0.13256288239293, "relative_gap_to_previous": 0.0001699524133242692, "parent_is_longest": true, "runtime_str": "780 us"}}}, "id": "e8KLBnUHn2FoGfQa", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.159.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 780000, "runtime_str": "780 us", "start_timestamp": "01:23:41.928.928949", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 153, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021929174000, "dur": 47000, "relative_dur": 0.007987763426240652, "relative_gap_to_previous": 0.02362338545207342, "parent_is_longest": true, "runtime_str": "47 us"}}, "gpu3": {"time": {"ts": 1677893021935131000, "dur": 320000, "relative_dur": 0.054384772263766146, "relative_gap_to_previous": 0.0003399048266485384, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "JBtOtVUnrICWotNm", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.160.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "01:23:41.929.929174", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 154, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021929359000, "dur": 63000, "relative_dur": 0.01070700203942896, "relative_gap_to_previous": 0.02124405166553365, "parent_is_longest": true, "runtime_str": "63 us"}}, "gpu3": {"time": {"ts": 1677893021935452000, "dur": 388000, "relative_dur": 0.06594153636981645, "relative_gap_to_previous": 0.0001699524133242692, "parent_is_longest": true, "runtime_str": "388 us"}}}, "id": "f6NgsWWSnm0wL2AV", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.161.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 388000, "runtime_str": "388 us", "start_timestamp": "01:23:41.929.929359", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 155, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021929568000, "dur": 68000, "relative_dur": 0.011556764106050306, "relative_gap_to_previous": 0.022603670972127805, "parent_is_longest": true, "runtime_str": "68 us"}}, "gpu3": {"time": {"ts": 1677893021935841000, "dur": 455000, "relative_dur": 0.07732834806254249, "relative_gap_to_previous": 0.0001699524133242692, "parent_is_longest": true, "runtime_str": "455 us"}}}, "id": "UYUbfQO8GhnKxlbp", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.162.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 455000, "runtime_str": "455 us", "start_timestamp": "01:23:41.929.929568", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 156, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021929780000, "dur": 136000, "relative_dur": 0.023113528212100613, "relative_gap_to_previous": 0.022263766145479265, "parent_is_longest": true, "runtime_str": "136 us"}}, "gpu3": {"time": {"ts": 1677893021936298000, "dur": 961000, "relative_dur": 0.1633242692046227, "relative_gap_to_previous": 0.0003399048266485384, "parent_is_longest": true, "runtime_str": "961 us"}}}, "id": "9dDeWMKvvtV4eoGz", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.163.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 961000, "runtime_str": "961 us", "start_timestamp": "01:23:41.929.929780", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021927731000, "dur": 2172000, "relative_dur": 0.10767400356930398, "relative_gap_to_previous": 0.006395002974419988, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893021931375000, "dur": 5884000, "relative_dur": 0.29169145349990083, "relative_gap_to_previous": 9.9147332936744e-05, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "lER2oybVGGLaQs40", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.154.pt.trace.json", "trace_disk_size": "23.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5884000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.927.927731", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 193}, {"idx": 157, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu2": {"time": {"ts": 1677893021929922000, "dur": 220000, "relative_dur": 0.01090620662304184, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "220 us"}}, "gpu3": {"time": {"ts": 1677893021937261000, "dur": 99000, "relative_dur": 0.004907792980368828, "relative_gap_to_previous": 9.9147332936744e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "6TorOSCM81DywLia", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.164.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 220000, "runtime_str": "220 us", "start_timestamp": "01:23:41.929.929922", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 158, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu2": {"time": {"ts": 1677893021930155000, "dur": 215000, "relative_dur": 0.01065833829069998, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "215 us"}}, "gpu3": {"time": {"ts": 1677893021937361000, "dur": 223000, "relative_dur": 0.011054927622446956, "relative_gap_to_previous": 4.9573666468372e-05, "parent_is_longest": true, "runtime_str": "223 us"}}}, "id": "6ni7GRdNGPS8Z5S6", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.165.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 223000, "runtime_str": "223 us", "start_timestamp": "01:23:41.930.930155", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 159, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"idx": 160, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu2": {"time": {"ts": 1677893021930495000, "dur": 213000, "relative_dur": 0.02790514869644963, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "213 us"}}, "gpu3": {"time": {"ts": 1677893021937586000, "dur": 3409000, "relative_dur": 0.44661338923097077, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "wDYcNUhSGO0Z54A2", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.167.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3409000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.930.930495", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 161, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu2": {"time": {"ts": 1677893021930836000, "dur": 51000, "relative_dur": 0.0066815144766146995, "relative_gap_to_previous": 0.01676929123542513, "parent_is_longest": true, "runtime_str": "51 us"}}, "gpu3": {"time": {"ts": 1677893021940996000, "dur": 319000, "relative_dur": 0.04179221800078606, "relative_gap_to_previous": 0.00013101008777675882, "parent_is_longest": true, "runtime_str": "319 us"}}}, "id": "K7NIBmg4GG54o2XJ", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.168.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 319000, "runtime_str": "319 us", "start_timestamp": "01:23:41.930.930836", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 162, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu2": {"time": {"ts": 1677893021931008000, "dur": 65000, "relative_dur": 0.008515655705489322, "relative_gap_to_previous": 0.015852220620987817, "parent_is_longest": true, "runtime_str": "65 us"}}, "gpu3": {"time": {"ts": 1677893021941317000, "dur": 388000, "relative_dur": 0.05083191405738242, "relative_gap_to_previous": 0.00026202017555351765, "parent_is_longest": true, "runtime_str": "388 us"}}}, "id": "DSFtm5T0w5rFNdsF", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.169.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 388000, "runtime_str": "388 us", "start_timestamp": "01:23:41.931.931008", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 163, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu2": {"time": {"ts": 1677893021931196000, "dur": 103000, "relative_dur": 0.013494039041006158, "relative_gap_to_previous": 0.016114240796541335, "parent_is_longest": true, "runtime_str": "103 us"}}, "gpu3": {"time": {"ts": 1677893021941706000, "dur": 3412000, "relative_dur": 0.4470064194943011, "relative_gap_to_previous": 0.00013101008777675882, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "EuiTcE17gST4gcvd", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.170.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3412000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.931.931196", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 164, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu2": {"time": {"ts": 1677893021931427000, "dur": 67000, "relative_dur": 0.00877767588104284, "relative_gap_to_previous": 0.01676929123542513, "parent_is_longest": true, "runtime_str": "67 us"}}, "gpu3": {"time": {"ts": 1677893021945120000, "dur": 99000, "relative_dur": 0.012969998689899122, "relative_gap_to_previous": 0.00026202017555351765, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "Cb1uvnLcUzeZBVcf", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.171.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "01:23:41.931.931427", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1677893021930495000, "dur": 999000, "relative_dur": 0.049524092801903626, "relative_gap_to_previous": 0.004858219313900456, "parent_is_longest": true, "runtime_str": "999 us"}}, "gpu3": {"time": {"ts": 1677893021937586000, "dur": 7633000, "relative_dur": 0.3783957961530835, "relative_gap_to_previous": 9.9147332936744e-05, "parent_is_longest": true, "runtime_str": "8 ms"}}}, "id": "tMlmHXbhaa6IQDyQ", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.166.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7633000, "runtime_str": "8 ms", "start_timestamp": "01:23:41.930.930495", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1677893021924541000, "dur": 6926000, "relative_dur": 0.057459535246438855, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "7 ms"}}, "gpu3": {"time": {"ts": 1677893021925047000, "dur": 20172000, "relative_dur": 0.16735110381044824, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "20 ms"}}}, "id": "DtT0W260UnsbNY4o", "pretty_name": "Layer1", "trace_file": "/results/Transformer/Transformer.141.pt.trace.json", "trace_disk_size": "64.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 20172000, "runtime_str": "20 ms", "start_timestamp": "01:23:41.924.924541", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 536}, {"idx": 165, "name": "Layer2", "type": "Layer2", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"idx": 166, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"idx": 167, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021931509000, "dur": 644000, "relative_dur": 0.10660486674391657, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "644 us"}}, "gpu3": {"time": {"ts": 1677893021945220000, "dur": 1178000, "relative_dur": 0.19500082767753682, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1 ms"}}}, "id": "ln33bcdlmuZ99pfe", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.174.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1178000, "runtime_str": "1 ms", "start_timestamp": "01:23:41.931.931509", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 51}, {"idx": 168, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021932296000, "dur": 126000, "relative_dur": 0.02085747392815759, "relative_gap_to_previous": 0.02168515146498924, "parent_is_longest": true, "runtime_str": "126 us"}}, "gpu3": {"time": {"ts": 1677893021946400000, "dur": 944000, "relative_dur": 0.1562655189538156, "relative_gap_to_previous": 0.00033107101473266014, "parent_is_longest": true, "runtime_str": "944 us"}}}, "id": "w7zCguop8Hm3Omue", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.175.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 944000, "runtime_str": "944 us", "start_timestamp": "01:23:41.932.932296", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 169, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021932573000, "dur": 156000, "relative_dur": 0.025823539149147492, "relative_gap_to_previous": 0.02300943552391988, "parent_is_longest": true, "runtime_str": "156 us"}}, "gpu3": {"time": {"ts": 1677893021947347000, "dur": 944000, "relative_dur": 0.1562655189538156, "relative_gap_to_previous": 0.0004966065220989903, "parent_is_longest": true, "runtime_str": "944 us"}}}, "id": "nDS3jUjHjx5brkrf", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.176.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 944000, "runtime_str": "944 us", "start_timestamp": "01:23:41.932.932573", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 170, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021932747000, "dur": 226000, "relative_dur": 0.0374110246647906, "relative_gap_to_previous": 0.0009932130441979805, "parent_is_longest": true, "runtime_str": "226 us"}}, "gpu3": {"time": {"ts": 1677893021948293000, "dur": 82000, "relative_dur": 0.013573911604039066, "relative_gap_to_previous": 0.00033107101473266014, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "uTQspSFkL8eB0wBS", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.177.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 226000, "runtime_str": "226 us", "start_timestamp": "01:23:41.932.932747", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 171, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021933108000, "dur": 75000, "relative_dur": 0.012415163052474755, "relative_gap_to_previous": 0.0203608674060586, "parent_is_longest": true, "runtime_str": "75 us"}}, "gpu3": {"time": {"ts": 1677893021948376000, "dur": 780000, "relative_dur": 0.12911769574573745, "relative_gap_to_previous": 0.00016553550736633007, "parent_is_longest": true, "runtime_str": "780 us"}}}, "id": "5V6YAufHbIbvIZVS", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.178.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 780000, "runtime_str": "780 us", "start_timestamp": "01:23:41.933.933108", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 172, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021933338000, "dur": 48000, "relative_dur": 0.007945704353583844, "relative_gap_to_previous": 0.0236715775533852, "parent_is_longest": true, "runtime_str": "48 us"}}, "gpu3": {"time": {"ts": 1677893021949157000, "dur": 320000, "relative_dur": 0.052971362357225625, "relative_gap_to_previous": 0.00016553550736633007, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "GB0zMQJW9Tpyq3Aa", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.179.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "01:23:41.933.933338", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 173, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021933523000, "dur": 76000, "relative_dur": 0.012580698559841086, "relative_gap_to_previous": 0.02069193842079126, "parent_is_longest": true, "runtime_str": "76 us"}}, "gpu3": {"time": {"ts": 1677893021949479000, "dur": 387000, "relative_dur": 0.06406224135076974, "relative_gap_to_previous": 0.00033107101473266014, "parent_is_longest": true, "runtime_str": "387 us"}}}, "id": "oQP6LMEKmjJdh8Kv", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.180.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 387000, "runtime_str": "387 us", "start_timestamp": "01:23:41.933.933523", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 174, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021933756000, "dur": 58000, "relative_dur": 0.009601059427247144, "relative_gap_to_previous": 0.02400264856811786, "parent_is_longest": true, "runtime_str": "58 us"}}, "gpu3": {"time": {"ts": 1677893021949867000, "dur": 447000, "relative_dur": 0.07399437179274955, "relative_gap_to_previous": 0.00016553550736633007, "parent_is_longest": true, "runtime_str": "447 us"}}}, "id": "QNezufEX0tgegoHN", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.181.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 447000, "runtime_str": "447 us", "start_timestamp": "01:23:41.933.933756", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 175, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021933947000, "dur": 137000, "relative_dur": 0.02267836450918722, "relative_gap_to_previous": 0.02002979639132594, "parent_is_longest": true, "runtime_str": "137 us"}}, "gpu3": {"time": {"ts": 1677893021950316000, "dur": 945000, "relative_dur": 0.1564310544611819, "relative_gap_to_previous": 0.00033107101473266014, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "5gEV2HtzHUvudLoK", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.182.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.933.933947", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021931509000, "dur": 2563000, "relative_dur": 0.12793251472496756, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}, "gpu3": {"time": {"ts": 1677893021945220000, "dur": 6041000, "relative_dur": 0.30153738644304684, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "JsJCRPahXupzlf4v", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.173.pt.trace.json", "trace_disk_size": "25.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 6041000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.931.931509", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 207}, {"idx": 176, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu2": {"time": {"ts": 1677893021934089000, "dur": 225000, "relative_dur": 0.011230907457322551, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "225 us"}}, "gpu3": {"time": {"ts": 1677893021951263000, "dur": 98000, "relative_dur": 0.004891684136967156, "relative_gap_to_previous": 9.98302885095338e-05, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "o5SjUfLHlyC8IlFC", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.183.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 225000, "runtime_str": "225 us", "start_timestamp": "01:23:41.934.934089", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 177, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu2": {"time": {"ts": 1677893021934327000, "dur": 218000, "relative_dur": 0.010881501447539183, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "218 us"}}, "gpu3": {"time": {"ts": 1677893021951362000, "dur": 233000, "relative_dur": 0.011630228611360687, "relative_gap_to_previous": 4.99151442547669e-05, "parent_is_longest": true, "runtime_str": "233 us"}}}, "id": "9RdxjHLmo6OqXpFL", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.184.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 233000, "runtime_str": "233 us", "start_timestamp": "01:23:41.934.934327", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 178, "name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"idx": 179, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021934670000, "dur": 309000, "relative_dur": 0.05325749741468459, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "309 us"}}, "gpu3": {"time": {"ts": 1677893021951597000, "dur": 947000, "relative_dur": 0.16321957945536023, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "947 us"}}}, "id": "RUWfWGf404nIWN5Z", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.186.pt.trace.json", "trace_disk_size": "4.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 947000, "runtime_str": "947 us", "start_timestamp": "01:23:41.934.934670", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 37}, {"idx": 180, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021935123000, "dur": 125000, "relative_dur": 0.021544295070665288, "relative_gap_to_previous": 0.021371940710099964, "parent_is_longest": true, "runtime_str": "125 us"}}, "gpu3": {"time": {"ts": 1677893021952545000, "dur": 945000, "relative_dur": 0.16287487073422957, "relative_gap_to_previous": 0.0001723543605653223, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "mrqQFGQCvOE3eBa3", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.187.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.935.935123", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 181, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021935389000, "dur": 125000, "relative_dur": 0.021544295070665288, "relative_gap_to_previous": 0.020854877628404, "parent_is_longest": true, "runtime_str": "125 us"}}, "gpu3": {"time": {"ts": 1677893021953491000, "dur": 946000, "relative_dur": 0.1630472250947949, "relative_gap_to_previous": 0.0001723543605653223, "parent_is_longest": true, "runtime_str": "946 us"}}}, "id": "n6t2GLn8NkdO2UtN", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.188.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 946000, "runtime_str": "946 us", "start_timestamp": "01:23:41.935.935389", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 182, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021935531000, "dur": 261000, "relative_dur": 0.04498448810754912, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "261 us"}}, "gpu3": {"time": {"ts": 1677893021954438000, "dur": 82000, "relative_dur": 0.014133057566356428, "relative_gap_to_previous": 0.0001723543605653223, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "XfD5uwvJcySsaVwr", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.189.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 261000, "runtime_str": "261 us", "start_timestamp": "01:23:41.935.935531", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 183, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021935927000, "dur": 79000, "relative_dur": 0.013615994484660462, "relative_gap_to_previous": 0.019820751465012065, "parent_is_longest": true, "runtime_str": "79 us"}}, "gpu3": {"time": {"ts": 1677893021954522000, "dur": 776000, "relative_dur": 0.1337469837986901, "relative_gap_to_previous": 0.0003447087211306446, "parent_is_longest": true, "runtime_str": "776 us"}}}, "id": "sJL13Yxkd3EcI6tg", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.190.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 776000, "runtime_str": "776 us", "start_timestamp": "01:23:41.935.935927", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 184, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021936166000, "dur": 48000, "relative_dur": 0.008273009307135471, "relative_gap_to_previous": 0.024129610479145122, "parent_is_longest": true, "runtime_str": "48 us"}}, "gpu3": {"time": {"ts": 1677893021955299000, "dur": 321000, "relative_dur": 0.05532574974146846, "relative_gap_to_previous": 0.0001723543605653223, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "bjI1rcvoi6LiVPnD", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.191.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "01:23:41.936.936166", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 185, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021936348000, "dur": 62000, "relative_dur": 0.010685970355049982, "relative_gap_to_previous": 0.01964839710444674, "parent_is_longest": true, "runtime_str": "62 us"}}, "gpu3": {"time": {"ts": 1677893021955621000, "dur": 384000, "relative_dur": 0.06618407445708377, "relative_gap_to_previous": 0.0001723543605653223, "parent_is_longest": true, "runtime_str": "384 us"}}}, "id": "I4gTkazYiNOfafHq", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.192.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 384000, "runtime_str": "384 us", "start_timestamp": "01:23:41.936.936348", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 186, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021936545000, "dur": 61000, "relative_dur": 0.01051361599448466, "relative_gap_to_previous": 0.019820751465012065, "parent_is_longest": true, "runtime_str": "61 us"}}, "gpu3": {"time": {"ts": 1677893021956007000, "dur": 446000, "relative_dur": 0.07687004481213375, "relative_gap_to_previous": 0.0003447087211306446, "parent_is_longest": true, "runtime_str": "446 us"}}}, "id": "baecdtuHnxcC448b", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.193.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 446000, "runtime_str": "446 us", "start_timestamp": "01:23:41.936.936545", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 187, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021936742000, "dur": 146000, "relative_dur": 0.025163736642537057, "relative_gap_to_previous": 0.01999310582557739, "parent_is_longest": true, "runtime_str": "146 us"}}, "gpu3": {"time": {"ts": 1677893021956454000, "dur": 945000, "relative_dur": 0.16287487073422957, "relative_gap_to_previous": 0.0001723543605653223, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "RefvgGMVwYno70nC", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.194.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.936.936742", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021934670000, "dur": 2198000, "relative_dur": 0.10971348707197764, "relative_gap_to_previous": 0.004642108415693322, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893021951597000, "dur": 5802000, "relative_dur": 0.28960766696615753, "relative_gap_to_previous": 9.98302885095338e-05, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "MaUMU16U3WCRt1CA", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.185.pt.trace.json", "trace_disk_size": "23.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5802000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.934.934670", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 193}, {"idx": 188, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu2": {"time": {"ts": 1677893021936893000, "dur": 223000, "relative_dur": 0.011131077168813017, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "223 us"}}, "gpu3": {"time": {"ts": 1677893021957401000, "dur": 99000, "relative_dur": 0.004941599281221923, "relative_gap_to_previous": 9.98302885095338e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "3Mt2XkumSerC6Q5o", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.195.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 223000, "runtime_str": "223 us", "start_timestamp": "01:23:41.936.936893", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 189, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu2": {"time": {"ts": 1677893021937128000, "dur": 219000, "relative_dur": 0.01093141659179395, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "219 us"}}, "gpu3": {"time": {"ts": 1677893021957501000, "dur": 234000, "relative_dur": 0.011680143755615454, "relative_gap_to_previous": 4.99151442547669e-05, "parent_is_longest": true, "runtime_str": "234 us"}}}, "id": "frO2LViNAkGxreJb", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.196.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 234000, "runtime_str": "234 us", "start_timestamp": "01:23:41.937.937128", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 190, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"idx": 191, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu2": {"time": {"ts": 1677893021937472000, "dur": 250000, "relative_dur": 0.03325352487363661, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "250 us"}}, "gpu3": {"time": {"ts": 1677893021957736000, "dur": 3345000, "relative_dur": 0.4449321628092578, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "36DiIfvqKv4Ed6KQ", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.198.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3345000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.937.937472", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 192, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu2": {"time": {"ts": 1677893021937850000, "dur": 50000, "relative_dur": 0.006650704974727321, "relative_gap_to_previous": 0.017025804735301943, "parent_is_longest": true, "runtime_str": "50 us"}}, "gpu3": {"time": {"ts": 1677893021961082000, "dur": 321000, "relative_dur": 0.0426975259377494, "relative_gap_to_previous": 0.00013301409949454643, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "dscS8e2heu7BEnbB", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.199.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "01:23:41.937.937850", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 193, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu2": {"time": {"ts": 1677893021938024000, "dur": 65000, "relative_dur": 0.008645916467145517, "relative_gap_to_previous": 0.016493748337323755, "parent_is_longest": true, "runtime_str": "65 us"}}, "gpu3": {"time": {"ts": 1677893021961405000, "dur": 389000, "relative_dur": 0.051742484703378555, "relative_gap_to_previous": 0.00026602819898909286, "parent_is_longest": true, "runtime_str": "389 us"}}}, "id": "tReS7z2puHjho0dF", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.200.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 389000, "runtime_str": "389 us", "start_timestamp": "01:23:41.938.938024", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 194, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu2": {"time": {"ts": 1677893021938211000, "dur": 112000, "relative_dur": 0.0148975791433892, "relative_gap_to_previous": 0.016227720138334664, "parent_is_longest": true, "runtime_str": "112 us"}}, "gpu3": {"time": {"ts": 1677893021961796000, "dur": 3359000, "relative_dur": 0.44679436020218144, "relative_gap_to_previous": 0.00026602819898909286, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "sC4jkgdzUA0wS2rN", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.201.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3359000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.938.938211", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 195, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu2": {"time": {"ts": 1677893021938450000, "dur": 69000, "relative_dur": 0.009177972865123703, "relative_gap_to_previous": 0.016892790635807394, "parent_is_longest": true, "runtime_str": "69 us"}}, "gpu3": {"time": {"ts": 1677893021965156000, "dur": 98000, "relative_dur": 0.01303538175046555, "relative_gap_to_previous": 0.00013301409949454643, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "U9Q6GAtbuOYcoC7z", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.202.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 98000, "runtime_str": "98 us", "start_timestamp": "01:23:41.938.938450", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1677893021937472000, "dur": 1047000, "relative_dur": 0.05226115603474094, "relative_gap_to_previous": 0.004642108415693322, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893021957736000, "dur": 7518000, "relative_dur": 0.3752620545073375, "relative_gap_to_previous": 4.99151442547669e-05, "parent_is_longest": true, "runtime_str": "8 ms"}}}, "id": "qgper165SbzltqQJ", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.197.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7518000, "runtime_str": "8 ms", "start_timestamp": "01:23:41.937.937472", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1677893021931509000, "dur": 6978000, "relative_dur": 0.05789093805221633, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "7 ms"}}, "gpu3": {"time": {"ts": 1677893021945220000, "dur": 20034000, "relative_dur": 0.16620622713357724, "relative_gap_to_previous": 8.29620780341306e-06, "parent_is_longest": true, "runtime_str": "20 ms"}}}, "id": "VoR8CfmCraKhdVJ4", "pretty_name": "Layer2", "trace_file": "/results/Transformer/Transformer.172.pt.trace.json", "trace_disk_size": "64.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 20034000, "runtime_str": "20 ms", "start_timestamp": "01:23:41.931.931509", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 535}, {"idx": 196, "name": "Layer3", "type": "Layer3", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"idx": 197, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"idx": 198, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021938534000, "dur": 639000, "relative_dur": 0.10577718920708493, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "639 us"}}, "gpu3": {"time": {"ts": 1677893021965256000, "dur": 1179000, "relative_dur": 0.19516636318490316, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1 ms"}}}, "id": "sQ6gLoGAs45KkeN9", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.205.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1179000, "runtime_str": "1 ms", "start_timestamp": "01:23:41.938.938534", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 51}, {"idx": 199, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021939329000, "dur": 127000, "relative_dur": 0.02102300943552392, "relative_gap_to_previous": 0.0236715775533852, "parent_is_longest": true, "runtime_str": "127 us"}}, "gpu3": {"time": {"ts": 1677893021966437000, "dur": 944000, "relative_dur": 0.1562655189538156, "relative_gap_to_previous": 0.00033107101473266014, "parent_is_longest": true, "runtime_str": "944 us"}}}, "id": "rhuZeHig7ivBpyxm", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.206.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 944000, "runtime_str": "944 us", "start_timestamp": "01:23:41.939.939329", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 200, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021939621000, "dur": 138000, "relative_dur": 0.02284390001655355, "relative_gap_to_previous": 0.025161397119682172, "parent_is_longest": true, "runtime_str": "138 us"}}, "gpu3": {"time": {"ts": 1677893021967383000, "dur": 945000, "relative_dur": 0.1564310544611819, "relative_gap_to_previous": 0.00033107101473266014, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "1Wi1veds07uiY8HZ", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.207.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.939.939621", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 201, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021939776000, "dur": 224000, "relative_dur": 0.03707995365005794, "relative_gap_to_previous": 0.0006621420294653203, "parent_is_longest": true, "runtime_str": "224 us"}}, "gpu3": {"time": {"ts": 1677893021968330000, "dur": 82000, "relative_dur": 0.013573911604039066, "relative_gap_to_previous": 0.00033107101473266014, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "VUVzOFKs2vCTHIbx", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.208.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 224000, "runtime_str": "224 us", "start_timestamp": "01:23:41.939.939776", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 202, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021940135000, "dur": 73000, "relative_dur": 0.012084092037742095, "relative_gap_to_previous": 0.02019533189869227, "parent_is_longest": true, "runtime_str": "73 us"}}, "gpu3": {"time": {"ts": 1677893021968413000, "dur": 778000, "relative_dur": 0.1287866247310048, "relative_gap_to_previous": 0.00016553550736633007, "parent_is_longest": true, "runtime_str": "778 us"}}}, "id": "EP7YvXeDYplazt0w", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.209.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 778000, "runtime_str": "778 us", "start_timestamp": "01:23:41.940.940135", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 203, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021940361000, "dur": 47000, "relative_dur": 0.007780168846217513, "relative_gap_to_previous": 0.023174971031286212, "parent_is_longest": true, "runtime_str": "47 us"}}, "gpu3": {"time": {"ts": 1677893021969193000, "dur": 320000, "relative_dur": 0.052971362357225625, "relative_gap_to_previous": 0.00033107101473266014, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "LIEZ3iEkPybiqQzO", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.210.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "01:23:41.940.940361", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 204, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021940544000, "dur": 62000, "relative_dur": 0.010263201456712464, "relative_gap_to_previous": 0.0203608674060586, "parent_is_longest": true, "runtime_str": "62 us"}}, "gpu3": {"time": {"ts": 1677893021969514000, "dur": 388000, "relative_dur": 0.06422777685813608, "relative_gap_to_previous": 0.00016553550736633007, "parent_is_longest": true, "runtime_str": "388 us"}}}, "id": "SeAI0cwBtrrQNgoj", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.211.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 388000, "runtime_str": "388 us", "start_timestamp": "01:23:41.940.940544", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 205, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021940739000, "dur": 60000, "relative_dur": 0.009932130441979804, "relative_gap_to_previous": 0.01986426088395961, "parent_is_longest": true, "runtime_str": "60 us"}}, "gpu3": {"time": {"ts": 1677893021969903000, "dur": 447000, "relative_dur": 0.07399437179274955, "relative_gap_to_previous": 0.00016553550736633007, "parent_is_longest": true, "runtime_str": "447 us"}}}, "id": "vXaKvvvc5PxG9tse", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.212.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 447000, "runtime_str": "447 us", "start_timestamp": "01:23:41.940.940739", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 206, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021940929000, "dur": 139000, "relative_dur": 0.02300943552391988, "relative_gap_to_previous": 0.01936765436186062, "parent_is_longest": true, "runtime_str": "139 us"}}, "gpu3": {"time": {"ts": 1677893021970352000, "dur": 945000, "relative_dur": 0.1564310544611819, "relative_gap_to_previous": 0.00033107101473266014, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "ugDy9WS4OPbRF9O6", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.213.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.940.940929", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021938534000, "dur": 2521000, "relative_dur": 0.12598700649675162, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}, "gpu3": {"time": {"ts": 1677893021965256000, "dur": 6041000, "relative_dur": 0.3018990504747626, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "8PuNBaTHJgdsLY8z", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.204.pt.trace.json", "trace_disk_size": "25.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 6041000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.938.938534", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 207}, {"idx": 207, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu2": {"time": {"ts": 1677893021941073000, "dur": 223000, "relative_dur": 0.011144427786106947, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "223 us"}}, "gpu3": {"time": {"ts": 1677893021971298000, "dur": 100000, "relative_dur": 0.004997501249375313, "relative_gap_to_previous": 4.9975012493753125e-05, "parent_is_longest": true, "runtime_str": "100 us"}}}, "id": "kCLAoaoroFRCsOFo", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.214.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 223000, "runtime_str": "223 us", "start_timestamp": "01:23:41.941.941073", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 208, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu2": {"time": {"ts": 1677893021941310000, "dur": 215000, "relative_dur": 0.01074462768615692, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "215 us"}}, "gpu3": {"time": {"ts": 1677893021971399000, "dur": 221000, "relative_dur": 0.01104447776111944, "relative_gap_to_previous": 4.9975012493753125e-05, "parent_is_longest": true, "runtime_str": "221 us"}}}, "id": "ME2dxDJWL8Yu0vZr", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.215.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 221000, "runtime_str": "221 us", "start_timestamp": "01:23:41.941.941310", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 209, "name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"idx": 210, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021941648000, "dur": 307000, "relative_dur": 0.05293103448275862, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "307 us"}}, "gpu3": {"time": {"ts": 1677893021971622000, "dur": 945000, "relative_dur": 0.1629310344827586, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "7D4MGsUBo4dkyZo7", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.217.pt.trace.json", "trace_disk_size": "4.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.941.941648", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 37}, {"idx": 211, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021942097000, "dur": 164000, "relative_dur": 0.028275862068965516, "relative_gap_to_previous": 0.02224137931034483, "parent_is_longest": true, "runtime_str": "164 us"}}, "gpu3": {"time": {"ts": 1677893021972568000, "dur": 945000, "relative_dur": 0.1629310344827586, "relative_gap_to_previous": 0.00017241379310344826, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "JvgP8KTpfH72TFk3", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.218.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.942.942097", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 212, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021942413000, "dur": 135000, "relative_dur": 0.02327586206896552, "relative_gap_to_previous": 0.02396551724137931, "parent_is_longest": true, "runtime_str": "135 us"}}, "gpu3": {"time": {"ts": 1677893021973514000, "dur": 946000, "relative_dur": 0.16310344827586207, "relative_gap_to_previous": 0.00017241379310344826, "parent_is_longest": true, "runtime_str": "946 us"}}}, "id": "2JkhaImysedKfzso", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.219.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 946000, "runtime_str": "946 us", "start_timestamp": "01:23:41.942.942413", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 213, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021942568000, "dur": 234000, "relative_dur": 0.040344827586206895, "relative_gap_to_previous": 0.001206896551724138, "parent_is_longest": true, "runtime_str": "234 us"}}, "gpu3": {"time": {"ts": 1677893021974461000, "dur": 82000, "relative_dur": 0.014137931034482758, "relative_gap_to_previous": 0.00017241379310344826, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "6TOyyws0TTQYfbnE", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.220.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 234000, "runtime_str": "234 us", "start_timestamp": "01:23:41.942.942568", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 214, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021942936000, "dur": 75000, "relative_dur": 0.01293103448275862, "relative_gap_to_previous": 0.02086206896551724, "parent_is_longest": true, "runtime_str": "75 us"}}, "gpu3": {"time": {"ts": 1677893021974545000, "dur": 774000, "relative_dur": 0.13344827586206898, "relative_gap_to_previous": 0.0003448275862068965, "parent_is_longest": true, "runtime_str": "774 us"}}}, "id": "xhkIYRpqg4YBNiIZ", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.221.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 774000, "runtime_str": "774 us", "start_timestamp": "01:23:41.942.942936", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 215, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021943164000, "dur": 47000, "relative_dur": 0.008103448275862069, "relative_gap_to_previous": 0.02413793103448276, "parent_is_longest": true, "runtime_str": "47 us"}}, "gpu3": {"time": {"ts": 1677893021975320000, "dur": 322000, "relative_dur": 0.05551724137931034, "relative_gap_to_previous": 0.00017241379310344826, "parent_is_longest": true, "runtime_str": "322 us"}}}, "id": "UuSv1BodpsM6JMdp", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.222.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 322000, "runtime_str": "322 us", "start_timestamp": "01:23:41.943.943164", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 216, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021943344000, "dur": 62000, "relative_dur": 0.010689655172413793, "relative_gap_to_previous": 0.020689655172413793, "parent_is_longest": true, "runtime_str": "62 us"}}, "gpu3": {"time": {"ts": 1677893021975643000, "dur": 385000, "relative_dur": 0.06637931034482758, "relative_gap_to_previous": 0.00017241379310344826, "parent_is_longest": true, "runtime_str": "385 us"}}}, "id": "cFnhB7XDIMCqAIPW", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.223.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 385000, "runtime_str": "385 us", "start_timestamp": "01:23:41.943.943344", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 217, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021943540000, "dur": 58000, "relative_dur": 0.01, "relative_gap_to_previous": 0.02086206896551724, "parent_is_longest": true, "runtime_str": "58 us"}}, "gpu3": {"time": {"ts": 1677893021976030000, "dur": 447000, "relative_dur": 0.07706896551724138, "relative_gap_to_previous": 0.0003448275862068965, "parent_is_longest": true, "runtime_str": "447 us"}}}, "id": "puprOcaa9A1QHP0l", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.224.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 447000, "runtime_str": "447 us", "start_timestamp": "01:23:41.943.943540", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 218, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021943730000, "dur": 136000, "relative_dur": 0.023448275862068966, "relative_gap_to_previous": 0.020517241379310346, "parent_is_longest": true, "runtime_str": "136 us"}}, "gpu3": {"time": {"ts": 1677893021976478000, "dur": 944000, "relative_dur": 0.16275862068965516, "relative_gap_to_previous": 0.00017241379310344826, "parent_is_longest": true, "runtime_str": "944 us"}}}, "id": "W7ZAm7P5qzR2QfRq", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.225.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 944000, "runtime_str": "944 us", "start_timestamp": "01:23:41.943.943730", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021941648000, "dur": 2205000, "relative_dur": 0.11019490254872563, "relative_gap_to_previous": 0.004847576211894053, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893021971622000, "dur": 5800000, "relative_dur": 0.2898550724637681, "relative_gap_to_previous": 9.995002498750625e-05, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "0K4AGXOWVvIw8OEk", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.216.pt.trace.json", "trace_disk_size": "23.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5800000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.941.941648", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 193}, {"idx": 219, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu2": {"time": {"ts": 1677893021943871000, "dur": 222000, "relative_dur": 0.011094452773613194, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "222 us"}}, "gpu3": {"time": {"ts": 1677893021977424000, "dur": 99000, "relative_dur": 0.004947526236881559, "relative_gap_to_previous": 9.995002498750625e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "dmvH8MHmV48jULKn", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.226.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 222000, "runtime_str": "222 us", "start_timestamp": "01:23:41.943.943871", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 220, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu2": {"time": {"ts": 1677893021944106000, "dur": 221000, "relative_dur": 0.01104447776111944, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "221 us"}}, "gpu3": {"time": {"ts": 1677893021977524000, "dur": 228000, "relative_dur": 0.011394302848575712, "relative_gap_to_previous": 4.9975012493753125e-05, "parent_is_longest": true, "runtime_str": "228 us"}}}, "id": "kMSgEhacaFMnZKie", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.227.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 228000, "runtime_str": "228 us", "start_timestamp": "01:23:41.944.944106", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 221, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"idx": 222, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu2": {"time": {"ts": 1677893021944451000, "dur": 214000, "relative_dur": 0.02848396113403434, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "214 us"}}, "gpu3": {"time": {"ts": 1677893021977753000, "dur": 3345000, "relative_dur": 0.4452282709969386, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "LUVStnFdrGGwH51P", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.229.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3345000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.944.944451", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 223, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu2": {"time": {"ts": 1677893021944791000, "dur": 52000, "relative_dur": 0.006921336350326102, "relative_gap_to_previous": 0.01677093038732863, "parent_is_longest": true, "runtime_str": "52 us"}}, "gpu3": {"time": {"ts": 1677893021981099000, "dur": 320000, "relative_dur": 0.042592839078929856, "relative_gap_to_previous": 0.0001331026221216558, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "UuaZYpBAUUTetZkE", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.230.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "01:23:41.944.944791", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 224, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu2": {"time": {"ts": 1677893021944965000, "dur": 65000, "relative_dur": 0.008651670437907627, "relative_gap_to_previous": 0.016238519898842008, "parent_is_longest": true, "runtime_str": "65 us"}}, "gpu3": {"time": {"ts": 1677893021981420000, "dur": 385000, "relative_dur": 0.05124450951683748, "relative_gap_to_previous": 0.0001331026221216558, "parent_is_longest": true, "runtime_str": "385 us"}}}, "id": "eUPffYJi6pBrOGiA", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.231.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 385000, "runtime_str": "385 us", "start_timestamp": "01:23:41.944.944965", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 225, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu2": {"time": {"ts": 1677893021945152000, "dur": 104000, "relative_dur": 0.013842672700652203, "relative_gap_to_previous": 0.016238519898842008, "parent_is_longest": true, "runtime_str": "104 us"}}, "gpu3": {"time": {"ts": 1677893021981807000, "dur": 3359000, "relative_dur": 0.4470917077066418, "relative_gap_to_previous": 0.0002662052442433116, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "Zt45idkOzywf1qo1", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.232.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3359000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.945.945152", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 226, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu2": {"time": {"ts": 1677893021945384000, "dur": 67000, "relative_dur": 0.008917875682150939, "relative_gap_to_previous": 0.017037135631571942, "parent_is_longest": true, "runtime_str": "67 us"}}, "gpu3": {"time": {"ts": 1677893021985167000, "dur": 99000, "relative_dur": 0.013177159590043924, "relative_gap_to_previous": 0.0001331026221216558, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "V4vodWYgN8IWUqjX", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.233.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "01:23:41.945.945384", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1677893021944451000, "dur": 1000000, "relative_dur": 0.04997501249375312, "relative_gap_to_previous": 0.004897551224387806, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893021977753000, "dur": 7513000, "relative_dur": 0.3754622688655672, "relative_gap_to_previous": 4.9975012493753125e-05, "parent_is_longest": true, "runtime_str": "8 ms"}}}, "id": "PN1HtWgm19Vlio9G", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.228.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7513000, "runtime_str": "8 ms", "start_timestamp": "01:23:41.944.944451", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1677893021938534000, "dur": 6891000, "relative_dur": 0.0571691679733194, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "7 ms"}}, "gpu3": {"time": {"ts": 1677893021965256000, "dur": 20010000, "relative_dur": 0.16600711814629532, "relative_gap_to_previous": 1.659241560682612e-05, "parent_is_longest": true, "runtime_str": "20 ms"}}}, "id": "gT3jBVpGREhB4iTE", "pretty_name": "Layer3", "trace_file": "/results/Transformer/Transformer.203.pt.trace.json", "trace_disk_size": "64.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 20010000, "runtime_str": "20 ms", "start_timestamp": "01:23:41.938.938534", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 535}, {"idx": 227, "name": "Layer4", "type": "Layer4", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"idx": 228, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"idx": 229, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021945466000, "dur": 659000, "relative_dur": 0.10910596026490066, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "659 us"}}, "gpu3": {"time": {"ts": 1677893021985267000, "dur": 1178000, "relative_dur": 0.19503311258278147, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1 ms"}}}, "id": "v2T1PQxQkTLl4M4Y", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.236.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1178000, "runtime_str": "1 ms", "start_timestamp": "01:23:41.945.945466", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 51}, {"idx": 230, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021946270000, "dur": 124000, "relative_dur": 0.02052980132450331, "relative_gap_to_previous": 0.02185430463576159, "parent_is_longest": true, "runtime_str": "124 us"}}, "gpu3": {"time": {"ts": 1677893021986448000, "dur": 946000, "relative_dur": 0.1566225165562914, "relative_gap_to_previous": 0.0004966887417218543, "parent_is_longest": true, "runtime_str": "946 us"}}}, "id": "RWuqzwb31Smbd3RI", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.237.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 946000, "runtime_str": "946 us", "start_timestamp": "01:23:41.946.946270", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 231, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021946535000, "dur": 133000, "relative_dur": 0.022019867549668875, "relative_gap_to_previous": 0.02119205298013245, "parent_is_longest": true, "runtime_str": "133 us"}}, "gpu3": {"time": {"ts": 1677893021987395000, "dur": 944000, "relative_dur": 0.1562913907284768, "relative_gap_to_previous": 0.00016556291390728477, "parent_is_longest": true, "runtime_str": "944 us"}}}, "id": "Very4mFeEN1QNJDA", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.238.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 944000, "runtime_str": "944 us", "start_timestamp": "01:23:41.946.946535", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 232, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021946685000, "dur": 226000, "relative_dur": 0.03741721854304636, "relative_gap_to_previous": 0.0006622516556291391, "parent_is_longest": true, "runtime_str": "226 us"}}, "gpu3": {"time": {"ts": 1677893021988341000, "dur": 82000, "relative_dur": 0.013576158940397352, "relative_gap_to_previous": 0.00033112582781456954, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "izIiK24XEv0WfvL1", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.239.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 226000, "runtime_str": "226 us", "start_timestamp": "01:23:41.946.946685", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 233, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021947045000, "dur": 76000, "relative_dur": 0.012582781456953643, "relative_gap_to_previous": 0.020033112582781457, "parent_is_longest": true, "runtime_str": "76 us"}}, "gpu3": {"time": {"ts": 1677893021988424000, "dur": 777000, "relative_dur": 0.12864238410596027, "relative_gap_to_previous": 0.00016556291390728477, "parent_is_longest": true, "runtime_str": "777 us"}}}, "id": "lkX8GM8aDVzm8MXC", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.240.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 777000, "runtime_str": "777 us", "start_timestamp": "01:23:41.947.947045", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 234, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021947274000, "dur": 48000, "relative_dur": 0.007947019867549669, "relative_gap_to_previous": 0.023178807947019868, "parent_is_longest": true, "runtime_str": "48 us"}}, "gpu3": {"time": {"ts": 1677893021989202000, "dur": 321000, "relative_dur": 0.05314569536423841, "relative_gap_to_previous": 0.00016556291390728477, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "SUNy0yReTmR2jAX4", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.241.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "01:23:41.947.947274", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 235, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021947456000, "dur": 63000, "relative_dur": 0.010430463576158941, "relative_gap_to_previous": 0.020033112582781457, "parent_is_longest": true, "runtime_str": "63 us"}}, "gpu3": {"time": {"ts": 1677893021989524000, "dur": 388000, "relative_dur": 0.06423841059602649, "relative_gap_to_previous": 0.00016556291390728477, "parent_is_longest": true, "runtime_str": "388 us"}}}, "id": "rmj64Dd31f9GTY1G", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.242.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 388000, "runtime_str": "388 us", "start_timestamp": "01:23:41.947.947456", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 236, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021947683000, "dur": 68000, "relative_dur": 0.011258278145695364, "relative_gap_to_previous": 0.025, "parent_is_longest": true, "runtime_str": "68 us"}}, "gpu3": {"time": {"ts": 1677893021989914000, "dur": 447000, "relative_dur": 0.07400662251655629, "relative_gap_to_previous": 0.00033112582781456954, "parent_is_longest": true, "runtime_str": "447 us"}}}, "id": "sVi7FjTqAYGnTfTF", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.243.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 447000, "runtime_str": "447 us", "start_timestamp": "01:23:41.947.947683", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 237, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021947884000, "dur": 137000, "relative_dur": 0.022682119205298012, "relative_gap_to_previous": 0.019867549668874173, "parent_is_longest": true, "runtime_str": "137 us"}}, "gpu3": {"time": {"ts": 1677893021990362000, "dur": 945000, "relative_dur": 0.1564569536423841, "relative_gap_to_previous": 0.00016556291390728477, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "Az9UIqFLHm0X7wCh", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.244.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.947.947884", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021945466000, "dur": 2542000, "relative_dur": 0.12696034362201578, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}, "gpu3": {"time": {"ts": 1677893021985267000, "dur": 6040000, "relative_dur": 0.30166816501847965, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "6lKscdOY4LnIB6ZX", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.235.pt.trace.json", "trace_disk_size": "25.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 6040000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.945.945466", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 207}, {"idx": 238, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu2": {"time": {"ts": 1677893021948026000, "dur": 223000, "relative_dur": 0.011137748476675657, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "223 us"}}, "gpu3": {"time": {"ts": 1677893021991308000, "dur": 99000, "relative_dur": 0.0049445609829187895, "relative_gap_to_previous": 4.9945060433523126e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "EqIf5fvoFYybY69c", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.245.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 223000, "runtime_str": "223 us", "start_timestamp": "01:23:41.948.948026", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 239, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu2": {"time": {"ts": 1677893021948262000, "dur": 223000, "relative_dur": 0.011137748476675657, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "223 us"}}, "gpu3": {"time": {"ts": 1677893021991408000, "dur": 235000, "relative_dur": 0.011737089201877934, "relative_gap_to_previous": 4.9945060433523126e-05, "parent_is_longest": true, "runtime_str": "235 us"}}}, "id": "Xy7gKooFnNcx1k2b", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.246.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 235000, "runtime_str": "235 us", "start_timestamp": "01:23:41.948.948262", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 240, "name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"idx": 241, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021948609000, "dur": 307000, "relative_dur": 0.05290367051525073, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "307 us"}}, "gpu3": {"time": {"ts": 1677893021991644000, "dur": 945000, "relative_dur": 0.16284680337756333, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "63SsLE6WOAmfronS", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.248.pt.trace.json", "trace_disk_size": "4.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.948.948609", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 37}, {"idx": 242, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021949057000, "dur": 126000, "relative_dur": 0.021712907117008445, "relative_gap_to_previous": 0.022057556436326037, "parent_is_longest": true, "runtime_str": "126 us"}}, "gpu3": {"time": {"ts": 1677893021992591000, "dur": 945000, "relative_dur": 0.16284680337756333, "relative_gap_to_previous": 0.0003446493193175943, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "AQJXKIHqtP0qPFvH", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.249.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.949.949057", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 243, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021949329000, "dur": 126000, "relative_dur": 0.021712907117008445, "relative_gap_to_previous": 0.022919179734620022, "parent_is_longest": true, "runtime_str": "126 us"}}, "gpu3": {"time": {"ts": 1677893021993538000, "dur": 945000, "relative_dur": 0.16284680337756333, "relative_gap_to_previous": 0.0003446493193175943, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "oVWc4hM8goH5d7n0", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.250.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.949.949329", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 244, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021949471000, "dur": 265000, "relative_dur": 0.04566603480958125, "relative_gap_to_previous": 0.0005169739789763915, "parent_is_longest": true, "runtime_str": "265 us"}}, "gpu3": {"time": {"ts": 1677893021994484000, "dur": 83000, "relative_dur": 0.014302946751680165, "relative_gap_to_previous": 0.00017232465965879716, "parent_is_longest": true, "runtime_str": "83 us"}}}, "id": "vqbqNjzJSvbgpFL7", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.251.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 265000, "runtime_str": "265 us", "start_timestamp": "01:23:41.949.949471", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 245, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021949872000, "dur": 72000, "relative_dur": 0.012407375495433396, "relative_gap_to_previous": 0.02119593313803205, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1677893021994568000, "dur": 777000, "relative_dur": 0.1338962605548854, "relative_gap_to_previous": 0.00017232465965879716, "parent_is_longest": true, "runtime_str": "777 us"}}}, "id": "vJpDsnPXO8NJfRds", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.252.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 777000, "runtime_str": "777 us", "start_timestamp": "01:23:41.949.949872", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 246, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021950097000, "dur": 47000, "relative_dur": 0.008099259003963468, "relative_gap_to_previous": 0.024125452352231604, "parent_is_longest": true, "runtime_str": "47 us"}}, "gpu3": {"time": {"ts": 1677893021995346000, "dur": 320000, "relative_dur": 0.0551438910908151, "relative_gap_to_previous": 0.00017232465965879716, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "G8VYeiwmpxL8ZC1g", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.253.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "01:23:41.950.950097", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 247, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021950279000, "dur": 61000, "relative_dur": 0.010511804239186628, "relative_gap_to_previous": 0.021023608478373257, "parent_is_longest": true, "runtime_str": "61 us"}}, "gpu3": {"time": {"ts": 1677893021995667000, "dur": 386000, "relative_dur": 0.06651731862829571, "relative_gap_to_previous": 0.00017232465965879716, "parent_is_longest": true, "runtime_str": "386 us"}}}, "id": "TbHk2DNPLOH2TRyT", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.254.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 386000, "runtime_str": "386 us", "start_timestamp": "01:23:41.950.950279", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 248, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021950472000, "dur": 59000, "relative_dur": 0.010167154919869033, "relative_gap_to_previous": 0.020506634499396863, "parent_is_longest": true, "runtime_str": "59 us"}}, "gpu3": {"time": {"ts": 1677893021996054000, "dur": 446000, "relative_dur": 0.07685679820782354, "relative_gap_to_previous": 0.00017232465965879716, "parent_is_longest": true, "runtime_str": "446 us"}}}, "id": "Kc8jCuRctPHFw76n", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.255.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 446000, "runtime_str": "446 us", "start_timestamp": "01:23:41.950.950472", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 249, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021950665000, "dur": 139000, "relative_dur": 0.023953127692572806, "relative_gap_to_previous": 0.02085128381871446, "parent_is_longest": true, "runtime_str": "139 us"}}, "gpu3": {"time": {"ts": 1677893021996501000, "dur": 946000, "relative_dur": 0.16301912803722213, "relative_gap_to_previous": 0.00017232465965879716, "parent_is_longest": true, "runtime_str": "946 us"}}}, "id": "DGv8ebSY4pyQ3gxz", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.256.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 946000, "runtime_str": "946 us", "start_timestamp": "01:23:41.950.950665", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021948609000, "dur": 2182000, "relative_dur": 0.10898012186594745, "relative_gap_to_previous": 0.004894615922485266, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893021991644000, "dur": 5803000, "relative_dur": 0.2898311856957347, "relative_gap_to_previous": 4.9945060433523126e-05, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "R2nbxKgFkpd6WuKO", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.247.pt.trace.json", "trace_disk_size": "23.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5803000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.948.948609", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 193}, {"idx": 250, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu2": {"time": {"ts": 1677893021950809000, "dur": 222000, "relative_dur": 0.011087803416242133, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "222 us"}}, "gpu3": {"time": {"ts": 1677893021997448000, "dur": 99000, "relative_dur": 0.0049445609829187895, "relative_gap_to_previous": 4.9945060433523126e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "jSjQeRE0Wenjhwzh", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.257.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 222000, "runtime_str": "222 us", "start_timestamp": "01:23:41.950.950809", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 251, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu2": {"time": {"ts": 1677893021951043000, "dur": 217000, "relative_dur": 0.010838078114074518, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "217 us"}}, "gpu3": {"time": {"ts": 1677893021997549000, "dur": 226000, "relative_dur": 0.011287583657976226, "relative_gap_to_previous": 9.989012086704625e-05, "parent_is_longest": true, "runtime_str": "226 us"}}}, "id": "QgKlCNK9EKfB8sH3", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.258.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 226000, "runtime_str": "226 us", "start_timestamp": "01:23:41.951.951043", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 252, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"idx": 253, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu2": {"time": {"ts": 1677893021951382000, "dur": 215000, "relative_dur": 0.02862087326943557, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "215 us"}}, "gpu3": {"time": {"ts": 1677893021997777000, "dur": 3344000, "relative_dur": 0.44515441959531415, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "3ODOm9MWAs8rcrE2", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.260.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3344000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.951.951382", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 254, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu2": {"time": {"ts": 1677893021951724000, "dur": 49000, "relative_dur": 0.0065228966986155485, "relative_gap_to_previous": 0.016906283280085196, "parent_is_longest": true, "runtime_str": "49 us"}}, "gpu3": {"time": {"ts": 1677893022001122000, "dur": 320000, "relative_dur": 0.042598509052183174, "relative_gap_to_previous": 0.00013312034078807243, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "MjT8CSDUjhqQrYxn", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.261.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "01:23:41.951.951724", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 255, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu2": {"time": {"ts": 1677893021951896000, "dur": 66000, "relative_dur": 0.00878594249201278, "relative_gap_to_previous": 0.01637380191693291, "parent_is_longest": true, "runtime_str": "66 us"}}, "gpu3": {"time": {"ts": 1677893022001443000, "dur": 385000, "relative_dur": 0.05125133120340788, "relative_gap_to_previous": 0.00013312034078807243, "parent_is_longest": true, "runtime_str": "385 us"}}}, "id": "NwzVNJvz79jvoRqv", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.262.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 385000, "runtime_str": "385 us", "start_timestamp": "01:23:41.951.951896", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 256, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu2": {"time": {"ts": 1677893021952086000, "dur": 104000, "relative_dur": 0.013844515441959531, "relative_gap_to_previous": 0.01650692225772098, "parent_is_longest": true, "runtime_str": "104 us"}}, "gpu3": {"time": {"ts": 1677893022001830000, "dur": 3360000, "relative_dur": 0.4472843450479233, "relative_gap_to_previous": 0.00026624068157614486, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "lwFIHhrFLARRMUkO", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.263.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3360000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.952.952086", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 257, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu2": {"time": {"ts": 1677893021952317000, "dur": 68000, "relative_dur": 0.009052183173588925, "relative_gap_to_previous": 0.016906283280085196, "parent_is_longest": true, "runtime_str": "68 us"}}, "gpu3": {"time": {"ts": 1677893022005191000, "dur": 98000, "relative_dur": 0.013045793397231097, "relative_gap_to_previous": 0.00013312034078807243, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "mbH0s6RhIEZzjO77", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.264.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 98000, "runtime_str": "98 us", "start_timestamp": "01:23:41.952.952317", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1677893021951382000, "dur": 1003000, "relative_dur": 0.05009489561482369, "relative_gap_to_previous": 0.00479472580161822, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893021997777000, "dur": 7512000, "relative_dur": 0.3751872939766257, "relative_gap_to_previous": 9.989012086704625e-05, "parent_is_longest": true, "runtime_str": "8 ms"}}}, "id": "EzqIuZkfMcdBKVbu", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.259.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7512000, "runtime_str": "8 ms", "start_timestamp": "01:23:41.951.951382", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1677893021945466000, "dur": 6893000, "relative_dur": 0.05718576038892622, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "7 ms"}}, "gpu3": {"time": {"ts": 1677893021985267000, "dur": 20022000, "relative_dur": 0.16610667263993628, "relative_gap_to_previous": 8.29620780341306e-06, "parent_is_longest": true, "runtime_str": "20 ms"}}}, "id": "c1Wt9rhHsBq6yMQd", "pretty_name": "Layer4", "trace_file": "/results/Transformer/Transformer.234.pt.trace.json", "trace_disk_size": "64.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 20022000, "runtime_str": "20 ms", "start_timestamp": "01:23:41.945.945466", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 535}, {"idx": 258, "name": "Layer5", "type": "Layer5", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"idx": 259, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"idx": 260, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021952400000, "dur": 695000, "relative_dur": 0.11516155758077878, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "695 us"}}, "gpu3": {"time": {"ts": 1677893022005290000, "dur": 1177000, "relative_dur": 0.19502899751449876, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1 ms"}}}, "id": "ecL2Px3RsQUHdCfb", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.267.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1177000, "runtime_str": "1 ms", "start_timestamp": "01:23:41.952.952400", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 51}, {"idx": 261, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021953238000, "dur": 131000, "relative_dur": 0.021706710853355425, "relative_gap_to_previous": 0.021706710853355425, "parent_is_longest": true, "runtime_str": "131 us"}}, "gpu3": {"time": {"ts": 1677893022006468000, "dur": 945000, "relative_dur": 0.15658657829328915, "relative_gap_to_previous": 0.00016570008285004143, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "71A3CJqCh0CuVKn4", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.268.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.953.953238", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 262, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021953511000, "dur": 129000, "relative_dur": 0.021375310687655344, "relative_gap_to_previous": 0.021541010770505385, "parent_is_longest": true, "runtime_str": "129 us"}}, "gpu3": {"time": {"ts": 1677893022007415000, "dur": 946000, "relative_dur": 0.1567522783761392, "relative_gap_to_previous": 0.00033140016570008286, "parent_is_longest": true, "runtime_str": "946 us"}}}, "id": "kndguMEJi5KhJnes", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.269.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 946000, "runtime_str": "946 us", "start_timestamp": "01:23:41.953.953511", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 263, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021953657000, "dur": 224000, "relative_dur": 0.03711681855840928, "relative_gap_to_previous": 0.0008285004142502071, "parent_is_longest": true, "runtime_str": "224 us"}}, "gpu3": {"time": {"ts": 1677893022008362000, "dur": 82000, "relative_dur": 0.013587406793703396, "relative_gap_to_previous": 0.00016570008285004143, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "Cadkjfn7FWhVksiJ", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.270.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 224000, "runtime_str": "224 us", "start_timestamp": "01:23:41.953.953657", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 264, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021954015000, "dur": 75000, "relative_dur": 0.012427506213753107, "relative_gap_to_previous": 0.020215410107705053, "parent_is_longest": true, "runtime_str": "75 us"}}, "gpu3": {"time": {"ts": 1677893022008446000, "dur": 775000, "relative_dur": 0.12841756420878211, "relative_gap_to_previous": 0.00033140016570008286, "parent_is_longest": true, "runtime_str": "775 us"}}}, "id": "699uQlwlziV71WGM", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.271.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 775000, "runtime_str": "775 us", "start_timestamp": "01:23:41.954.954015", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 265, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021954241000, "dur": 48000, "relative_dur": 0.007953603976801988, "relative_gap_to_previous": 0.023032311516155757, "parent_is_longest": true, "runtime_str": "48 us"}}, "gpu3": {"time": {"ts": 1677893022009223000, "dur": 321000, "relative_dur": 0.0531897265948633, "relative_gap_to_previous": 0.00033140016570008286, "parent_is_longest": true, "runtime_str": "321 us"}}}, "id": "PAjpG5tDFSTOkIRr", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.272.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 321000, "runtime_str": "321 us", "start_timestamp": "01:23:41.954.954241", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 266, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021954423000, "dur": 62000, "relative_dur": 0.010273405136702569, "relative_gap_to_previous": 0.020215410107705053, "parent_is_longest": true, "runtime_str": "62 us"}}, "gpu3": {"time": {"ts": 1677893022009545000, "dur": 387000, "relative_dur": 0.06412593206296603, "relative_gap_to_previous": 0.00016570008285004143, "parent_is_longest": true, "runtime_str": "387 us"}}}, "id": "GpSanxK1R98CHmPv", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.273.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 387000, "runtime_str": "387 us", "start_timestamp": "01:23:41.954.954423", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 267, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021954619000, "dur": 57000, "relative_dur": 0.00944490472245236, "relative_gap_to_previous": 0.020215410107705053, "parent_is_longest": true, "runtime_str": "57 us"}}, "gpu3": {"time": {"ts": 1677893022009933000, "dur": 446000, "relative_dur": 0.07390223695111847, "relative_gap_to_previous": 0.00016570008285004143, "parent_is_longest": true, "runtime_str": "446 us"}}}, "id": "rsI6uEefX3Omvpae", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.274.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 446000, "runtime_str": "446 us", "start_timestamp": "01:23:41.954.954619", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 268, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021954809000, "dur": 137000, "relative_dur": 0.022700911350455676, "relative_gap_to_previous": 0.020049710024855012, "parent_is_longest": true, "runtime_str": "137 us"}}, "gpu3": {"time": {"ts": 1677893022010381000, "dur": 944000, "relative_dur": 0.1564208782104391, "relative_gap_to_previous": 0.00033140016570008286, "parent_is_longest": true, "runtime_str": "944 us"}}}, "id": "A54GX1ru5mgAR4uq", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.275.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 944000, "runtime_str": "944 us", "start_timestamp": "01:23:41.954.954809", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021952400000, "dur": 2534000, "relative_dur": 0.12651655100104847, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}, "gpu3": {"time": {"ts": 1677893022005290000, "dur": 6035000, "relative_dur": 0.3013130960107844, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "XwAnekXwEAKS6aqK", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.266.pt.trace.json", "trace_disk_size": "25.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 6035000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.952.952400", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 207}, {"idx": 269, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu2": {"time": {"ts": 1677893021954951000, "dur": 222000, "relative_dur": 0.011083928303959259, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "222 us"}}, "gpu3": {"time": {"ts": 1677893022011327000, "dur": 99000, "relative_dur": 0.004942832892306156, "relative_gap_to_previous": 9.98552099455789e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "fEUbdzPMk9IcJi0B", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.276.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 222000, "runtime_str": "222 us", "start_timestamp": "01:23:41.954.954951", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 270, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu2": {"time": {"ts": 1677893021955194000, "dur": 215000, "relative_dur": 0.010734435069149733, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "215 us"}}, "gpu3": {"time": {"ts": 1677893022011428000, "dur": 231000, "relative_dur": 0.011533276748714363, "relative_gap_to_previous": 9.98552099455789e-05, "parent_is_longest": true, "runtime_str": "231 us"}}}, "id": "0wj3SOctFuBAkyxk", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.277.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 231000, "runtime_str": "231 us", "start_timestamp": "01:23:41.955.955194", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 271, "name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"idx": 272, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021955533000, "dur": 310000, "relative_dur": 0.05339304168101963, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "310 us"}}, "gpu3": {"time": {"ts": 1677893022011661000, "dur": 945000, "relative_dur": 0.16276265931794695, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "fdpKwQJDr8UlS1Pc", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.279.pt.trace.json", "trace_disk_size": "4.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.955.955533", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 37}, {"idx": 273, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021955997000, "dur": 126000, "relative_dur": 0.021701687909059592, "relative_gap_to_previous": 0.023768515328970032, "parent_is_longest": true, "runtime_str": "126 us"}}, "gpu3": {"time": {"ts": 1677893022012607000, "dur": 946000, "relative_dur": 0.16293489493627283, "relative_gap_to_previous": 0.0001722356183258698, "parent_is_longest": true, "runtime_str": "946 us"}}}, "id": "IdIFWBo8X9DotPwW", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.280.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 946000, "runtime_str": "946 us", "start_timestamp": "01:23:41.955.955997", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 274, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021956264000, "dur": 124000, "relative_dur": 0.021357216672407853, "relative_gap_to_previous": 0.021529452290733723, "parent_is_longest": true, "runtime_str": "124 us"}}, "gpu3": {"time": {"ts": 1677893022013555000, "dur": 946000, "relative_dur": 0.16293489493627283, "relative_gap_to_previous": 0.0003444712366517396, "parent_is_longest": true, "runtime_str": "946 us"}}}, "id": "H1QSodwbDhzEAyqt", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.281.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 946000, "runtime_str": "946 us", "start_timestamp": "01:23:41.956.956264", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 275, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021956405000, "dur": 249000, "relative_dur": 0.042886668963141576, "relative_gap_to_previous": 0.0001722356183258698, "parent_is_longest": true, "runtime_str": "249 us"}}, "gpu3": {"time": {"ts": 1677893022014502000, "dur": 82000, "relative_dur": 0.014123320702721322, "relative_gap_to_previous": 0.0001722356183258698, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "ckd3DEwUtbQQsjCD", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.282.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 249000, "runtime_str": "249 us", "start_timestamp": "01:23:41.956.956405", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 276, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021956800000, "dur": 73000, "relative_dur": 0.012573200137788494, "relative_gap_to_previous": 0.022390630382363073, "parent_is_longest": true, "runtime_str": "73 us"}}, "gpu3": {"time": {"ts": 1677893022014585000, "dur": 774000, "relative_dur": 0.1333103685842232, "relative_gap_to_previous": 0.0001722356183258698, "parent_is_longest": true, "runtime_str": "774 us"}}}, "id": "RwK8BrKHMjxCx4YB", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.283.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 774000, "runtime_str": "774 us", "start_timestamp": "01:23:41.956.956800", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 277, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021957029000, "dur": 47000, "relative_dur": 0.00809507406131588, "relative_gap_to_previous": 0.02411298656562177, "parent_is_longest": true, "runtime_str": "47 us"}}, "gpu3": {"time": {"ts": 1677893022015361000, "dur": 323000, "relative_dur": 0.05563210471925594, "relative_gap_to_previous": 0.0003444712366517396, "parent_is_longest": true, "runtime_str": "323 us"}}}, "id": "XByIW2jslT4jC4Ic", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.284.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 323000, "runtime_str": "323 us", "start_timestamp": "01:23:41.957.957029", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 278, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021957211000, "dur": 63000, "relative_dur": 0.010850843954529796, "relative_gap_to_previous": 0.020496038580778506, "parent_is_longest": true, "runtime_str": "63 us"}}, "gpu3": {"time": {"ts": 1677893022015685000, "dur": 387000, "relative_dur": 0.0666551842921116, "relative_gap_to_previous": 0.0001722356183258698, "parent_is_longest": true, "runtime_str": "387 us"}}}, "id": "l33SmUdacoen5GvX", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.285.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 387000, "runtime_str": "387 us", "start_timestamp": "01:23:41.957.957211", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 279, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021957406000, "dur": 58000, "relative_dur": 0.009989665862900447, "relative_gap_to_previous": 0.019979331725800895, "parent_is_longest": true, "runtime_str": "58 us"}}, "gpu3": {"time": {"ts": 1677893022016073000, "dur": 447000, "relative_dur": 0.0769893213916638, "relative_gap_to_previous": 0.0001722356183258698, "parent_is_longest": true, "runtime_str": "447 us"}}}, "id": "oU6ttO82RVwtm5lQ", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.286.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 447000, "runtime_str": "447 us", "start_timestamp": "01:23:41.957.957406", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 280, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021957606000, "dur": 163000, "relative_dur": 0.028074405787116775, "relative_gap_to_previous": 0.021701687909059592, "parent_is_longest": true, "runtime_str": "163 us"}}, "gpu3": {"time": {"ts": 1677893022016522000, "dur": 945000, "relative_dur": 0.16276265931794695, "relative_gap_to_previous": 0.0003444712366517396, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "oedDnjpEKJwK92ta", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.287.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.957.957606", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021955533000, "dur": 2220000, "relative_dur": 0.11083928303959259, "relative_gap_to_previous": 0.004793050077387788, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893022011661000, "dur": 5806000, "relative_dur": 0.28987967447201557, "relative_gap_to_previous": 9.98552099455789e-05, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "MRSLCJ5Ut5tlI4tO", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.278.pt.trace.json", "trace_disk_size": "23.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5806000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.955.955533", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 193}, {"idx": 281, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu2": {"time": {"ts": 1677893021957774000, "dur": 221000, "relative_dur": 0.01103400069898647, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "221 us"}}, "gpu3": {"time": {"ts": 1677893022017469000, "dur": 99000, "relative_dur": 0.004942832892306156, "relative_gap_to_previous": 9.98552099455789e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "dK5kqdFKnxkH69AY", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.288.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 221000, "runtime_str": "221 us", "start_timestamp": "01:23:41.957.957774", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 282, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu2": {"time": {"ts": 1677893021958007000, "dur": 218000, "relative_dur": 0.010884217884068102, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "218 us"}}, "gpu3": {"time": {"ts": 1677893022017569000, "dur": 231000, "relative_dur": 0.011533276748714363, "relative_gap_to_previous": 4.992760497278945e-05, "parent_is_longest": true, "runtime_str": "231 us"}}}, "id": "DszkSLHlzowsUwSS", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.289.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 231000, "runtime_str": "231 us", "start_timestamp": "01:23:41.958.958007", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 283, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"idx": 284, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu2": {"time": {"ts": 1677893021958353000, "dur": 215000, "relative_dur": 0.02859803139132748, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "215 us"}}, "gpu3": {"time": {"ts": 1677893022017801000, "dur": 3345000, "relative_dur": 0.4449321628092578, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "EOXMxjE0i9zD7sIW", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.291.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3345000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.958.958353", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 285, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu2": {"time": {"ts": 1677893021958695000, "dur": 55000, "relative_dur": 0.007315775472200053, "relative_gap_to_previous": 0.016892790635807394, "parent_is_longest": true, "runtime_str": "55 us"}}, "gpu3": {"time": {"ts": 1677893022021148000, "dur": 320000, "relative_dur": 0.042564511838254854, "relative_gap_to_previous": 0.00026602819898909286, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "3oFxlEnYr9b0jGRj", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.292.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "01:23:41.958.958695", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 286, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu2": {"time": {"ts": 1677893021958900000, "dur": 65000, "relative_dur": 0.008645916467145517, "relative_gap_to_previous": 0.019952114924181964, "parent_is_longest": true, "runtime_str": "65 us"}}, "gpu3": {"time": {"ts": 1677893022021470000, "dur": 387000, "relative_dur": 0.051476456504389465, "relative_gap_to_previous": 0.00026602819898909286, "parent_is_longest": true, "runtime_str": "387 us"}}}, "id": "mq96NoYiJ7gQ3sGv", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.293.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 387000, "runtime_str": "387 us", "start_timestamp": "01:23:41.958.958900", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 287, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu2": {"time": {"ts": 1677893021959088000, "dur": 107000, "relative_dur": 0.014232508645916467, "relative_gap_to_previous": 0.01636073423782921, "parent_is_longest": true, "runtime_str": "107 us"}}, "gpu3": {"time": {"ts": 1677893022021859000, "dur": 3360000, "relative_dur": 0.44692737430167595, "relative_gap_to_previous": 0.00026602819898909286, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "IVxjWR2zo1mEJg8n", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.294.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3360000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.959.959088", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 288, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu2": {"time": {"ts": 1677893021959326000, "dur": 69000, "relative_dur": 0.009177972865123703, "relative_gap_to_previous": 0.017424847033785582, "parent_is_longest": true, "runtime_str": "69 us"}}, "gpu3": {"time": {"ts": 1677893022025220000, "dur": 99000, "relative_dur": 0.013168395849960097, "relative_gap_to_previous": 0.00013301409949454643, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "ty5QNyRnxnSuEGXH", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.295.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "01:23:41.959.959326", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1677893021958353000, "dur": 1042000, "relative_dur": 0.05202456438164661, "relative_gap_to_previous": 0.004992760497278945, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022017801000, "dur": 7518000, "relative_dur": 0.3753557341854311, "relative_gap_to_previous": 4.992760497278945e-05, "parent_is_longest": true, "runtime_str": "8 ms"}}}, "id": "V25RLECMHAJtWOAw", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.290.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7518000, "runtime_str": "8 ms", "start_timestamp": "01:23:41.958.958353", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1677893021952400000, "dur": 6967000, "relative_dur": 0.057799679766378786, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "7 ms"}}, "gpu3": {"time": {"ts": 1677893022005290000, "dur": 20029000, "relative_dur": 0.16616474609456017, "relative_gap_to_previous": 8.29620780341306e-06, "parent_is_longest": true, "runtime_str": "20 ms"}}}, "id": "ZNJM3vwofSTnSTnC", "pretty_name": "Layer5", "trace_file": "/results/Transformer/Transformer.265.pt.trace.json", "trace_disk_size": "64.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 20029000, "runtime_str": "20 ms", "start_timestamp": "01:23:41.952.952400", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 535}, {"idx": 289, "name": "Layer6", "type": "Layer6", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"idx": 290, "name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"idx": 291, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021959410000, "dur": 643000, "relative_dur": 0.1064922159655515, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "643 us"}}, "gpu3": {"time": {"ts": 1677893022025319000, "dur": 1173000, "relative_dur": 0.19426962570387546, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1 ms"}}}, "id": "9dTwFuzGhwJJExLn", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.298.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1173000, "runtime_str": "1 ms", "start_timestamp": "01:23:41.959.959410", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 51}, {"idx": 292, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021960196000, "dur": 126000, "relative_dur": 0.020867837032129844, "relative_gap_to_previous": 0.021530308049022857, "parent_is_longest": true, "runtime_str": "126 us"}}, "gpu3": {"time": {"ts": 1677893022026494000, "dur": 944000, "relative_dur": 0.15634315998675058, "relative_gap_to_previous": 0.0003312355084465055, "parent_is_longest": true, "runtime_str": "944 us"}}}, "id": "bclab560fIwHZuKq", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.299.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 944000, "runtime_str": "944 us", "start_timestamp": "01:23:41.960.960196", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 293, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021960462000, "dur": 125000, "relative_dur": 0.02070221927790659, "relative_gap_to_previous": 0.021033454786353097, "parent_is_longest": true, "runtime_str": "125 us"}}, "gpu3": {"time": {"ts": 1677893022027441000, "dur": 945000, "relative_dur": 0.15650877774097383, "relative_gap_to_previous": 0.0004968532626697582, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "2kGt85XRCDnDoToA", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.300.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.960.960462", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 294, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021960604000, "dur": 225000, "relative_dur": 0.03726399470023187, "relative_gap_to_previous": 0.000662471016893011, "parent_is_longest": true, "runtime_str": "225 us"}}, "gpu3": {"time": {"ts": 1677893022028387000, "dur": 82000, "relative_dur": 0.013580655846306725, "relative_gap_to_previous": 0.00016561775422325274, "parent_is_longest": true, "runtime_str": "82 us"}}}, "id": "LS29Fa33fFL1m2xc", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.301.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 225000, "runtime_str": "225 us", "start_timestamp": "01:23:41.960.960604", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 295, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021960964000, "dur": 75000, "relative_dur": 0.012421331566743955, "relative_gap_to_previous": 0.020205366015236835, "parent_is_longest": true, "runtime_str": "75 us"}}, "gpu3": {"time": {"ts": 1677893022028470000, "dur": 783000, "relative_dur": 0.12967870155680689, "relative_gap_to_previous": 0.00016561775422325274, "parent_is_longest": true, "runtime_str": "783 us"}}}, "id": "JHDBH00FxD9cmsRn", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.302.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 783000, "runtime_str": "783 us", "start_timestamp": "01:23:41.960.960964", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 296, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021961192000, "dur": 48000, "relative_dur": 0.007949652202716132, "relative_gap_to_previous": 0.02318648559125538, "parent_is_longest": true, "runtime_str": "48 us"}}, "gpu3": {"time": {"ts": 1677893022029255000, "dur": 320000, "relative_dur": 0.052997681351440874, "relative_gap_to_previous": 0.0003312355084465055, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "lqDH16crRle5aOt9", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.303.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "01:23:41.961.961192", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 297, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021961374000, "dur": 62000, "relative_dur": 0.010268300761841669, "relative_gap_to_previous": 0.020039748261013582, "parent_is_longest": true, "runtime_str": "62 us"}}, "gpu3": {"time": {"ts": 1677893022029577000, "dur": 386000, "relative_dur": 0.06392845313017556, "relative_gap_to_previous": 0.0003312355084465055, "parent_is_longest": true, "runtime_str": "386 us"}}}, "id": "XYOOMojyJLLzaVYj", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.304.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 386000, "runtime_str": "386 us", "start_timestamp": "01:23:41.961.961374", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 298, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021961569000, "dur": 56000, "relative_dur": 0.009274594236502154, "relative_gap_to_previous": 0.01987413050679033, "parent_is_longest": true, "runtime_str": "56 us"}}, "gpu3": {"time": {"ts": 1677893022029964000, "dur": 447000, "relative_dur": 0.07403113613779397, "relative_gap_to_previous": 0.00016561775422325274, "parent_is_longest": true, "runtime_str": "447 us"}}}, "id": "lQ7QyAkTBKo1kxYj", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.305.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 447000, "runtime_str": "447 us", "start_timestamp": "01:23:41.961.961569", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 299, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021961758000, "dur": 137000, "relative_dur": 0.022689632328585625, "relative_gap_to_previous": 0.01987413050679033, "parent_is_longest": true, "runtime_str": "137 us"}}, "gpu3": {"time": {"ts": 1677893022030413000, "dur": 944000, "relative_dur": 0.15634315998675058, "relative_gap_to_previous": 0.0003312355084465055, "parent_is_longest": true, "runtime_str": "944 us"}}}, "id": "ktt7cCEjwpmr8oUR", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.306.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 944000, "runtime_str": "944 us", "start_timestamp": "01:23:41.961.961758", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021959410000, "dur": 2472000, "relative_dur": 0.12338407786373846, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893022025319000, "dur": 6038000, "relative_dur": 0.30137259795358123, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "6fvicZKpTVTtvOmu", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.297.pt.trace.json", "trace_disk_size": "25.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 6038000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.959.959410", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 207}, {"idx": 300, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu2": {"time": {"ts": 1677893021961901000, "dur": 220000, "relative_dur": 0.010980783628649862, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "220 us"}}, "gpu3": {"time": {"ts": 1677893022031359000, "dur": 98000, "relative_dur": 0.004891439980034939, "relative_gap_to_previous": 9.982530571499875e-05, "parent_is_longest": true, "runtime_str": "98 us"}}}, "id": "nofojmuA4Kx9thnC", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.307.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 220000, "runtime_str": "220 us", "start_timestamp": "01:23:41.961.961901", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 301, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu2": {"time": {"ts": 1677893021962135000, "dur": 214000, "relative_dur": 0.010681307711504866, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "214 us"}}, "gpu3": {"time": {"ts": 1677893022031459000, "dur": 228000, "relative_dur": 0.011380084851509858, "relative_gap_to_previous": 9.982530571499875e-05, "parent_is_longest": true, "runtime_str": "228 us"}}}, "id": "fLQzRUErS6TzXiDb", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.308.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 228000, "runtime_str": "228 us", "start_timestamp": "01:23:41.962.962135", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 302, "name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"idx": 303, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu2": {"time": {"ts": 1677893021962472000, "dur": 353000, "relative_dur": 0.06076777414357032, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "353 us"}}, "gpu3": {"time": {"ts": 1677893022031688000, "dur": 945000, "relative_dur": 0.16267860216904803, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "pNl2Cj1Dc6HhPl8T", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.310.pt.trace.json", "trace_disk_size": "4.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.962.962472", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 37}, {"idx": 304, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu2": {"time": {"ts": 1677893021962970000, "dur": 124000, "relative_dur": 0.021346186951282493, "relative_gap_to_previous": 0.022723360302978137, "parent_is_longest": true, "runtime_str": "124 us"}}, "gpu3": {"time": {"ts": 1677893022032634000, "dur": 947000, "relative_dur": 0.16302289550697194, "relative_gap_to_previous": 0.00017214666896195557, "parent_is_longest": true, "runtime_str": "947 us"}}}, "id": "1ZBJQNzgAESDsYra", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.311.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 947000, "runtime_str": "947 us", "start_timestamp": "01:23:41.962.962970", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 305, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu2": {"time": {"ts": 1677893021963241000, "dur": 123000, "relative_dur": 0.021174040282320537, "relative_gap_to_previous": 0.023067653640902047, "parent_is_longest": true, "runtime_str": "123 us"}}, "gpu3": {"time": {"ts": 1677893022033582000, "dur": 945000, "relative_dur": 0.16267860216904803, "relative_gap_to_previous": 0.00017214666896195557, "parent_is_longest": true, "runtime_str": "945 us"}}}, "id": "sbRthJwCUELCTlE7", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.312.pt.trace.json", "trace_disk_size": "3.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 945000, "runtime_str": "945 us", "start_timestamp": "01:23:41.963.963241", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 29}, {"idx": 306, "name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu2": {"time": {"ts": 1677893021963381000, "dur": 242000, "relative_dur": 0.04165949388879325, "relative_gap_to_previous": 0.0006885866758478223, "parent_is_longest": true, "runtime_str": "242 us"}}, "gpu3": {"time": {"ts": 1677893022034529000, "dur": 83000, "relative_dur": 0.014288173523842315, "relative_gap_to_previous": 0.00034429333792391115, "parent_is_longest": true, "runtime_str": "83 us"}}}, "id": "UwVJ5R3Td96mBeoi", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.313.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 242000, "runtime_str": "242 us", "start_timestamp": "01:23:41.963.963381", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 307, "name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu2": {"time": {"ts": 1677893021963772000, "dur": 74000, "relative_dur": 0.012738853503184714, "relative_gap_to_previous": 0.02341194697882596, "parent_is_longest": true, "runtime_str": "74 us"}}, "gpu3": {"time": {"ts": 1677893022034613000, "dur": 778000, "relative_dur": 0.13393010845240144, "relative_gap_to_previous": 0.00017214666896195557, "parent_is_longest": true, "runtime_str": "778 us"}}}, "id": "GGKg3tshtGHWhGmL", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.314.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 778000, "runtime_str": "778 us", "start_timestamp": "01:23:41.963.963772", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 308, "name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu2": {"time": {"ts": 1677893021963999000, "dur": 47000, "relative_dur": 0.008090893441211913, "relative_gap_to_previous": 0.02410053365467378, "parent_is_longest": true, "runtime_str": "47 us"}}, "gpu3": {"time": {"ts": 1677893022035392000, "dur": 322000, "relative_dur": 0.0554312274057497, "relative_gap_to_previous": 0.00017214666896195557, "parent_is_longest": true, "runtime_str": "322 us"}}}, "id": "biUmddZeVAT6emyg", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.315.pt.trace.json", "trace_disk_size": "901 Bytes", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 322000, "runtime_str": "322 us", "start_timestamp": "01:23:41.963.963999", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 309, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu2": {"time": {"ts": 1677893021964181000, "dur": 60000, "relative_dur": 0.010328800137717335, "relative_gap_to_previous": 0.02100189361335858, "parent_is_longest": true, "runtime_str": "60 us"}}, "gpu3": {"time": {"ts": 1677893022035716000, "dur": 386000, "relative_dur": 0.06644861421931486, "relative_gap_to_previous": 0.00034429333792391115, "parent_is_longest": true, "runtime_str": "386 us"}}}, "id": "iM4kCDJpoQzdrJJJ", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.316.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 386000, "runtime_str": "386 us", "start_timestamp": "01:23:41.964.964181", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 310, "name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu2": {"time": {"ts": 1677893021964375000, "dur": 58000, "relative_dur": 0.009984506799793425, "relative_gap_to_previous": 0.020829746944396627, "parent_is_longest": true, "runtime_str": "58 us"}}, "gpu3": {"time": {"ts": 1677893022036103000, "dur": 446000, "relative_dur": 0.0767774143570322, "relative_gap_to_previous": 0.00017214666896195557, "parent_is_longest": true, "runtime_str": "446 us"}}}, "id": "6VbmlBdbWYLZScZT", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.317.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 446000, "runtime_str": "446 us", "start_timestamp": "01:23:41.964.964375", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 311, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu2": {"time": {"ts": 1677893021964563000, "dur": 141000, "relative_dur": 0.024272680323635738, "relative_gap_to_previous": 0.020141160268548803, "parent_is_longest": true, "runtime_str": "141 us"}}, "gpu3": {"time": {"ts": 1677893022036550000, "dur": 947000, "relative_dur": 0.16302289550697194, "relative_gap_to_previous": 0.00017214666896195557, "parent_is_longest": true, "runtime_str": "947 us"}}}, "id": "IoGqhyFpE8BTiG1N", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.318.pt.trace.json", "trace_disk_size": "3.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 947000, "runtime_str": "947 us", "start_timestamp": "01:23:41.964.964563", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 30}], "resources": {"cpu2": {"time": {"ts": 1677893021962472000, "dur": 2219000, "relative_dur": 0.11075617669079112, "relative_gap_to_previous": 0.00484152732717744, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893022031688000, "dur": 5809000, "relative_dur": 0.2899426004492139, "relative_gap_to_previous": 4.991265285749938e-05, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "id": "hS4NhxGurNKxKK70", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.309.pt.trace.json", "trace_disk_size": "23.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5809000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.962.962472", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 193}, {"idx": 312, "name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu2": {"time": {"ts": 1677893021964710000, "dur": 220000, "relative_dur": 0.010980783628649862, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "220 us"}}, "gpu3": {"time": {"ts": 1677893022037498000, "dur": 99000, "relative_dur": 0.0049413526328924385, "relative_gap_to_previous": 4.991265285749938e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "id": "EueVpEjmVCPYtcRT", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.319.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 220000, "runtime_str": "220 us", "start_timestamp": "01:23:41.964.964710", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 16}, {"idx": 313, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu2": {"time": {"ts": 1677893021964943000, "dur": 216000, "relative_dur": 0.010781133017219864, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "216 us"}}, "gpu3": {"time": {"ts": 1677893022037598000, "dur": 233000, "relative_dur": 0.011629648115797355, "relative_gap_to_previous": 4.991265285749938e-05, "parent_is_longest": true, "runtime_str": "233 us"}}}, "id": "VzMUGbbsvUjtew23", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.320.pt.trace.json", "trace_disk_size": "2.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 233000, "runtime_str": "233 us", "start_timestamp": "01:23:41.964.964943", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}, {"idx": 314, "name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"idx": 315, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu2": {"time": {"ts": 1677893021965283000, "dur": 212000, "relative_dur": 0.028187740991889376, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "212 us"}}, "gpu3": {"time": {"ts": 1677893022037833000, "dur": 3345000, "relative_dur": 0.44475468687674513, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "BP9C5CBvmP6mLeBx", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.322.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3345000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.965.965283", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"idx": 316, "name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu2": {"time": {"ts": 1677893021965646000, "dur": 60000, "relative_dur": 0.007977662544874352, "relative_gap_to_previous": 0.020077117404600452, "parent_is_longest": true, "runtime_str": "60 us"}}, "gpu3": {"time": {"ts": 1677893022041180000, "dur": 320000, "relative_dur": 0.04254753357266321, "relative_gap_to_previous": 0.00026592208482914504, "parent_is_longest": true, "runtime_str": "320 us"}}}, "id": "q8ktFb1EICYoPcyl", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.323.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 320000, "runtime_str": "320 us", "start_timestamp": "01:23:41.965.965646", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"idx": 317, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu2": {"time": {"ts": 1677893021965830000, "dur": 64000, "relative_dur": 0.008509506714532641, "relative_gap_to_previous": 0.016487169259406995, "parent_is_longest": true, "runtime_str": "64 us"}}, "gpu3": {"time": {"ts": 1677893022041503000, "dur": 389000, "relative_dur": 0.05172184549926871, "relative_gap_to_previous": 0.00039888312724371757, "parent_is_longest": true, "runtime_str": "389 us"}}}, "id": "Q1rZmRiuDzfv7WOs", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.324.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 389000, "runtime_str": "389 us", "start_timestamp": "01:23:41.965.965830", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}, {"idx": 318, "name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu2": {"time": {"ts": 1677893021966030000, "dur": 105000, "relative_dur": 0.013960909453530115, "relative_gap_to_previous": 0.018082701768381865, "parent_is_longest": true, "runtime_str": "105 us"}}, "gpu3": {"time": {"ts": 1677893022041893000, "dur": 3360000, "relative_dur": 0.4467491025129637, "relative_gap_to_previous": 0.00013296104241457252, "parent_is_longest": true, "runtime_str": "3 ms"}}}, "id": "ZBfmn4lRbYKzj9Z9", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.325.pt.trace.json", "trace_disk_size": "1.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 3360000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.966.966030", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"idx": 319, "name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu2": {"time": {"ts": 1677893021966263000, "dur": 67000, "relative_dur": 0.00890838984177636, "relative_gap_to_previous": 0.017019013429065283, "parent_is_longest": true, "runtime_str": "67 us"}}, "gpu3": {"time": {"ts": 1677893022045254000, "dur": 100000, "relative_dur": 0.013296104241457253, "relative_gap_to_previous": 0.00013296104241457252, "parent_is_longest": true, "runtime_str": "100 us"}}}, "id": "xKL8FU6ksn8sLaT8", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.326.pt.trace.json", "trace_disk_size": "1.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 100000, "runtime_str": "100 us", "start_timestamp": "01:23:41.966.966263", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 13}], "resources": {"cpu2": {"time": {"ts": 1677893021965283000, "dur": 1047000, "relative_dur": 0.05225854754180185, "relative_gap_to_previous": 0.004891439980034939, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022037833000, "dur": 7521000, "relative_dur": 0.3753930621412528, "relative_gap_to_previous": 9.982530571499875e-05, "parent_is_longest": true, "runtime_str": "8 ms"}}}, "id": "IfVQA4rb3sH8Js8q", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.321.pt.trace.json", "trace_disk_size": "8.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 7521000, "runtime_str": "8 ms", "start_timestamp": "01:23:41.965.965283", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 71}], "resources": {"cpu2": {"time": {"ts": 1677893021959410000, "dur": 6894000, "relative_dur": 0.05719405659672964, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "7 ms"}}, "gpu3": {"time": {"ts": 1677893022025319000, "dur": 20035000, "relative_dur": 0.16621452334138065, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "20 ms"}}}, "id": "erOiow34Ad3BLVO1", "pretty_name": "Layer6", "trace_file": "/results/Transformer/Transformer.296.pt.trace.json", "trace_disk_size": "64.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 20035000, "runtime_str": "20 ms", "start_timestamp": "01:23:41.959.959410", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 535}, {"idx": 320, "name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 604, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 604, "resources": {"cpu2": {"time": {"ts": 1677893021966345000, "dur": 219000, "relative_dur": 0.0018168695089474602, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "219 us"}}, "gpu3": {"time": {"ts": 1677893022045355000, "dur": 229000, "relative_dur": 0.0018998315869815908, "relative_gap_to_previous": 8.29620780341306e-06, "parent_is_longest": true, "runtime_str": "229 us"}}}, "id": "qUkwrh1tgxNQ8xE8", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.327.pt.trace.json", "trace_disk_size": "1.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 229000, "runtime_str": "229 us", "start_timestamp": "01:23:41.966.966345", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 16}], "resources": {"cpu2": {"time": {"ts": 1677893021924541000, "dur": 41858000, "relative_dur": 0.27947067620980665, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "42 ms"}}, "gpu3": {"time": {"ts": 1677893021925047000, "dur": 120537000, "relative_dur": 0.8047818074991988, "relative_gap_to_previous": 6.67663711141972e-06, "parent_is_longest": true, "runtime_str": "121 ms"}}}, "id": "hZlPkDqd3mXoswn7", "pretty_name": "TransformerDecoder", "trace_file": "/results/Transformer/Transformer.140.pt.trace.json", "trace_disk_size": "390.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 120537000, "runtime_str": "121 ms", "start_timestamp": "01:23:41.924.924541", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 3227}, {"idx": 321, "name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 326, in forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 326, "resources": {"cpu2": {"time": {"ts": 1677893021966706000, "dur": 95000, "relative_dur": 0.0006342805255848735, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "95 us"}}, "gpu3": {"time": {"ts": 1677893022045585000, "dur": 26273000, "relative_dur": 0.1754152868283303, "relative_gap_to_previous": 6.67663711141972e-06, "parent_is_longest": true, "runtime_str": "26 ms"}}}, "id": "KyNE2qELBzI1X8EK", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.328.pt.trace.json", "trace_disk_size": "1.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 26273000, "runtime_str": "26 ms", "start_timestamp": "01:23:41.966.966706", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 15}], "resources": {"cpu2": {"time": {"ts": 1677893021867669000, "dur": 44431000, "relative_dur": 0.18754695955357822, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "44 ms"}}, "gpu3": {"time": {"ts": 1677893021922082000, "dur": 149776000, "relative_dur": 0.6322169974589078, "relative_gap_to_previous": 4.22108346770449e-06, "parent_is_longest": true, "runtime_str": "150 ms"}}}, "id": "CZjmG6fnTdCAnYSB", "pretty_name": "Decoder", "trace_file": "/results/Transformer/Transformer.136.pt.trace.json", "trace_disk_size": "450.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 149776000, "runtime_str": "150 ms", "start_timestamp": "01:23:41.867.867669", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 3592}], "is_model_pass": "Forward", "idx": 3, "id": "zR7Q2RhDODhD3aZn", "pretty_name": "Forward", "trace_file": "/results/Transformer/Transformer.10.pt.trace.json", "trace_disk_size": "712.4 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "runtime": 236906000, "runtime_str": "237 ms", "start_timestamp": "01:23:41.836.836082", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5713}, {"name": "Calc Loss", "type": "training loop", "instances": 9, "resources": {"cpu2": {"time": {"ts": 1677893021967152000, "dur": 594000, "relative_dur": 0.0008147164676966292, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "594 us"}}, "gpu3": {"time": {"ts": 1677893022074359000, "dur": 40335000, "relative_dur": 0.05532253994030899, "relative_gap_to_previous": 2.7431530898876404e-06, "parent_is_longest": true, "runtime_str": "40 ms"}}}, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 148, in update_params\n    with hotline.annotate('Calc Loss'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "source_file_num": 148, "ops": [{"name": "aten::to", "type": "generated", "generated_depth": 3, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1677893021967158000, "dur": 0, "relative_dur": 0.0, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "0 us"}, "res_name": "aten::to"}}, "id": "WHh7f71IgpJQRWRt", "pretty_name": "aten::to", "trace_file": "/results/Transformer/Transformer.330.pt.trace.json", "trace_disk_size": "91 Bytes", "runtime": 0, "runtime_str": "0 us", "start_timestamp": "01:23:41.967.967158", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 1}, {"name": "aten::_log_softmax", "type": "generated", "generated_depth": 3, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1677893021967160000, "dur": 54000, "relative_dur": 0.001338787653402752, "relative_gap_to_previous": 4.958472790380563e-05, "parent_is_longest": true, "runtime_str": "54 us"}, "res_name": "aten::_log_softmax"}, "gpu3": {"time": {"ts": 1677893022074359000, "dur": 39089000, "relative_dur": 0.9691087145159291, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "39 ms"}, "res_name": "cunn_SpatialSoftMaxForward  LogSoftMaxForwardEpilogue>(87%) and 1 other\u2026"}}, "id": "dnsTKt8vePRMcQXf", "pretty_name": "aten::_log_softmax", "trace_file": "/results/Transformer/Transformer.331.pt.trace.json", "trace_disk_size": "2.1 kB", "runtime": 39089000, "runtime_str": "39 ms", "start_timestamp": "01:23:41.967.967160", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 11}, {"name": "aten::nll_loss_nd(18%) and 12 others\u2026", "type": "generated", "instances": 52, "id": "Hm3AwDLcIWv2Rs6X", "resources": {"cpu2": {"time": {"ts": 1677893021967221000, "dur": 525000, "relative_dur": 0.013015991074748977, "relative_gap_to_previous": 0.0001735465476633197, "parent_is_longest": true, "runtime_str": "525 us"}, "res_name": "aten::nll_loss_nd(18%) and 12 others\u2026"}, "gpu3": {"time": {"ts": 1677893022113449000, "dur": 1245000, "relative_dur": 0.030866493120119003, "relative_gap_to_previous": 2.4792363951902815e-05, "parent_is_longest": true, "runtime_str": "1 ms"}, "res_name": "reduce_kernel<128 4 ReduceOpfunc_wrapper_tsum_functor::operatorTensorIterator&::{lambda #1}>    4>ReduceOpfunc_wrapper_tsum_functor::operatorTensorIterator&::{lambda #1}>    4>(97%) and 10 others\u2026"}}, "pretty_name": "aten::nll_loss_nd(18%) and 12 others\u2026", "trace_file": "/results/Transformer/Transformer.332.pt.trace.json", "trace_disk_size": "10.4 kB", "runtime": 1245000, "runtime_str": "1 ms", "start_timestamp": "01:23:41.967.967221", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 65}], "idx": 322, "id": "EtZIBrI9GYQtIzFn", "pretty_name": "Calc Loss", "trace_file": "/results/Transformer/Transformer.329.pt.trace.json", "trace_disk_size": "12.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "runtime": 40335000, "runtime_str": "40 ms", "start_timestamp": "01:23:41.967.967152", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 79}, {"name": "Zero Grad", "type": "training loop", "instances": 4, "resources": {"cpu2": {"time": {"ts": 1677893021967751000, "dur": 2936000, "relative_dur": 0.004026948735955056, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}}, "gpu3": {"time": {"ts": 1677893022114696000, "dur": 1479000, "relative_dur": 0.00202856170997191, "relative_gap_to_previous": 2.7431530898876404e-06, "parent_is_longest": true, "runtime_str": "1 ms"}}}, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 154, in update_params\n    with hotline.annotate('Zero Grad'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "source_file_num": 154, "ops": [{"name": "aten::zero_(100%) and 1 other\u2026", "type": "generated", "generated_depth": 2, "instances": 186, "resources": {"cpu2": {"time": {"ts": 1677893021967752000, "dur": 2791000, "relative_dur": 0.9506130790190735, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "3 ms"}, "res_name": "aten::zero_(100%) and 1 other\u2026"}, "gpu3": {"time": {"ts": 1677893022114696000, "dur": 1476000, "relative_dur": 0.502724795640327, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "1 ms"}, "res_name": "vectorized_elementwise_kernel<4 FillFunctor Array<char 1> FillFunctor Array<char 1>"}}, "id": "XzqWmTMJESurWsLK", "pretty_name": "aten::zero_(100%) and 1 other\u2026", "trace_file": "/results/Transformer/Transformer.334.pt.trace.json", "trace_disk_size": "101.9 kB", "runtime": 2791000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.967.967752", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 741}, {"name": "aten::ones_like(82%) and 2 others\u2026", "type": "generated", "generated_depth": 1, "instances": 3, "resources": {"cpu2": {"time": {"ts": 1677893021970626000, "dur": 61000, "relative_dur": 0.02077656675749319, "relative_gap_to_previous": 0.02826975476839237, "parent_is_longest": true, "runtime_str": "61 us"}}, "gpu3": {"time": {"ts": 1677893022116173000, "dur": 2000, "relative_dur": 0.0006811989100817438, "relative_gap_to_previous": 0.0003405994550408719, "parent_is_longest": true, "runtime_str": "2 us"}}}, "ops": [{"name": "aten::zeros", "type": "generated", "generated_depth": 2, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1677893021970626000, "dur": 5000, "relative_dur": 0.08196721311475409, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "5 us"}}}, "ops": [{"name": "aten::empty", "type": "generated", "generated_depth": 3, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1677893021970627000, "dur": 2000, "relative_dur": 0.4, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 us"}, "res_name": "aten::empty"}}, "id": "vasEKLAPBHIhyX7j", "pretty_name": "aten::empty", "trace_file": "/results/Transformer/Transformer.337.pt.trace.json", "trace_disk_size": "94 Bytes", "runtime": 2000, "runtime_str": "2 us", "start_timestamp": "01:23:41.970.970627", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 1}, {"name": "aten::zero_", "type": "generated", "generated_depth": 3, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1677893021970630000, "dur": 1000, "relative_dur": 0.2, "relative_gap_to_previous": 0.2, "parent_is_longest": true, "runtime_str": "1 us"}, "res_name": "aten::zero_"}}, "id": "nmTJ4OPPypeH2qU7", "pretty_name": "aten::zero_", "trace_file": "/results/Transformer/Transformer.338.pt.trace.json", "trace_disk_size": "94 Bytes", "runtime": 1000, "runtime_str": "1 us", "start_timestamp": "01:23:41.970.970630", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 1}], "id": "Q0m5z3YOMTPO4wvk", "pretty_name": "aten::zeros", "trace_file": "/results/Transformer/Transformer.336.pt.trace.json", "trace_disk_size": "280 Bytes", "runtime": 5000, "runtime_str": "5 us", "start_timestamp": "01:23:41.970.970626", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 3}, {"name": "aten::empty", "type": "generated", "generated_depth": 2, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1677893021970637000, "dur": 1000, "relative_dur": 0.01639344262295082, "relative_gap_to_previous": 0.09836065573770492, "parent_is_longest": true, "runtime_str": "1 us"}, "res_name": "aten::empty"}}, "id": "GTR9xxOibfLZ7j6v", "pretty_name": "aten::empty", "trace_file": "/results/Transformer/Transformer.339.pt.trace.json", "trace_disk_size": "94 Bytes", "runtime": 1000, "runtime_str": "1 us", "start_timestamp": "01:23:41.970.970637", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 1}, {"name": "aten::ones_like", "type": "generated", "generated_depth": 2, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1677893021970660000, "dur": 27000, "relative_dur": 0.4426229508196721, "relative_gap_to_previous": 0.36065573770491804, "parent_is_longest": true, "runtime_str": "27 us"}}, "gpu3": {"time": {"ts": 1677893022116173000, "dur": 2000, "relative_dur": 0.03278688524590164, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "2 us"}}}, "ops": [{"name": "aten::empty_like", "type": "generated", "generated_depth": 3, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1677893021970661000, "dur": 9000, "relative_dur": 0.3333333333333333, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "9 us"}, "res_name": "aten::empty_like"}}, "id": "mKHJw46i9pz7G99V", "pretty_name": "aten::empty_like", "trace_file": "/results/Transformer/Transformer.341.pt.trace.json", "trace_disk_size": "200 Bytes", "runtime": 9000, "runtime_str": "9 us", "start_timestamp": "01:23:41.970.970661", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 2}, {"name": "aten::fill_", "type": "generated", "generated_depth": 3, "instances": 1, "resources": {"cpu2": {"time": {"ts": 1677893021970671000, "dur": 15000, "relative_dur": 0.5555555555555556, "relative_gap_to_previous": 0.037037037037037035, "parent_is_longest": true, "runtime_str": "15 us"}, "res_name": "aten::fill_"}, "gpu3": {"time": {"ts": 1677893022116173000, "dur": 2000, "relative_dur": 0.07407407407407407, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "2 us"}, "res_name": "vectorized_elementwise_kernel<4 FillFunctor Array<char 1> FillFunctor Array<char 1>"}}, "id": "Nfz9zlE9XUmYDPeT", "pretty_name": "aten::fill_", "trace_file": "/results/Transformer/Transformer.342.pt.trace.json", "trace_disk_size": "458 Bytes", "runtime": 15000, "runtime_str": "15 us", "start_timestamp": "01:23:41.970.970671", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 3}], "id": "v7AYlCEEEgQbhX7g", "pretty_name": "aten::ones_like", "trace_file": "/results/Transformer/Transformer.340.pt.trace.json", "trace_disk_size": "755 Bytes", "runtime": 27000, "runtime_str": "27 us", "start_timestamp": "01:23:41.970.970660", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 6}], "id": "nKmVSivKM6j7NRkZ", "pretty_name": "aten::ones_like(82%) and 2 others\u2026", "trace_file": "/results/Transformer/Transformer.335.pt.trace.json", "trace_disk_size": "1.1 kB", "runtime": 61000, "runtime_str": "61 us", "start_timestamp": "01:23:41.970.970626", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 10}], "idx": 323, "id": "rmPU0UnkqyW05XBQ", "pretty_name": "Zero Grad", "trace_file": "/results/Transformer/Transformer.333.pt.trace.json", "trace_disk_size": "103.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "runtime": 2936000, "runtime_str": "3 ms", "start_timestamp": "01:23:41.967.967751", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 752}, {"name": "Backward", "type": "training loop", "instances": 1061, "resources": {"cpu2": {"time": {"ts": 1677893022534155000, "dur": 277000, "relative_dur": 0.049206680126404494, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "277 us"}}, "cpu4": {"time": {"ts": 1677893021971155000, "dur": 35876000, "relative_dur": 0.049206680126404494, "relative_gap_to_previous": 0, "parent_is_longest": false, "runtime_str": "36 ms"}}, "gpu3": {"time": {"ts": 1677893022116176000, "dur": 418682000, "relative_dur": 0.5742544109901685, "relative_gap_to_previous": 1.3715765449438202e-06, "parent_is_longest": true, "runtime_str": "419 ms"}}}, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 157, in update_params\n    with hotline.annotate('Backward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "source_file_num": 157, "ops": [{"name": "Decoder", "type": "Decoder", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 212, in forward\n    with hotline.annotate('Decoder'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 212, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 326, in forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 326, "resources": {"cpu4": {"time": {"ts": 1677893021971155000, "dur": 923000, "relative_dur": 0.0033436819625927844, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "923 us"}}, "gpu3": {"time": {"ts": 1677893022116176000, "dur": 67219000, "relative_dur": 0.24350916342743703, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "67 ms"}}}, "is_backward_op": true, "id": "PVvmOry6mnmexqRO", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.345.pt.trace.json", "trace_disk_size": "19.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 67219000, "runtime_str": "67 ms", "start_timestamp": "01:23:41.971.971155", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 135}, {"name": "TransformerDecoder", "type": "TransformerDecoder", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 312, in forward\n    with hotline.annotate('TransformerDecoder'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 312, "ops": [{"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 604, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 604, "resources": {"cpu4": {"time": {"ts": 1677893021972116000, "dur": 84000, "relative_dur": 0.00040576769799289907, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "84 us"}}, "gpu3": {"time": {"ts": 1677893022183396000, "dur": 255000, "relative_dur": 0.0012317947974784436, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "255 us"}}}, "is_backward_op": true, "id": "kqbKPPTGdxdRCS1N", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.347.pt.trace.json", "trace_disk_size": "2.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 255000, "runtime_str": "255 us", "start_timestamp": "01:23:41.972.972116", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 18}, {"name": "Layer6", "type": "Layer6", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu4": {"time": {"ts": 1677893021972210000, "dur": 85000, "relative_dur": 0.006782636450686243, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "85 us"}}, "gpu3": {"time": {"ts": 1677893022183652000, "dur": 96000, "relative_dur": 0.007660389403127992, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "wFi3LGt2UJ8oaaAn", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.350.pt.trace.json", "trace_disk_size": "2.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "01:23:41.972.972210", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu4": {"time": {"ts": 1677893021972300000, "dur": 148000, "relative_dur": 0.011809766996488988, "relative_gap_to_previous": 0.0003989786147462496, "parent_is_longest": true, "runtime_str": "148 us"}}, "gpu3": {"time": {"ts": 1677893022183749000, "dur": 5661000, "relative_dur": 0.4517235876157038, "relative_gap_to_previous": 7.979572294924992e-05, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "mDKOEoIsyUIc2u5g", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.351.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5661000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.972.972300", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu4": {"time": {"ts": 1677893021972455000, "dur": 101000, "relative_dur": 0.008059368017874243, "relative_gap_to_previous": 0.0005585700606447494, "parent_is_longest": true, "runtime_str": "101 us"}}, "gpu3": {"time": {"ts": 1677893022189411000, "dur": 423000, "relative_dur": 0.033753590807532714, "relative_gap_to_previous": 7.979572294924992e-05, "parent_is_longest": true, "runtime_str": "423 us"}}}, "is_backward_op": true, "id": "2KQDCJn6vso6Lot0", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.352.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 423000, "runtime_str": "423 us", "start_timestamp": "01:23:41.972.972455", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu4": {"time": {"ts": 1677893021972560000, "dur": 27000, "relative_dur": 0.002154484519629748, "relative_gap_to_previous": 0.0003191828917969997, "parent_is_longest": true, "runtime_str": "27 us"}}, "gpu3": {"time": {"ts": 1677893022189835000, "dur": 476000, "relative_dur": 0.03798276412384296, "relative_gap_to_previous": 7.979572294924992e-05, "parent_is_longest": true, "runtime_str": "476 us"}}}, "is_backward_op": true, "id": "6NJKnZWDkdTEht2p", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.353.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 476000, "runtime_str": "476 us", "start_timestamp": "01:23:41.972.972560", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu4": {"time": {"ts": 1677893021972592000, "dur": 156000, "relative_dur": 0.012448132780082987, "relative_gap_to_previous": 0.0003989786147462496, "parent_is_longest": true, "runtime_str": "156 us"}}, "gpu3": {"time": {"ts": 1677893022190313000, "dur": 5871000, "relative_dur": 0.4684806894350463, "relative_gap_to_previous": 0.00015959144589849984, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "V6WlsVC9lTiJforV", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.354.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5871000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.972.972592", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1677893021972210000, "dur": 538000, "relative_dur": 0.015776200809336696, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "538 us"}}, "gpu3": {"time": {"ts": 1677893022183652000, "dur": 12532000, "relative_dur": 0.36748577796023696, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "13 ms"}}}, "is_backward_op": true, "id": "sChH1Y9Omo61Zzhb", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.349.pt.trace.json", "trace_disk_size": "17.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12532000, "runtime_str": "13 ms", "start_timestamp": "01:23:41.972.972210", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 134}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu4": {"time": {"ts": 1677893021972756000, "dur": 141000, "relative_dur": 0.004134654858952554, "relative_gap_to_previous": 0.00023459034660723712, "parent_is_longest": true, "runtime_str": "141 us"}}, "gpu3": {"time": {"ts": 1677893022196186000, "dur": 432000, "relative_dur": 0.012667878716790804, "relative_gap_to_previous": 5.864758665180928e-05, "parent_is_longest": true, "runtime_str": "432 us"}}}, "is_backward_op": true, "id": "wHMlqSJ1HKdMcpbq", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.355.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 432000, "runtime_str": "432 us", "start_timestamp": "01:23:41.972.972756", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu4": {"time": {"ts": 1677893021972902000, "dur": 79000, "relative_dur": 0.0023165796727464666, "relative_gap_to_previous": 0.0001466189666295232, "parent_is_longest": true, "runtime_str": "79 us"}}, "gpu3": {"time": {"ts": 1677893022196620000, "dur": 95000, "relative_dur": 0.0027857603659609408, "relative_gap_to_previous": 5.864758665180928e-05, "parent_is_longest": true, "runtime_str": "95 us"}}}, "is_backward_op": true, "id": "nLhgHsKwTHvCKLia", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.356.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 95000, "runtime_str": "95 us", "start_timestamp": "01:23:41.972.972902", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893021972985000, "dur": 145000, "relative_dur": 0.014054473199573519, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "145 us"}}, "gpu3": {"time": {"ts": 1677893022196717000, "dur": 1756000, "relative_dur": 0.17020451681690413, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "FhJWPzsO1lE56vCB", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.358.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1756000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.972.972985", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893021973136000, "dur": 149000, "relative_dur": 0.014442182805078996, "relative_gap_to_previous": 0.0005815644082582146, "parent_is_longest": true, "runtime_str": "149 us"}}, "gpu3": {"time": {"ts": 1677893022198475000, "dur": 808000, "relative_dur": 0.07831734031210623, "relative_gap_to_previous": 0.0001938548027527382, "parent_is_longest": true, "runtime_str": "808 us"}}}, "is_backward_op": true, "id": "xh6rDEGNvTaWwJmw", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.359.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 808000, "runtime_str": "808 us", "start_timestamp": "01:23:41.973.973136", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893021973291000, "dur": 28000, "relative_dur": 0.0027139672385383348, "relative_gap_to_previous": 0.0005815644082582146, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1677893022199285000, "dur": 360000, "relative_dur": 0.03489386449549287, "relative_gap_to_previous": 0.0001938548027527382, "parent_is_longest": true, "runtime_str": "360 us"}}}, "is_backward_op": true, "id": "dQi323D9r25fXwT4", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.360.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "01:23:41.973.973291", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893021973324000, "dur": 42000, "relative_dur": 0.004070950857807502, "relative_gap_to_previous": 0.0004846370068818455, "parent_is_longest": true, "runtime_str": "42 us"}}, "gpu3": {"time": {"ts": 1677893022199646000, "dur": 953000, "relative_dur": 0.09237181351167975, "relative_gap_to_previous": 9.69274013763691e-05, "parent_is_longest": true, "runtime_str": "953 us"}}}, "is_backward_op": true, "id": "lrdzsDgdzqh59Gbg", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.361.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "01:23:41.973.973324", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893021973370000, "dur": 72000, "relative_dur": 0.006978772899098575, "relative_gap_to_previous": 0.0003877096055054764, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1677893022200600000, "dur": 866000, "relative_dur": 0.08393912959193564, "relative_gap_to_previous": 9.69274013763691e-05, "parent_is_longest": true, "runtime_str": "866 us"}}}, "is_backward_op": true, "id": "xgWtHE5zDbB3xt2d", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.362.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 866000, "runtime_str": "866 us", "start_timestamp": "01:23:41.973.973370", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893021973448000, "dur": 36000, "relative_dur": 0.0034893864495492877, "relative_gap_to_previous": 0.0005815644082582146, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1677893022201467000, "dur": 81000, "relative_dur": 0.007851119511485897, "relative_gap_to_previous": 9.69274013763691e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "KWQsFPvheuK7VyIB", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.363.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "01:23:41.973.973448", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893021973489000, "dur": 218000, "relative_dur": 0.021130173500048463, "relative_gap_to_previous": 0.0004846370068818455, "parent_is_longest": true, "runtime_str": "218 us"}}, "gpu3": {"time": {"ts": 1677893022201550000, "dur": 1841000, "relative_dur": 0.17844334593389552, "relative_gap_to_previous": 0.0001938548027527382, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "yPcOE5QU5onxaLp0", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.364.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1841000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.973.973489", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893021973713000, "dur": 182000, "relative_dur": 0.017640787050499176, "relative_gap_to_previous": 0.0005815644082582146, "parent_is_longest": true, "runtime_str": "182 us"}}, "gpu3": {"time": {"ts": 1677893022203393000, "dur": 1857000, "relative_dur": 0.1799941843559174, "relative_gap_to_previous": 0.0001938548027527382, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "arn4C2tKDszca6Lt", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.365.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1857000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.973.973713", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893021973901000, "dur": 173000, "relative_dur": 0.016768440438111855, "relative_gap_to_previous": 0.0005815644082582146, "parent_is_longest": true, "runtime_str": "173 us"}}, "gpu3": {"time": {"ts": 1677893022205252000, "dur": 1782000, "relative_dur": 0.17272462925268975, "relative_gap_to_previous": 0.0001938548027527382, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "HRSaN29y5KD3OKPS", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.366.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1782000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.973.973901", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893021972985000, "dur": 1089000, "relative_dur": 0.031933610931910154, "relative_gap_to_previous": 0.00011729517330361856, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022196717000, "dur": 10317000, "relative_dur": 0.30253357574335815, "relative_gap_to_previous": 5.864758665180928e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "jBQEuqiwBbNglY9Q", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.357.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10317000, "runtime_str": "10 ms", "start_timestamp": "01:23:41.972.972985", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu4": {"time": {"ts": 1677893021974080000, "dur": 149000, "relative_dur": 0.004369245205559791, "relative_gap_to_previous": 0.00017594275995542783, "parent_is_longest": true, "runtime_str": "149 us"}}, "gpu3": {"time": {"ts": 1677893022207036000, "dur": 398000, "relative_dur": 0.011670869743710047, "relative_gap_to_previous": 5.864758665180928e-05, "parent_is_longest": true, "runtime_str": "398 us"}}}, "is_backward_op": true, "id": "oFRUcIo1sDO0jEly", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.367.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 398000, "runtime_str": "398 us", "start_timestamp": "01:23:41.974.974080", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 37}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu4": {"time": {"ts": 1677893021974235000, "dur": 77000, "relative_dur": 0.0022579320860946573, "relative_gap_to_previous": 0.00017594275995542783, "parent_is_longest": true, "runtime_str": "77 us"}}, "gpu3": {"time": {"ts": 1677893022207436000, "dur": 96000, "relative_dur": 0.0028150841592868452, "relative_gap_to_previous": 5.864758665180928e-05, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "8Mf60LN9rmMdqPWV", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.368.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "01:23:41.974.974235", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893021974316000, "dur": 137000, "relative_dur": 0.013406399843428908, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "137 us"}}, "gpu3": {"time": {"ts": 1677893022207535000, "dur": 1726000, "relative_dur": 0.16890106664057147, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "3mDxu9MWdYtfAG7J", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.370.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1726000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.974.974316", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893021974459000, "dur": 140000, "relative_dur": 0.013699970642920051, "relative_gap_to_previous": 0.0005871415989822879, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1677893022209262000, "dur": 798000, "relative_dur": 0.0780898326646443, "relative_gap_to_previous": 9.785693316371466e-05, "parent_is_longest": true, "runtime_str": "798 us"}}}, "is_backward_op": true, "id": "vjGqaFW5PYp5hqqK", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.371.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 798000, "runtime_str": "798 us", "start_timestamp": "01:23:41.974.974459", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893021974604000, "dur": 29000, "relative_dur": 0.0028378510617477247, "relative_gap_to_previous": 0.0004892846658185732, "parent_is_longest": true, "runtime_str": "29 us"}}, "gpu3": {"time": {"ts": 1677893022210061000, "dur": 359000, "relative_dur": 0.03513063900577356, "relative_gap_to_previous": 9.785693316371466e-05, "parent_is_longest": true, "runtime_str": "359 us"}}}, "is_backward_op": true, "id": "P4l3KWkU5A0n89mU", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.372.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "01:23:41.974.974604", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893021974637000, "dur": 38000, "relative_dur": 0.0037185634602211566, "relative_gap_to_previous": 0.0003914277326548586, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1677893022210421000, "dur": 953000, "relative_dur": 0.09325765730502006, "relative_gap_to_previous": 9.785693316371466e-05, "parent_is_longest": true, "runtime_str": "953 us"}}}, "is_backward_op": true, "id": "KGsGLm45qUMHaxLK", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.373.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "01:23:41.974.974637", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893021974680000, "dur": 91000, "relative_dur": 0.008904980917898033, "relative_gap_to_previous": 0.0004892846658185732, "parent_is_longest": true, "runtime_str": "91 us"}}, "gpu3": {"time": {"ts": 1677893022211374000, "dur": 850000, "relative_dur": 0.08317839318915746, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "850 us"}}}, "is_backward_op": true, "id": "i3BLSYkDCrrlvkKo", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.374.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 850000, "runtime_str": "850 us", "start_timestamp": "01:23:41.974.974680", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893021974777000, "dur": 37000, "relative_dur": 0.003620706527057442, "relative_gap_to_previous": 0.0005871415989822879, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1677893022212225000, "dur": 81000, "relative_dur": 0.007926411586260887, "relative_gap_to_previous": 9.785693316371466e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "ccqL7YZDffZMQ0zr", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.375.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "01:23:41.974.974777", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893021974818000, "dur": 215000, "relative_dur": 0.02103924063019865, "relative_gap_to_previous": 0.0003914277326548586, "parent_is_longest": true, "runtime_str": "215 us"}}, "gpu3": {"time": {"ts": 1677893022212307000, "dur": 1811000, "relative_dur": 0.17721890595948722, "relative_gap_to_previous": 9.785693316371466e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "7mf1iOgt7A14QmQT", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.376.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1811000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.974.974818", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893021975040000, "dur": 180000, "relative_dur": 0.017614247969468637, "relative_gap_to_previous": 0.0006849985321460025, "parent_is_longest": true, "runtime_str": "180 us"}}, "gpu3": {"time": {"ts": 1677893022214120000, "dur": 1850000, "relative_dur": 0.1810353263528721, "relative_gap_to_previous": 0.0001957138663274293, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "Qrdsmse6SRWCM8XV", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.377.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1850000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.975.975040", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893021975226000, "dur": 173000, "relative_dur": 0.016929249437322633, "relative_gap_to_previous": 0.0005871415989822879, "parent_is_longest": true, "runtime_str": "173 us"}}, "gpu3": {"time": {"ts": 1677893022215971000, "dur": 1783000, "relative_dur": 0.17447891183090322, "relative_gap_to_previous": 9.785693316371466e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "ftLLLuQAUoRT9gny", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.378.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1783000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.975.975226", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893021974316000, "dur": 1083000, "relative_dur": 0.031757668171954724, "relative_gap_to_previous": 0.00011729517330361856, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022207535000, "dur": 10219000, "relative_dur": 0.2996598439974195, "relative_gap_to_previous": 8.797137997771391e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "BdvBgYudiS4dq79w", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.369.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10219000, "runtime_str": "10 ms", "start_timestamp": "01:23:41.974.974316", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}], "resources": {"cpu4": {"time": {"ts": 1677893021972210000, "dur": 3189000, "relative_dur": 0.015404680820230419, "relative_gap_to_previous": 4.8305678332487985e-05, "parent_is_longest": true, "runtime_str": "3 ms"}}, "gpu3": {"time": {"ts": 1677893022183652000, "dur": 34102000, "relative_dur": 0.16473202424945052, "relative_gap_to_previous": 4.830567833248799e-06, "parent_is_longest": true, "runtime_str": "34 ms"}}}, "is_backward_op": true, "id": "7v3IJZMZ7lv5h4Jp", "pretty_name": "Layer6", "trace_file": "/results/Transformer/Transformer.348.pt.trace.json", "trace_disk_size": "109.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 34102000, "runtime_str": "34 ms", "start_timestamp": "01:23:41.972.972210", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 839}, {"name": "Layer5", "type": "Layer5", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu4": {"time": {"ts": 1677893021975405000, "dur": 259000, "relative_dur": 0.020122756584569964, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "259 us"}}, "gpu3": {"time": {"ts": 1677893022217756000, "dur": 697000, "relative_dur": 0.05415274648434465, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "697 us"}}}, "is_backward_op": true, "id": "7Q1N3dZfx7WPK1Lh", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.381.pt.trace.json", "trace_disk_size": "9.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 697000, "runtime_str": "697 us", "start_timestamp": "01:23:41.975.975405", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 62}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu4": {"time": {"ts": 1677893021975668000, "dur": 140000, "relative_dur": 0.01087716572138917, "relative_gap_to_previous": 0.000310776163468262, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1677893022218455000, "dur": 5557000, "relative_dur": 0.431745785098283, "relative_gap_to_previous": 0.000155388081734131, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "eRzI8pT7iHTi7prZ", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.382.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5557000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.975.975668", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu4": {"time": {"ts": 1677893021975816000, "dur": 99000, "relative_dur": 0.007691710045839484, "relative_gap_to_previous": 0.000621552326936524, "parent_is_longest": true, "runtime_str": "99 us"}}, "gpu3": {"time": {"ts": 1677893022224013000, "dur": 422000, "relative_dur": 0.03278688524590164, "relative_gap_to_previous": 7.76940408670655e-05, "parent_is_longest": true, "runtime_str": "422 us"}}}, "is_backward_op": true, "id": "0AAm7Ojz0HE6ReC1", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.383.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 422000, "runtime_str": "422 us", "start_timestamp": "01:23:41.975.975816", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu4": {"time": {"ts": 1677893021975919000, "dur": 30000, "relative_dur": 0.002330821226011965, "relative_gap_to_previous": 0.000310776163468262, "parent_is_longest": true, "runtime_str": "30 us"}}, "gpu3": {"time": {"ts": 1677893022224436000, "dur": 476000, "relative_dur": 0.036982363452723176, "relative_gap_to_previous": 7.76940408670655e-05, "parent_is_longest": true, "runtime_str": "476 us"}}}, "is_backward_op": true, "id": "NzfNftKGIx86Tx3d", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.384.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 476000, "runtime_str": "476 us", "start_timestamp": "01:23:41.975.975919", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu4": {"time": {"ts": 1677893021975954000, "dur": 146000, "relative_dur": 0.011343329966591563, "relative_gap_to_previous": 0.0003884702043353275, "parent_is_longest": true, "runtime_str": "146 us"}}, "gpu3": {"time": {"ts": 1677893022224914000, "dur": 5713000, "relative_dur": 0.4438660554735452, "relative_gap_to_previous": 0.000155388081734131, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "DTDirM5WN486Jyqe", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.385.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5713000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.975.975954", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1677893021975405000, "dur": 695000, "relative_dur": 0.02024704305774049, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "695 us"}}, "gpu3": {"time": {"ts": 1677893022217756000, "dur": 12871000, "relative_dur": 0.3749635844549321, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "13 ms"}}}, "is_backward_op": true, "id": "3K0zKWYTEwayyNoO", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.380.pt.trace.json", "trace_disk_size": "23.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12871000, "runtime_str": "13 ms", "start_timestamp": "01:23:41.975.975405", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 177}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu4": {"time": {"ts": 1677893021976108000, "dur": 137000, "relative_dur": 0.003991143739439492, "relative_gap_to_previous": 0.00023305948843442288, "parent_is_longest": true, "runtime_str": "137 us"}}, "gpu3": {"time": {"ts": 1677893022230628000, "dur": 433000, "relative_dur": 0.01261434481151314, "relative_gap_to_previous": 2.913243605430286e-05, "parent_is_longest": true, "runtime_str": "433 us"}}}, "is_backward_op": true, "id": "CboYFH4QL5aLtirP", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.386.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 433000, "runtime_str": "433 us", "start_timestamp": "01:23:41.976.976108", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu4": {"time": {"ts": 1677893021976250000, "dur": 78000, "relative_dur": 0.002272330012235623, "relative_gap_to_previous": 0.0001456621802715143, "parent_is_longest": true, "runtime_str": "78 us"}}, "gpu3": {"time": {"ts": 1677893022231062000, "dur": 97000, "relative_dur": 0.0028258462972673773, "relative_gap_to_previous": 2.913243605430286e-05, "parent_is_longest": true, "runtime_str": "97 us"}}}, "is_backward_op": true, "id": "YUxv8Tp9t3bzLaZ6", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.387.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 97000, "runtime_str": "97 us", "start_timestamp": "01:23:41.976.976250", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893021976332000, "dur": 140000, "relative_dur": 0.013789027873534915, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1677893022231160000, "dur": 1712000, "relative_dur": 0.16862011228208412, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "ohm5lwsTDfi1q5un", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.389.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1712000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.976.976332", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893021976478000, "dur": 140000, "relative_dur": 0.013789027873534915, "relative_gap_to_previous": 0.0005909583374372107, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1677893022232873000, "dur": 794000, "relative_dur": 0.07820348665419088, "relative_gap_to_previous": 9.849305623953512e-05, "parent_is_longest": true, "runtime_str": "794 us"}}}, "is_backward_op": true, "id": "hYH3axGJHtwJRIkW", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.390.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 794000, "runtime_str": "794 us", "start_timestamp": "01:23:41.976.976478", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893021976624000, "dur": 29000, "relative_dur": 0.0028562986309465183, "relative_gap_to_previous": 0.0005909583374372107, "parent_is_longest": true, "runtime_str": "29 us"}}, "gpu3": {"time": {"ts": 1677893022233668000, "dur": 360000, "relative_dur": 0.03545750024623264, "relative_gap_to_previous": 9.849305623953512e-05, "parent_is_longest": true, "runtime_str": "360 us"}}}, "is_backward_op": true, "id": "wBCU33yM8UEBuR0k", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.391.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "01:23:41.976.976624", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893021976657000, "dur": 39000, "relative_dur": 0.0038412291933418692, "relative_gap_to_previous": 0.00039397222495814047, "parent_is_longest": true, "runtime_str": "39 us"}}, "gpu3": {"time": {"ts": 1677893022234030000, "dur": 953000, "relative_dur": 0.09386388259627697, "relative_gap_to_previous": 0.00019698611247907023, "parent_is_longest": true, "runtime_str": "953 us"}}}, "is_backward_op": true, "id": "KiyZutZbb0g9Nzc6", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.392.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "01:23:41.976.976657", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893021976700000, "dur": 71000, "relative_dur": 0.006993006993006993, "relative_gap_to_previous": 0.00039397222495814047, "parent_is_longest": true, "runtime_str": "71 us"}}, "gpu3": {"time": {"ts": 1677893022234984000, "dur": 843000, "relative_dur": 0.0830296464099281, "relative_gap_to_previous": 9.849305623953512e-05, "parent_is_longest": true, "runtime_str": "843 us"}}}, "is_backward_op": true, "id": "YEJIwJMUGp2ntGdX", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.393.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 843000, "runtime_str": "843 us", "start_timestamp": "01:23:41.976.976700", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893021976776000, "dur": 36000, "relative_dur": 0.0035457500246232642, "relative_gap_to_previous": 0.0004924652811976756, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1677893022235828000, "dur": 81000, "relative_dur": 0.007977937555402345, "relative_gap_to_previous": 9.849305623953512e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "Wej8CbGlpzjivR6j", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.394.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "01:23:41.976.976776", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893021976816000, "dur": 213000, "relative_dur": 0.02097902097902098, "relative_gap_to_previous": 0.00039397222495814047, "parent_is_longest": true, "runtime_str": "213 us"}}, "gpu3": {"time": {"ts": 1677893022235910000, "dur": 1797000, "relative_dur": 0.1769920220624446, "relative_gap_to_previous": 9.849305623953512e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "n9npgjzLgL1B7Fyd", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.395.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1797000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.976.976816", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893021977034000, "dur": 182000, "relative_dur": 0.01792573623559539, "relative_gap_to_previous": 0.0004924652811976756, "parent_is_longest": true, "runtime_str": "182 us"}}, "gpu3": {"time": {"ts": 1677893022237709000, "dur": 1834000, "relative_dur": 0.18063626514330738, "relative_gap_to_previous": 0.00019698611247907023, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "SZ5hBxCG2E0JH2Cz", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.396.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1834000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.977.977034", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893021977222000, "dur": 173000, "relative_dur": 0.017039298729439576, "relative_gap_to_previous": 0.0005909583374372107, "parent_is_longest": true, "runtime_str": "173 us"}}, "gpu3": {"time": {"ts": 1677893022239545000, "dur": 1768000, "relative_dur": 0.17413572343149808, "relative_gap_to_previous": 0.00019698611247907023, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "OJdpPbCxCeZkXua9", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.397.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1768000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.977.977222", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893021976332000, "dur": 1063000, "relative_dur": 0.03096777952572394, "relative_gap_to_previous": 0.00011652974421721144, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022231160000, "dur": 10153000, "relative_dur": 0.29578162325933693, "relative_gap_to_previous": 2.913243605430286e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "9nqliU9Gb46wJc8U", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.388.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10153000, "runtime_str": "10 ms", "start_timestamp": "01:23:41.976.976332", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu4": {"time": {"ts": 1677893021977401000, "dur": 165000, "relative_dur": 0.004806851948959972, "relative_gap_to_previous": 0.00017479461632581716, "parent_is_longest": true, "runtime_str": "165 us"}}, "gpu3": {"time": {"ts": 1677893022241315000, "dur": 519000, "relative_dur": 0.015119734312183185, "relative_gap_to_previous": 5.826487210860572e-05, "parent_is_longest": true, "runtime_str": "519 us"}}}, "is_backward_op": true, "id": "e4eZgRXXgDgEr6jS", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.398.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 519000, "runtime_str": "519 us", "start_timestamp": "01:23:41.977.977401", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu4": {"time": {"ts": 1677893021977572000, "dur": 77000, "relative_dur": 0.0022431975761813203, "relative_gap_to_previous": 0.00017479461632581716, "parent_is_longest": true, "runtime_str": "77 us"}}, "gpu3": {"time": {"ts": 1677893022241835000, "dur": 97000, "relative_dur": 0.0028258462972673773, "relative_gap_to_previous": 2.913243605430286e-05, "parent_is_longest": true, "runtime_str": "97 us"}}}, "is_backward_op": true, "id": "wvKFWH0mrI2Ttt46", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.399.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 97000, "runtime_str": "97 us", "start_timestamp": "01:23:41.977.977572", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893021977653000, "dur": 160000, "relative_dur": 0.01576510000985319, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "160 us"}}, "gpu3": {"time": {"ts": 1677893022241933000, "dur": 1711000, "relative_dur": 0.16858803823036753, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "QnOesqkGv5TgmTbt", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.401.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1711000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.977.977653", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893021977819000, "dur": 140000, "relative_dur": 0.01379446250862154, "relative_gap_to_previous": 0.0005911912503694946, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1677893022243645000, "dur": 795000, "relative_dur": 0.07833284067395803, "relative_gap_to_previous": 9.853187506158242e-05, "parent_is_longest": true, "runtime_str": "795 us"}}}, "is_backward_op": true, "id": "FBfxDZXZq1QCErGe", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.402.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 795000, "runtime_str": "795 us", "start_timestamp": "01:23:41.977.977819", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893021977964000, "dur": 28000, "relative_dur": 0.0027588925017243077, "relative_gap_to_previous": 0.0004926593753079121, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1677893022244441000, "dur": 359000, "relative_dur": 0.03537294314710809, "relative_gap_to_previous": 9.853187506158242e-05, "parent_is_longest": true, "runtime_str": "359 us"}}}, "is_backward_op": true, "id": "quKdcLeOE7R714kY", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.403.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "01:23:41.977.977964", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893021977997000, "dur": 37000, "relative_dur": 0.0036456793772785497, "relative_gap_to_previous": 0.0004926593753079121, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1677893022244801000, "dur": 952000, "relative_dur": 0.09380234505862646, "relative_gap_to_previous": 9.853187506158242e-05, "parent_is_longest": true, "runtime_str": "952 us"}}}, "is_backward_op": true, "id": "tvDQ10NniQa8R9DM", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.404.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "01:23:41.977.977997", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893021978039000, "dur": 70000, "relative_dur": 0.00689723125431077, "relative_gap_to_previous": 0.0004926593753079121, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1677893022245754000, "dur": 842000, "relative_dur": 0.0829638388018524, "relative_gap_to_previous": 9.853187506158242e-05, "parent_is_longest": true, "runtime_str": "842 us"}}}, "is_backward_op": true, "id": "UqOsmocIQam80PlW", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.405.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 842000, "runtime_str": "842 us", "start_timestamp": "01:23:41.978.978039", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893021978114000, "dur": 36000, "relative_dur": 0.0035471475022169673, "relative_gap_to_previous": 0.0004926593753079121, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1677893022246597000, "dur": 80000, "relative_dur": 0.007882550004926594, "relative_gap_to_previous": 9.853187506158242e-05, "parent_is_longest": true, "runtime_str": "80 us"}}}, "is_backward_op": true, "id": "NPubVaFbBDyVRGOI", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.406.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 80000, "runtime_str": "80 us", "start_timestamp": "01:23:41.978.978114", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893021978154000, "dur": 210000, "relative_dur": 0.020691693762932308, "relative_gap_to_previous": 0.0003941275002463297, "parent_is_longest": true, "runtime_str": "210 us"}}, "gpu3": {"time": {"ts": 1677893022246679000, "dur": 1798000, "relative_dur": 0.1771603113607252, "relative_gap_to_previous": 0.00019706375012316484, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "ThqqsXB2TixGd92n", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.407.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1798000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.978.978154", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893021978369000, "dur": 181000, "relative_dur": 0.017834269386146417, "relative_gap_to_previous": 0.0004926593753079121, "parent_is_longest": true, "runtime_str": "181 us"}}, "gpu3": {"time": {"ts": 1677893022248478000, "dur": 1833000, "relative_dur": 0.18060892698788059, "relative_gap_to_previous": 9.853187506158242e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "JeifbRCg6rmFBMM8", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.408.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1833000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.978.978369", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893021978556000, "dur": 172000, "relative_dur": 0.016947482510592176, "relative_gap_to_previous": 0.0005911912503694946, "parent_is_longest": true, "runtime_str": "172 us"}}, "gpu3": {"time": {"ts": 1677893022250313000, "dur": 1769000, "relative_dur": 0.1743028869839393, "relative_gap_to_previous": 0.00019706375012316484, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "l29gJpNTPiLjHoQL", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.409.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1769000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.978.978556", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893021977653000, "dur": 1075000, "relative_dur": 0.031317368758375576, "relative_gap_to_previous": 0.00011652974421721144, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022241933000, "dur": 10149000, "relative_dur": 0.2956650935151197, "relative_gap_to_previous": 2.913243605430286e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "HABLALpbT076P57L", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.400.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10149000, "runtime_str": "10 ms", "start_timestamp": "01:23:41.977.977653", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}], "resources": {"cpu4": {"time": {"ts": 1677893021975405000, "dur": 3323000, "relative_dur": 0.016051976909885756, "relative_gap_to_previous": 2.898340699949279e-05, "parent_is_longest": true, "runtime_str": "3 ms"}}, "gpu3": {"time": {"ts": 1677893022217756000, "dur": 34326000, "relative_dur": 0.16581407144409827, "relative_gap_to_previous": 9.661135666497597e-06, "parent_is_longest": true, "runtime_str": "34 ms"}}}, "is_backward_op": true, "id": "lpuIbQbYHKH0kbFO", "pretty_name": "Layer5", "trace_file": "/results/Transformer/Transformer.379.pt.trace.json", "trace_disk_size": "116.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 34326000, "runtime_str": "34 ms", "start_timestamp": "01:23:41.975.975405", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 885}, {"name": "Layer4", "type": "Layer4", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu4": {"time": {"ts": 1677893021978734000, "dur": 256000, "relative_dur": 0.01864530225782957, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "256 us"}}, "gpu3": {"time": {"ts": 1677893022252083000, "dur": 696000, "relative_dur": 0.05069191551347414, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "696 us"}}}, "is_backward_op": true, "id": "HLQvZLqitk4ppfYM", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.412.pt.trace.json", "trace_disk_size": "9.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 696000, "runtime_str": "696 us", "start_timestamp": "01:23:41.978.978734", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 62}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu4": {"time": {"ts": 1677893021978995000, "dur": 138000, "relative_dur": 0.010050983248361253, "relative_gap_to_previous": 0.0003641660597232338, "parent_is_longest": true, "runtime_str": "138 us"}}, "gpu3": {"time": {"ts": 1677893022252781000, "dur": 5516000, "relative_dur": 0.4017479970866715, "relative_gap_to_previous": 0.0001456664238892935, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "9TTBVKsB3te7YUDU", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.413.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5516000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.978.978995", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu4": {"time": {"ts": 1677893021979141000, "dur": 96000, "relative_dur": 0.006991988346686089, "relative_gap_to_previous": 0.000582665695557174, "parent_is_longest": true, "runtime_str": "96 us"}}, "gpu3": {"time": {"ts": 1677893022258298000, "dur": 423000, "relative_dur": 0.030808448652585578, "relative_gap_to_previous": 7.283321194464675e-05, "parent_is_longest": true, "runtime_str": "423 us"}}}, "is_backward_op": true, "id": "WuSFnjWNSF690S79", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.414.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 423000, "runtime_str": "423 us", "start_timestamp": "01:23:41.979.979141", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu4": {"time": {"ts": 1677893021979241000, "dur": 23000, "relative_dur": 0.0016751638747268755, "relative_gap_to_previous": 0.000291332847778587, "parent_is_longest": true, "runtime_str": "23 us"}}, "gpu3": {"time": {"ts": 1677893022258723000, "dur": 862000, "relative_dur": 0.06278222869628551, "relative_gap_to_previous": 0.0001456664238892935, "parent_is_longest": true, "runtime_str": "862 us"}}}, "is_backward_op": true, "id": "kz7pjUqVWwSO1e0p", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.415.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 862000, "runtime_str": "862 us", "start_timestamp": "01:23:41.979.979241", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu4": {"time": {"ts": 1677893021979268000, "dur": 164000, "relative_dur": 0.011944646758922069, "relative_gap_to_previous": 0.000291332847778587, "parent_is_longest": true, "runtime_str": "164 us"}}, "gpu3": {"time": {"ts": 1677893022259587000, "dur": 6226000, "relative_dur": 0.4534595775673707, "relative_gap_to_previous": 0.0001456664238892935, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "TUPg4oIASDyzloHF", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.416.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 6226000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.979.979268", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1677893021978734000, "dur": 698000, "relative_dur": 0.019772810968527805, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "698 us"}}, "gpu3": {"time": {"ts": 1677893022252083000, "dur": 13730000, "relative_dur": 0.3889408232061415, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "14 ms"}}}, "is_backward_op": true, "id": "pv3gZdlgz28zHv35", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.411.pt.trace.json", "trace_disk_size": "23.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 13730000, "runtime_str": "14 ms", "start_timestamp": "01:23:41.978.978734", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 177}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu4": {"time": {"ts": 1677893021979439000, "dur": 138000, "relative_dur": 0.003909237698648763, "relative_gap_to_previous": 0.000198294665873488, "parent_is_longest": true, "runtime_str": "138 us"}}, "gpu3": {"time": {"ts": 1677893022265815000, "dur": 439000, "relative_dur": 0.012435908331208747, "relative_gap_to_previous": 5.6655618820996573e-05, "parent_is_longest": true, "runtime_str": "439 us"}}}, "is_backward_op": true, "id": "O4wZd01kuZ6lrz8f", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.417.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 439000, "runtime_str": "439 us", "start_timestamp": "01:23:41.979.979439", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu4": {"time": {"ts": 1677893021979582000, "dur": 78000, "relative_dur": 0.0022095691340188665, "relative_gap_to_previous": 0.00014163904705249142, "parent_is_longest": true, "runtime_str": "78 us"}}, "gpu3": {"time": {"ts": 1677893022266255000, "dur": 97000, "relative_dur": 0.0027477975128183335, "relative_gap_to_previous": 2.8327809410498287e-05, "parent_is_longest": true, "runtime_str": "97 us"}}}, "is_backward_op": true, "id": "vKYcUY4ElLctxb5Z", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.418.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 97000, "runtime_str": "97 us", "start_timestamp": "01:23:41.979.979582", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893021979665000, "dur": 142000, "relative_dur": 0.01388617250146685, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "142 us"}}, "gpu3": {"time": {"ts": 1677893022266353000, "dur": 1725000, "relative_dur": 0.1686876589086642, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "M8vy5ypHdxVJW4h3", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.420.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1725000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.979.979665", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893021979813000, "dur": 143000, "relative_dur": 0.013983962448660278, "relative_gap_to_previous": 0.000586739683160571, "parent_is_longest": true, "runtime_str": "143 us"}}, "gpu3": {"time": {"ts": 1677893022268079000, "dur": 800000, "relative_dur": 0.07823195775474281, "relative_gap_to_previous": 9.778994719342852e-05, "parent_is_longest": true, "runtime_str": "800 us"}}}, "is_backward_op": true, "id": "Mtv8GJbTg0e2gQyT", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.421.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 800000, "runtime_str": "800 us", "start_timestamp": "01:23:41.979.979813", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893021979962000, "dur": 28000, "relative_dur": 0.0027381185214159984, "relative_gap_to_previous": 0.000586739683160571, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1677893022268880000, "dur": 359000, "relative_dur": 0.035106591042440835, "relative_gap_to_previous": 9.778994719342852e-05, "parent_is_longest": true, "runtime_str": "359 us"}}}, "is_backward_op": true, "id": "ZI0zgN4cRpaofMVd", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.422.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "01:23:41.979.979962", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893021979995000, "dur": 37000, "relative_dur": 0.003618228046156855, "relative_gap_to_previous": 0.0004889497359671426, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1677893022269241000, "dur": 950000, "relative_dur": 0.09290044983375709, "relative_gap_to_previous": 0.00019557989438685703, "parent_is_longest": true, "runtime_str": "950 us"}}}, "is_backward_op": true, "id": "jPonufjtoyvc3wF4", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.423.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 950000, "runtime_str": "950 us", "start_timestamp": "01:23:41.979.979995", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893021980037000, "dur": 70000, "relative_dur": 0.006845296303539996, "relative_gap_to_previous": 0.0004889497359671426, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1677893022270193000, "dur": 850000, "relative_dur": 0.08312145511441424, "relative_gap_to_previous": 0.00019557989438685703, "parent_is_longest": true, "runtime_str": "850 us"}}}, "is_backward_op": true, "id": "gY0omtnKXSuOnUqY", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.424.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 850000, "runtime_str": "850 us", "start_timestamp": "01:23:41.980.980037", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893021980113000, "dur": 37000, "relative_dur": 0.003618228046156855, "relative_gap_to_previous": 0.000586739683160571, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1677893022271045000, "dur": 81000, "relative_dur": 0.00792098572266771, "relative_gap_to_previous": 0.00019557989438685703, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "SugcIUoerXmkpdX6", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.425.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "01:23:41.980.980113", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893021980154000, "dur": 215000, "relative_dur": 0.02102483864658713, "relative_gap_to_previous": 0.00039115978877371407, "parent_is_longest": true, "runtime_str": "215 us"}}, "gpu3": {"time": {"ts": 1677893022271128000, "dur": 1812000, "relative_dur": 0.17719538431449247, "relative_gap_to_previous": 0.00019557989438685703, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "yPLhiZJYzetgof4T", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.426.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1812000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.980.980154", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893021980375000, "dur": 186000, "relative_dur": 0.018188930177977704, "relative_gap_to_previous": 0.000586739683160571, "parent_is_longest": true, "runtime_str": "186 us"}}, "gpu3": {"time": {"ts": 1677893022272942000, "dur": 1852000, "relative_dur": 0.18110698220222962, "relative_gap_to_previous": 0.00019557989438685703, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "qA7NF77BY9bFak70", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.427.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1852000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.980.980375", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893021980567000, "dur": 180000, "relative_dur": 0.017602190494817132, "relative_gap_to_previous": 0.000586739683160571, "parent_is_longest": true, "runtime_str": "180 us"}}, "gpu3": {"time": {"ts": 1677893022274795000, "dur": 1784000, "relative_dur": 0.17445726579307647, "relative_gap_to_previous": 9.778994719342852e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "8dq7icwB16JlwEo0", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.428.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1784000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.980.980567", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893021979665000, "dur": 1082000, "relative_dur": 0.030650689782159145, "relative_gap_to_previous": 0.00014163904705249142, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022266353000, "dur": 10226000, "relative_dur": 0.28968017903175547, "relative_gap_to_previous": 2.8327809410498287e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "DDeHO60BL9dN4foO", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.419.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10226000, "runtime_str": "10 ms", "start_timestamp": "01:23:41.979.979665", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu4": {"time": {"ts": 1677893021980753000, "dur": 191000, "relative_dur": 0.005410611597405172, "relative_gap_to_previous": 0.0001699668564629897, "parent_is_longest": true, "runtime_str": "191 us"}}, "gpu3": {"time": {"ts": 1677893022276580000, "dur": 511000, "relative_dur": 0.014475510608764624, "relative_gap_to_previous": 2.8327809410498287e-05, "parent_is_longest": true, "runtime_str": "511 us"}}}, "is_backward_op": true, "id": "dVR90XUVgsn5OPcs", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.429.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 511000, "runtime_str": "511 us", "start_timestamp": "01:23:41.980.980753", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu4": {"time": {"ts": 1677893021980950000, "dur": 77000, "relative_dur": 0.002181241324608368, "relative_gap_to_previous": 0.0001699668564629897, "parent_is_longest": true, "runtime_str": "77 us"}}, "gpu3": {"time": {"ts": 1677893022277092000, "dur": 96000, "relative_dur": 0.0027194697034078354, "relative_gap_to_previous": 2.8327809410498287e-05, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "UpSmemKQPIKDoEiv", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.430.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "01:23:41.980.980950", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893021981031000, "dur": 140000, "relative_dur": 0.01373222167729279, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1677893022277189000, "dur": 1725000, "relative_dur": 0.16920058852378617, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "GpeeGI2cOfUhfKHX", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.432.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1725000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.981.981031", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893021981177000, "dur": 142000, "relative_dur": 0.013928396272682688, "relative_gap_to_previous": 0.0005885237861696911, "parent_is_longest": true, "runtime_str": "142 us"}}, "gpu3": {"time": {"ts": 1677893022278915000, "dur": 797000, "relative_dur": 0.07817557626287396, "relative_gap_to_previous": 9.808729769494851e-05, "parent_is_longest": true, "runtime_str": "797 us"}}}, "is_backward_op": true, "id": "pbgQEN1ZkWUoHqkN", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.433.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 797000, "runtime_str": "797 us", "start_timestamp": "01:23:41.981.981177", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893021981324000, "dur": 28000, "relative_dur": 0.0027464443354585583, "relative_gap_to_previous": 0.0004904364884747426, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1677893022279713000, "dur": 359000, "relative_dur": 0.03521333987248651, "relative_gap_to_previous": 9.808729769494851e-05, "parent_is_longest": true, "runtime_str": "359 us"}}}, "is_backward_op": true, "id": "2fVMr4gogsLqwM0Z", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.434.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "01:23:41.981.981324", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893021981357000, "dur": 37000, "relative_dur": 0.0036292300147130947, "relative_gap_to_previous": 0.0004904364884747426, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1677893022280074000, "dur": 951000, "relative_dur": 0.09328102010789603, "relative_gap_to_previous": 0.00019617459538989702, "parent_is_longest": true, "runtime_str": "951 us"}}}, "is_backward_op": true, "id": "KObWK7HIpxhs7z6I", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.435.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 951000, "runtime_str": "951 us", "start_timestamp": "01:23:41.981.981357", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893021981399000, "dur": 70000, "relative_dur": 0.006866110838646395, "relative_gap_to_previous": 0.0004904364884747426, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1677893022281027000, "dur": 849000, "relative_dur": 0.08327611574301128, "relative_gap_to_previous": 0.00019617459538989702, "parent_is_longest": true, "runtime_str": "849 us"}}}, "is_backward_op": true, "id": "kl5QxKjorwYfFlqL", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.436.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 849000, "runtime_str": "849 us", "start_timestamp": "01:23:41.981.981399", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893021981474000, "dur": 37000, "relative_dur": 0.0036292300147130947, "relative_gap_to_previous": 0.0004904364884747426, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1677893022281877000, "dur": 81000, "relative_dur": 0.00794507111329083, "relative_gap_to_previous": 9.808729769494851e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "8LQ3DrbIXaXOSgl6", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.437.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "01:23:41.981.981474", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893021981516000, "dur": 215000, "relative_dur": 0.021088769004413928, "relative_gap_to_previous": 0.0004904364884747426, "parent_is_longest": true, "runtime_str": "215 us"}}, "gpu3": {"time": {"ts": 1677893022281959000, "dur": 1812000, "relative_dur": 0.17773418342324668, "relative_gap_to_previous": 9.808729769494851e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "4mB8WZaxhuE4KTbj", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.438.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1812000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.981.981516", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893021981737000, "dur": 196000, "relative_dur": 0.019225110348209905, "relative_gap_to_previous": 0.0005885237861696911, "parent_is_longest": true, "runtime_str": "196 us"}}, "gpu3": {"time": {"ts": 1677893022283773000, "dur": 1842000, "relative_dur": 0.18067680235409514, "relative_gap_to_previous": 0.00019617459538989702, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "xNhfrIxMVprGDu5r", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.439.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1842000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.981.981737", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893021981939000, "dur": 176000, "relative_dur": 0.017263364394310938, "relative_gap_to_previous": 0.0005885237861696911, "parent_is_longest": true, "runtime_str": "176 us"}}, "gpu3": {"time": {"ts": 1677893022285616000, "dur": 1768000, "relative_dur": 0.17341834232466896, "relative_gap_to_previous": 9.808729769494851e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "XJ4Vjyt95vWpi990", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.440.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1768000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.981.981939", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893021981031000, "dur": 1084000, "relative_dur": 0.030707345400980142, "relative_gap_to_previous": 0.00011331123764199315, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022277189000, "dur": 10195000, "relative_dur": 0.28880201694003005, "relative_gap_to_previous": 2.8327809410498287e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "mywFixWw30A69AiN", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.431.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10195000, "runtime_str": "10 ms", "start_timestamp": "01:23:41.981.981031", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}], "resources": {"cpu4": {"time": {"ts": 1677893021978734000, "dur": 3381000, "relative_dur": 0.01633214984421419, "relative_gap_to_previous": 2.898340699949279e-05, "parent_is_longest": true, "runtime_str": "3 ms"}}, "gpu3": {"time": {"ts": 1677893022252083000, "dur": 35301000, "relative_dur": 0.17052387508151584, "relative_gap_to_previous": 4.830567833248799e-06, "parent_is_longest": true, "runtime_str": "35 ms"}}}, "is_backward_op": true, "id": "iE813MsLxW5iD4vU", "pretty_name": "Layer4", "trace_file": "/results/Transformer/Transformer.410.pt.trace.json", "trace_disk_size": "116.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 35301000, "runtime_str": "35 ms", "start_timestamp": "01:23:41.978.978734", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 885}, {"name": "Layer3", "type": "Layer3", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu4": {"time": {"ts": 1677893021982121000, "dur": 270000, "relative_dur": 0.021044427123928292, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "270 us"}}, "gpu3": {"time": {"ts": 1677893022287386000, "dur": 702000, "relative_dur": 0.05471551052221356, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "702 us"}}}, "is_backward_op": true, "id": "G8H8gf7Gy0jokNa0", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.443.pt.trace.json", "trace_disk_size": "9.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 702000, "runtime_str": "702 us", "start_timestamp": "01:23:41.982.982121", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 62}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu4": {"time": {"ts": 1677893021982395000, "dur": 147000, "relative_dur": 0.011457521434138737, "relative_gap_to_previous": 0.0003117692907248636, "parent_is_longest": true, "runtime_str": "147 us"}}, "gpu3": {"time": {"ts": 1677893022288089000, "dur": 5513000, "relative_dur": 0.42969602494154324, "relative_gap_to_previous": 7.79423226812159e-05, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "5rlEbZW1qTvfJCIN", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.444.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5513000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.982.982395", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu4": {"time": {"ts": 1677893021982549000, "dur": 106000, "relative_dur": 0.008261886204208885, "relative_gap_to_previous": 0.0005455962587685113, "parent_is_longest": true, "runtime_str": "106 us"}}, "gpu3": {"time": {"ts": 1677893022293603000, "dur": 423000, "relative_dur": 0.03296960249415433, "relative_gap_to_previous": 7.79423226812159e-05, "parent_is_longest": true, "runtime_str": "423 us"}}}, "is_backward_op": true, "id": "sUqdLuIb09NKgnsm", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.445.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 423000, "runtime_str": "423 us", "start_timestamp": "01:23:41.982.982549", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu4": {"time": {"ts": 1677893021982659000, "dur": 24000, "relative_dur": 0.0018706157443491816, "relative_gap_to_previous": 0.0003117692907248636, "parent_is_longest": true, "runtime_str": "24 us"}}, "gpu3": {"time": {"ts": 1677893022294027000, "dur": 476000, "relative_dur": 0.03710054559625877, "relative_gap_to_previous": 7.79423226812159e-05, "parent_is_longest": true, "runtime_str": "476 us"}}}, "is_backward_op": true, "id": "sa0IoBDqUowsUEAg", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.446.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 476000, "runtime_str": "476 us", "start_timestamp": "01:23:41.982.982659", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu4": {"time": {"ts": 1677893021982687000, "dur": 159000, "relative_dur": 0.012392829306313328, "relative_gap_to_previous": 0.0003117692907248636, "parent_is_longest": true, "runtime_str": "159 us"}}, "gpu3": {"time": {"ts": 1677893022294505000, "dur": 5711000, "relative_dur": 0.445128604832424, "relative_gap_to_previous": 0.0001558846453624318, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "97LuNVAMJPP9s5PW", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.447.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5711000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.982.982687", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1677893021982121000, "dur": 725000, "relative_dur": 0.021146890677867226, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "725 us"}}, "gpu3": {"time": {"ts": 1677893022287386000, "dur": 12830000, "relative_dur": 0.3742270446855676, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "13 ms"}}}, "is_backward_op": true, "id": "44C5LSzZksguEz5c", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.442.pt.trace.json", "trace_disk_size": "23.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12830000, "runtime_str": "13 ms", "start_timestamp": "01:23:41.982.982121", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 177}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu4": {"time": {"ts": 1677893021982854000, "dur": 138000, "relative_dur": 0.004025201260063003, "relative_gap_to_previous": 0.0002333450005833625, "parent_is_longest": true, "runtime_str": "138 us"}}, "gpu3": {"time": {"ts": 1677893022300217000, "dur": 433000, "relative_dur": 0.012629798156574496, "relative_gap_to_previous": 2.9168125072920312e-05, "parent_is_longest": true, "runtime_str": "433 us"}}}, "is_backward_op": true, "id": "tGL0XWFkNXNgBSNX", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.448.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 433000, "runtime_str": "433 us", "start_timestamp": "01:23:41.982.982854", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu4": {"time": {"ts": 1677893021982998000, "dur": 78000, "relative_dur": 0.0022751137556877845, "relative_gap_to_previous": 0.00017500875043752187, "parent_is_longest": true, "runtime_str": "78 us"}}, "gpu3": {"time": {"ts": 1677893022300651000, "dur": 96000, "relative_dur": 0.00280014000700035, "relative_gap_to_previous": 2.9168125072920312e-05, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "spIX4IA5AV5Aj1bG", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.449.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "01:23:41.982.982998", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893021983080000, "dur": 154000, "relative_dur": 0.015179891572203055, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "154 us"}}, "gpu3": {"time": {"ts": 1677893022300748000, "dur": 1713000, "relative_dur": 0.1688516510596353, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "Lyg5pEWcQZwayH0r", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.451.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1713000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.983.983080", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893021983241000, "dur": 142000, "relative_dur": 0.013997042878265155, "relative_gap_to_previous": 0.0006899950714637752, "parent_is_longest": true, "runtime_str": "142 us"}}, "gpu3": {"time": {"ts": 1677893022302462000, "dur": 792000, "relative_dur": 0.07806801379990143, "relative_gap_to_previous": 9.857072449482504e-05, "parent_is_longest": true, "runtime_str": "792 us"}}}, "is_backward_op": true, "id": "NsDCcoGQLooysiXY", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.452.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 792000, "runtime_str": "792 us", "start_timestamp": "01:23:41.983.983241", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893021983388000, "dur": 29000, "relative_dur": 0.002858551010349926, "relative_gap_to_previous": 0.0004928536224741252, "parent_is_longest": true, "runtime_str": "29 us"}}, "gpu3": {"time": {"ts": 1677893022303255000, "dur": 360000, "relative_dur": 0.03548546081813701, "relative_gap_to_previous": 9.857072449482504e-05, "parent_is_longest": true, "runtime_str": "360 us"}}}, "is_backward_op": true, "id": "SdMaXiHRXyT2SwHR", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.453.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "01:23:41.983.983388", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893021983421000, "dur": 38000, "relative_dur": 0.0037456875308033515, "relative_gap_to_previous": 0.00039428289797930016, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1677893022303616000, "dur": 950000, "relative_dur": 0.09364218827008379, "relative_gap_to_previous": 9.857072449482504e-05, "parent_is_longest": true, "runtime_str": "950 us"}}}, "is_backward_op": true, "id": "T0brJVPat4EnqFvR", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.454.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 950000, "runtime_str": "950 us", "start_timestamp": "01:23:41.983.983421", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893021983463000, "dur": 71000, "relative_dur": 0.006998521439132577, "relative_gap_to_previous": 0.00039428289797930016, "parent_is_longest": true, "runtime_str": "71 us"}}, "gpu3": {"time": {"ts": 1677893022304568000, "dur": 843000, "relative_dur": 0.08309512074913751, "relative_gap_to_previous": 0.00019714144898965008, "parent_is_longest": true, "runtime_str": "843 us"}}}, "is_backward_op": true, "id": "i54WNNu6LxRX5svC", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.455.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 843000, "runtime_str": "843 us", "start_timestamp": "01:23:41.983.983463", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893021983539000, "dur": 40000, "relative_dur": 0.003942828979793002, "relative_gap_to_previous": 0.0004928536224741252, "parent_is_longest": true, "runtime_str": "40 us"}}, "gpu3": {"time": {"ts": 1677893022305413000, "dur": 81000, "relative_dur": 0.007984228684080828, "relative_gap_to_previous": 0.00019714144898965008, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "sJVUtkFhhafiL1KO", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.456.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "01:23:41.983.983539", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893021983583000, "dur": 230000, "relative_dur": 0.022671266633809757, "relative_gap_to_previous": 0.00039428289797930016, "parent_is_longest": true, "runtime_str": "230 us"}}, "gpu3": {"time": {"ts": 1677893022305495000, "dur": 1796000, "relative_dur": 0.17703302119270575, "relative_gap_to_previous": 9.857072449482504e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "JgeExhKKisZ7VFeN", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.457.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1796000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.983.983583", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893021983819000, "dur": 215000, "relative_dur": 0.021192705766387383, "relative_gap_to_previous": 0.0005914243469689502, "parent_is_longest": true, "runtime_str": "215 us"}}, "gpu3": {"time": {"ts": 1677893022307293000, "dur": 1831000, "relative_dur": 0.18048299655002464, "relative_gap_to_previous": 0.00019714144898965008, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "xDOp2a4soTx3YeOH", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.458.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1831000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.983.983819", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893021984040000, "dur": 178000, "relative_dur": 0.017545588960078858, "relative_gap_to_previous": 0.0005914243469689502, "parent_is_longest": true, "runtime_str": "178 us"}}, "gpu3": {"time": {"ts": 1677893022309126000, "dur": 1767000, "relative_dur": 0.17417447018235585, "relative_gap_to_previous": 0.00019714144898965008, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "rnNrX3oAGaJI1Nii", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.459.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1767000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.984.984040", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893021983080000, "dur": 1138000, "relative_dur": 0.03319332633298332, "relative_gap_to_previous": 0.00011667250029168125, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022300748000, "dur": 10145000, "relative_dur": 0.2959106288647766, "relative_gap_to_previous": 2.9168125072920312e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "SfmUpiX1DqcslcIS", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.450.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10145000, "runtime_str": "10 ms", "start_timestamp": "01:23:41.983.983080", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu4": {"time": {"ts": 1677893021984224000, "dur": 175000, "relative_dur": 0.005104421887761054, "relative_gap_to_previous": 0.00017500875043752187, "parent_is_longest": true, "runtime_str": "175 us"}}, "gpu3": {"time": {"ts": 1677893022310895000, "dur": 520000, "relative_dur": 0.015167425037918562, "relative_gap_to_previous": 5.8336250145840624e-05, "parent_is_longest": true, "runtime_str": "520 us"}}}, "is_backward_op": true, "id": "KvFSMTASuvSujo3C", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.460.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 520000, "runtime_str": "520 us", "start_timestamp": "01:23:41.984.984224", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu4": {"time": {"ts": 1677893021984405000, "dur": 79000, "relative_dur": 0.002304281880760705, "relative_gap_to_previous": 0.00017500875043752187, "parent_is_longest": true, "runtime_str": "79 us"}}, "gpu3": {"time": {"ts": 1677893022311416000, "dur": 98000, "relative_dur": 0.0028584762571461906, "relative_gap_to_previous": 2.9168125072920312e-05, "parent_is_longest": true, "runtime_str": "98 us"}}}, "is_backward_op": true, "id": "G00KlvizZo2CtPk3", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.461.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 98000, "runtime_str": "98 us", "start_timestamp": "01:23:41.984.984405", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893021984488000, "dur": 141000, "relative_dur": 0.013884785819793206, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "141 us"}}, "gpu3": {"time": {"ts": 1677893022311515000, "dur": 1712000, "relative_dur": 0.1685869030034466, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "XGEmtKVlKoxdjTau", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.463.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1712000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.984.984488", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893021984635000, "dur": 142000, "relative_dur": 0.013983259478089611, "relative_gap_to_previous": 0.0005908419497784342, "parent_is_longest": true, "runtime_str": "142 us"}}, "gpu3": {"time": {"ts": 1677893022313229000, "dur": 797000, "relative_dur": 0.07848350566223536, "relative_gap_to_previous": 0.00019694731659281143, "parent_is_longest": true, "runtime_str": "797 us"}}}, "is_backward_op": true, "id": "TIPYP3qHY6TK1rHl", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.464.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 797000, "runtime_str": "797 us", "start_timestamp": "01:23:41.984.984635", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893021984783000, "dur": 38000, "relative_dur": 0.003741999015263417, "relative_gap_to_previous": 0.0005908419497784342, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1677893022314028000, "dur": 360000, "relative_dur": 0.03545051698670606, "relative_gap_to_previous": 0.00019694731659281143, "parent_is_longest": true, "runtime_str": "360 us"}}}, "is_backward_op": true, "id": "akFrS0Rqu4JeCc5h", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.465.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "01:23:41.984.984783", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893021984826000, "dur": 38000, "relative_dur": 0.003741999015263417, "relative_gap_to_previous": 0.0004923682914820286, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1677893022314389000, "dur": 949000, "relative_dur": 0.09345150172328902, "relative_gap_to_previous": 9.847365829640571e-05, "parent_is_longest": true, "runtime_str": "949 us"}}}, "is_backward_op": true, "id": "PhXHs7Qxbi1CtaCx", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.466.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 949000, "runtime_str": "949 us", "start_timestamp": "01:23:41.984.984826", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893021984869000, "dur": 71000, "relative_dur": 0.0069916297390448055, "relative_gap_to_previous": 0.0004923682914820286, "parent_is_longest": true, "runtime_str": "71 us"}}, "gpu3": {"time": {"ts": 1677893022315340000, "dur": 843000, "relative_dur": 0.08301329394387001, "relative_gap_to_previous": 0.00019694731659281143, "parent_is_longest": true, "runtime_str": "843 us"}}}, "is_backward_op": true, "id": "rcPU7LTDpOGSrg3V", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.467.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 843000, "runtime_str": "843 us", "start_timestamp": "01:23:41.984.984869", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893021984946000, "dur": 37000, "relative_dur": 0.0036435253569670114, "relative_gap_to_previous": 0.0005908419497784342, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1677893022316184000, "dur": 80000, "relative_dur": 0.007877892663712457, "relative_gap_to_previous": 9.847365829640571e-05, "parent_is_longest": true, "runtime_str": "80 us"}}}, "is_backward_op": true, "id": "P0uyER20O6vdQDS7", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.468.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 80000, "runtime_str": "80 us", "start_timestamp": "01:23:41.984.984946", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893021984987000, "dur": 227000, "relative_dur": 0.022353520433284098, "relative_gap_to_previous": 0.00039389463318562285, "parent_is_longest": true, "runtime_str": "227 us"}}, "gpu3": {"time": {"ts": 1677893022316265000, "dur": 1797000, "relative_dur": 0.17695716395864106, "relative_gap_to_previous": 9.847365829640571e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "S2prnU8dEnHKCflw", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.469.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1797000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.984.984987", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893021985220000, "dur": 182000, "relative_dur": 0.01792220580994584, "relative_gap_to_previous": 0.0005908419497784342, "parent_is_longest": true, "runtime_str": "182 us"}}, "gpu3": {"time": {"ts": 1677893022318063000, "dur": 1837000, "relative_dur": 0.1808961102904973, "relative_gap_to_previous": 9.847365829640571e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "3gFw5MKdD18telsd", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.470.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1837000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.985.985220", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893021985408000, "dur": 186000, "relative_dur": 0.018316100443131464, "relative_gap_to_previous": 0.0005908419497784342, "parent_is_longest": true, "runtime_str": "186 us"}}, "gpu3": {"time": {"ts": 1677893022319901000, "dur": 1769000, "relative_dur": 0.1741999015263417, "relative_gap_to_previous": 9.847365829640571e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "TpADr8tL1ryVlxDn", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.471.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1769000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.985.985408", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893021984488000, "dur": 1106000, "relative_dur": 0.032259946330649866, "relative_gap_to_previous": 0.00011667250029168125, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022311515000, "dur": 10155000, "relative_dur": 0.2962023101155058, "relative_gap_to_previous": 2.9168125072920312e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "9l2qnnr1eF4OxAL4", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.462.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10155000, "runtime_str": "10 ms", "start_timestamp": "01:23:41.984.984488", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}], "resources": {"cpu4": {"time": {"ts": 1677893021982121000, "dur": 3473000, "relative_dur": 0.016776562084873076, "relative_gap_to_previous": 2.898340699949279e-05, "parent_is_longest": true, "runtime_str": "3 ms"}}, "gpu3": {"time": {"ts": 1677893022287386000, "dur": 34284000, "relative_dur": 0.1656111875951018, "relative_gap_to_previous": 9.661135666497597e-06, "parent_is_longest": true, "runtime_str": "34 ms"}}}, "is_backward_op": true, "id": "H8p2RZpd1MUPe3d0", "pretty_name": "Layer3", "trace_file": "/results/Transformer/Transformer.441.pt.trace.json", "trace_disk_size": "116.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 34284000, "runtime_str": "34 ms", "start_timestamp": "01:23:41.982.982121", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 885}, {"name": "Layer2", "type": "Layer2", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu4": {"time": {"ts": 1677893021985600000, "dur": 261000, "relative_dur": 0.020352464129756706, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "261 us"}}, "gpu3": {"time": {"ts": 1677893022321671000, "dur": 689000, "relative_dur": 0.053727386150966935, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "689 us"}}}, "is_backward_op": true, "id": "kuLHtz5Hd4l3kBUp", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.474.pt.trace.json", "trace_disk_size": "9.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 689000, "runtime_str": "689 us", "start_timestamp": "01:23:41.985.985600", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 62}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu4": {"time": {"ts": 1677893021985865000, "dur": 157000, "relative_dur": 0.012242669993761697, "relative_gap_to_previous": 0.00031191515907673113, "parent_is_longest": true, "runtime_str": "157 us"}}, "gpu3": {"time": {"ts": 1677893022322362000, "dur": 5515000, "relative_dur": 0.43005302557704306, "relative_gap_to_previous": 0.00015595757953836556, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "BLcTeoTBYieCe9H9", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.475.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5515000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.985.985865", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu4": {"time": {"ts": 1677893021986031000, "dur": 100000, "relative_dur": 0.007797878976918278, "relative_gap_to_previous": 0.0007018091079226451, "parent_is_longest": true, "runtime_str": "100 us"}}, "gpu3": {"time": {"ts": 1677893022327879000, "dur": 422000, "relative_dur": 0.032907049282595136, "relative_gap_to_previous": 0.00015595757953836556, "parent_is_longest": true, "runtime_str": "422 us"}}}, "is_backward_op": true, "id": "b8uHnPFQfHXtlswv", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.476.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 422000, "runtime_str": "422 us", "start_timestamp": "01:23:41.986.986031", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu4": {"time": {"ts": 1677893021986135000, "dur": 24000, "relative_dur": 0.0018714909544603868, "relative_gap_to_previous": 0.00031191515907673113, "parent_is_longest": true, "runtime_str": "24 us"}}, "gpu3": {"time": {"ts": 1677893022328303000, "dur": 476000, "relative_dur": 0.03711790393013101, "relative_gap_to_previous": 0.00015595757953836556, "parent_is_longest": true, "runtime_str": "476 us"}}}, "is_backward_op": true, "id": "TxbmAZcjaJnlbSG4", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.477.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 476000, "runtime_str": "476 us", "start_timestamp": "01:23:41.986.986135", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu4": {"time": {"ts": 1677893021986163000, "dur": 149000, "relative_dur": 0.011618839675608235, "relative_gap_to_previous": 0.00031191515907673113, "parent_is_longest": true, "runtime_str": "149 us"}}, "gpu3": {"time": {"ts": 1677893022328781000, "dur": 5714000, "relative_dur": 0.44557080474111044, "relative_gap_to_previous": 0.00015595757953836556, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "TYJJed3hn0gPvITd", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.478.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5714000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.986.986163", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1677893021985600000, "dur": 712000, "relative_dur": 0.020765282314512367, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "712 us"}}, "gpu3": {"time": {"ts": 1677893022321671000, "dur": 12824000, "relative_dur": 0.37400839944003733, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "13 ms"}}}, "is_backward_op": true, "id": "K595NC6c7n6ZGZ8f", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.473.pt.trace.json", "trace_disk_size": "23.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12824000, "runtime_str": "13 ms", "start_timestamp": "01:23:41.985.985600", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 177}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu4": {"time": {"ts": 1677893021986320000, "dur": 144000, "relative_dur": 0.004199720018665422, "relative_gap_to_previous": 0.0002333177788147457, "parent_is_longest": true, "runtime_str": "144 us"}}, "gpu3": {"time": {"ts": 1677893022334497000, "dur": 432000, "relative_dur": 0.012599160055996267, "relative_gap_to_previous": 5.832944470368642e-05, "parent_is_longest": true, "runtime_str": "432 us"}}}, "is_backward_op": true, "id": "y0lD9VMPQYpbH36j", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.479.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 432000, "runtime_str": "432 us", "start_timestamp": "01:23:41.986.986320", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu4": {"time": {"ts": 1677893021986470000, "dur": 83000, "relative_dur": 0.0024206719552029863, "relative_gap_to_previous": 0.00017498833411105927, "parent_is_longest": true, "runtime_str": "83 us"}}, "gpu3": {"time": {"ts": 1677893022334930000, "dur": 97000, "relative_dur": 0.0028289780681287915, "relative_gap_to_previous": 2.916472235184321e-05, "parent_is_longest": true, "runtime_str": "97 us"}}}, "is_backward_op": true, "id": "vHGPxE33lmn39THg", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.480.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 97000, "runtime_str": "97 us", "start_timestamp": "01:23:41.986.986470", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893021986558000, "dur": 144000, "relative_dur": 0.014183000098493057, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "144 us"}}, "gpu3": {"time": {"ts": 1677893022335029000, "dur": 1710000, "relative_dur": 0.16842312616960503, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "2U6k0Rzvj4wFqzCr", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.482.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1710000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.986.986558", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893021986708000, "dur": 145000, "relative_dur": 0.01428149315473259, "relative_gap_to_previous": 0.0005909583374372107, "parent_is_longest": true, "runtime_str": "145 us"}}, "gpu3": {"time": {"ts": 1677893022336741000, "dur": 794000, "relative_dur": 0.07820348665419088, "relative_gap_to_previous": 0.00019698611247907023, "parent_is_longest": true, "runtime_str": "794 us"}}}, "is_backward_op": true, "id": "ZnsBNOOG0zwLOYKE", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.483.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 794000, "runtime_str": "794 us", "start_timestamp": "01:23:41.986.986708", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893021986858000, "dur": 30000, "relative_dur": 0.0029547916871860533, "relative_gap_to_previous": 0.0004924652811976756, "parent_is_longest": true, "runtime_str": "30 us"}}, "gpu3": {"time": {"ts": 1677893022337536000, "dur": 360000, "relative_dur": 0.03545750024623264, "relative_gap_to_previous": 9.849305623953512e-05, "parent_is_longest": true, "runtime_str": "360 us"}}}, "is_backward_op": true, "id": "8r6LQOHXcy0N50Mv", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.484.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "01:23:41.986.986858", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893021986892000, "dur": 49000, "relative_dur": 0.00482615975573722, "relative_gap_to_previous": 0.00039397222495814047, "parent_is_longest": true, "runtime_str": "49 us"}}, "gpu3": {"time": {"ts": 1677893022337898000, "dur": 952000, "relative_dur": 0.09376538954003742, "relative_gap_to_previous": 0.00019698611247907023, "parent_is_longest": true, "runtime_str": "952 us"}}}, "is_backward_op": true, "id": "2yJa1eGWKXMbt4Ad", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.485.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "01:23:41.986.986892", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893021986946000, "dur": 71000, "relative_dur": 0.006993006993006993, "relative_gap_to_previous": 0.0004924652811976756, "parent_is_longest": true, "runtime_str": "71 us"}}, "gpu3": {"time": {"ts": 1677893022338851000, "dur": 843000, "relative_dur": 0.0830296464099281, "relative_gap_to_previous": 9.849305623953512e-05, "parent_is_longest": true, "runtime_str": "843 us"}}}, "is_backward_op": true, "id": "V6lJQy5cRgj0X7Q4", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.486.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 843000, "runtime_str": "843 us", "start_timestamp": "01:23:41.986.986946", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893021987022000, "dur": 38000, "relative_dur": 0.0037427361371023342, "relative_gap_to_previous": 0.0004924652811976756, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1677893022339695000, "dur": 81000, "relative_dur": 0.007977937555402345, "relative_gap_to_previous": 9.849305623953512e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "Bco5RgAufjw7wYsB", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.487.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "01:23:41.987.987022", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893021987064000, "dur": 255000, "relative_dur": 0.025115729341081455, "relative_gap_to_previous": 0.00039397222495814047, "parent_is_longest": true, "runtime_str": "255 us"}}, "gpu3": {"time": {"ts": 1677893022339777000, "dur": 1796000, "relative_dur": 0.17689352900620506, "relative_gap_to_previous": 9.849305623953512e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "V3SyI3JpSD98Kyei", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.488.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1796000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.987.987064", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893021987325000, "dur": 183000, "relative_dur": 0.018024229291834927, "relative_gap_to_previous": 0.0005909583374372107, "parent_is_longest": true, "runtime_str": "183 us"}}, "gpu3": {"time": {"ts": 1677893022341575000, "dur": 1836000, "relative_dur": 0.18083325125578648, "relative_gap_to_previous": 0.00019698611247907023, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "eFQ5gcPV5AUMheQX", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.489.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1836000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.987.987325", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893021987514000, "dur": 187000, "relative_dur": 0.018418201516793065, "relative_gap_to_previous": 0.0005909583374372107, "parent_is_longest": true, "runtime_str": "187 us"}}, "gpu3": {"time": {"ts": 1677893022343413000, "dur": 1769000, "relative_dur": 0.17423421648773763, "relative_gap_to_previous": 0.00019698611247907023, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "h31wsAQ1swwEs5Xf", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.490.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1769000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.987.987514", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893021986558000, "dur": 1143000, "relative_dur": 0.03333527764815679, "relative_gap_to_previous": 0.00014582361175921604, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022335029000, "dur": 10153000, "relative_dur": 0.2961094260382641, "relative_gap_to_previous": 5.832944470368642e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "KNApFpTBeEsSjZ9t", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.481.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10153000, "runtime_str": "10 ms", "start_timestamp": "01:23:41.986.986558", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu4": {"time": {"ts": 1677893021987708000, "dur": 167000, "relative_dur": 0.004870508632757816, "relative_gap_to_previous": 0.00020415305646290246, "parent_is_longest": true, "runtime_str": "167 us"}}, "gpu3": {"time": {"ts": 1677893022345184000, "dur": 511000, "relative_dur": 0.01490317312179188, "relative_gap_to_previous": 5.832944470368642e-05, "parent_is_longest": true, "runtime_str": "511 us"}}}, "is_backward_op": true, "id": "zB5TCP0g18Tehvz3", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.491.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 511000, "runtime_str": "511 us", "start_timestamp": "01:23:41.987.987708", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu4": {"time": {"ts": 1677893021987881000, "dur": 77000, "relative_dur": 0.002245683621091927, "relative_gap_to_previous": 0.00017498833411105927, "parent_is_longest": true, "runtime_str": "77 us"}}, "gpu3": {"time": {"ts": 1677893022345696000, "dur": 96000, "relative_dur": 0.0027998133457769483, "relative_gap_to_previous": 2.916472235184321e-05, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "bmTbj8ZVmDzmvhdx", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.492.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "01:23:41.987.987881", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893021987963000, "dur": 151000, "relative_dur": 0.01485489424495819, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "151 us"}}, "gpu3": {"time": {"ts": 1677893022345794000, "dur": 1716000, "relative_dur": 0.16881455976389573, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "qs9cdI6zUD4g3SoB", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.494.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1716000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.987.987963", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893021988120000, "dur": 143000, "relative_dur": 0.014067879980324643, "relative_gap_to_previous": 0.0005902606984751599, "parent_is_longest": true, "runtime_str": "143 us"}}, "gpu3": {"time": {"ts": 1677893022347511000, "dur": 793000, "relative_dur": 0.0780127889818003, "relative_gap_to_previous": 9.837678307919331e-05, "parent_is_longest": true, "runtime_str": "793 us"}}}, "is_backward_op": true, "id": "kIGbRCPwV22YzQMx", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.495.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 793000, "runtime_str": "793 us", "start_timestamp": "01:23:41.988.988120", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893021988269000, "dur": 29000, "relative_dur": 0.002852926709296606, "relative_gap_to_previous": 0.0005902606984751599, "parent_is_longest": true, "runtime_str": "29 us"}}, "gpu3": {"time": {"ts": 1677893022348305000, "dur": 359000, "relative_dur": 0.035317265125430396, "relative_gap_to_previous": 9.837678307919331e-05, "parent_is_longest": true, "runtime_str": "359 us"}}}, "is_backward_op": true, "id": "EWshZyvnG63Ft9Sy", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.496.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "01:23:41.988.988269", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893021988302000, "dur": 38000, "relative_dur": 0.003738317757009346, "relative_gap_to_previous": 0.00039350713231677323, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1677893022348665000, "dur": 952000, "relative_dur": 0.09365469749139203, "relative_gap_to_previous": 9.837678307919331e-05, "parent_is_longest": true, "runtime_str": "952 us"}}}, "is_backward_op": true, "id": "6gjFdbtRPnaFtCNe", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.497.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "01:23:41.988.988302", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893021988344000, "dur": 72000, "relative_dur": 0.0070831283817019185, "relative_gap_to_previous": 0.00039350713231677323, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1677893022349619000, "dur": 843000, "relative_dur": 0.08293162813575997, "relative_gap_to_previous": 0.00019675356615838662, "parent_is_longest": true, "runtime_str": "843 us"}}}, "is_backward_op": true, "id": "kt8bo6w4Vg1seyFe", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.498.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 843000, "runtime_str": "843 us", "start_timestamp": "01:23:41.988.988344", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893021988422000, "dur": 36000, "relative_dur": 0.0035415641908509593, "relative_gap_to_previous": 0.0005902606984751599, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1677893022350463000, "dur": 81000, "relative_dur": 0.007968519429414659, "relative_gap_to_previous": 9.837678307919331e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "zE32DFPTaZD4M5mV", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.499.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "01:23:41.988.988422", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893021988462000, "dur": 228000, "relative_dur": 0.022429906542056073, "relative_gap_to_previous": 0.00039350713231677323, "parent_is_longest": true, "runtime_str": "228 us"}}, "gpu3": {"time": {"ts": 1677893022350545000, "dur": 1803000, "relative_dur": 0.17737333989178553, "relative_gap_to_previous": 9.837678307919331e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "HNSJ9k3pGWxt8YuW", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.500.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1803000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.988.988462", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893021988696000, "dur": 193000, "relative_dur": 0.01898671913428431, "relative_gap_to_previous": 0.0005902606984751599, "parent_is_longest": true, "runtime_str": "193 us"}}, "gpu3": {"time": {"ts": 1677893022352349000, "dur": 1838000, "relative_dur": 0.1808165272995573, "relative_gap_to_previous": 9.837678307919331e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "ocWJ9UBpbdgaAaOt", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.501.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1838000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.988.988696", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893021988895000, "dur": 178000, "relative_dur": 0.01751106738809641, "relative_gap_to_previous": 0.0005902606984751599, "parent_is_longest": true, "runtime_str": "178 us"}}, "gpu3": {"time": {"ts": 1677893022354188000, "dur": 1771000, "relative_dur": 0.17422528283325137, "relative_gap_to_previous": 9.837678307919331e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "x4Gx0xF1psQDvyXG", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.502.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1771000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.988.988895", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893021987963000, "dur": 1110000, "relative_dur": 0.03237284181054596, "relative_gap_to_previous": 0.00014582361175921604, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022345794000, "dur": 10165000, "relative_dur": 0.29645940270648624, "relative_gap_to_previous": 5.832944470368642e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "DW5BfreYOAVVEQWF", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.493.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10165000, "runtime_str": "10 ms", "start_timestamp": "01:23:41.987.987963", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}], "resources": {"cpu4": {"time": {"ts": 1677893021985600000, "dur": 3473000, "relative_dur": 0.016776562084873076, "relative_gap_to_previous": 2.898340699949279e-05, "parent_is_longest": true, "runtime_str": "3 ms"}}, "gpu3": {"time": {"ts": 1677893022321671000, "dur": 34288000, "relative_dur": 0.1656305098664348, "relative_gap_to_previous": 4.830567833248799e-06, "parent_is_longest": true, "runtime_str": "34 ms"}}}, "is_backward_op": true, "id": "1ax6Xi9TxGT4yIfL", "pretty_name": "Layer2", "trace_file": "/results/Transformer/Transformer.472.pt.trace.json", "trace_disk_size": "116.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 34288000, "runtime_str": "34 ms", "start_timestamp": "01:23:41.985.985600", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 885}, {"name": "Layer1", "type": "Layer1", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 592, in forward\n    with hotline.annotate(f'Layer{idx+1}'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 592, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 783, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 783, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 792, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 792, "resources": {"cpu4": {"time": {"ts": 1677893021989079000, "dur": 277000, "relative_dur": 0.021551388780829378, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "277 us"}}, "gpu3": {"time": {"ts": 1677893022355961000, "dur": 699000, "relative_dur": 0.054384190461370885, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "699 us"}}}, "is_backward_op": true, "id": "qEX3Z7RtgPs6AnIm", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.505.pt.trace.json", "trace_disk_size": "9.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 699000, "runtime_str": "699 us", "start_timestamp": "01:23:41.989.989079", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 62}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 790, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 790, "resources": {"cpu4": {"time": {"ts": 1677893021989361000, "dur": 143000, "relative_dur": 0.011125807204543686, "relative_gap_to_previous": 0.0003890142379211079, "parent_is_longest": true, "runtime_str": "143 us"}}, "gpu3": {"time": {"ts": 1677893022356661000, "dur": 5511000, "relative_dur": 0.42877149303664513, "relative_gap_to_previous": 7.780284758422158e-05, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "VQzEDigGPCv4gWIm", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.506.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5511000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.989.989361", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 788, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 788, "resources": {"cpu4": {"time": {"ts": 1677893021989512000, "dur": 103000, "relative_dur": 0.008013693301174823, "relative_gap_to_previous": 0.0006224227806737727, "parent_is_longest": true, "runtime_str": "103 us"}}, "gpu3": {"time": {"ts": 1677893022362174000, "dur": 423000, "relative_dur": 0.03291060452812573, "relative_gap_to_previous": 0.00015560569516844317, "parent_is_longest": true, "runtime_str": "423 us"}}}, "is_backward_op": true, "id": "KR1k8jnb94s2xUKi", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.507.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 423000, "runtime_str": "423 us", "start_timestamp": "01:23:41.989.989512", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 786, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 786, "resources": {"cpu4": {"time": {"ts": 1677893021989619000, "dur": 24000, "relative_dur": 0.001867268342021318, "relative_gap_to_previous": 0.00031121139033688633, "parent_is_longest": true, "runtime_str": "24 us"}}, "gpu3": {"time": {"ts": 1677893022362598000, "dur": 476000, "relative_dur": 0.03703415545008947, "relative_gap_to_previous": 7.780284758422158e-05, "parent_is_longest": true, "runtime_str": "476 us"}}}, "is_backward_op": true, "id": "Bd1yZ088cExa5rd1", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.508.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 476000, "runtime_str": "476 us", "start_timestamp": "01:23:41.989.989619", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 729, in forward\n    x = x + self._ff_block(x)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 784, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 784, "resources": {"cpu4": {"time": {"ts": 1677893021989647000, "dur": 160000, "relative_dur": 0.012448455613475453, "relative_gap_to_previous": 0.00031121139033688633, "parent_is_longest": true, "runtime_str": "160 us"}}, "gpu3": {"time": {"ts": 1677893022363076000, "dur": 5738000, "relative_dur": 0.44643273943826345, "relative_gap_to_previous": 0.00015560569516844317, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "SF7dVgfum6OmUXv8", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.509.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5738000, "runtime_str": "6 ms", "start_timestamp": "01:23:41.989.989647", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1677893021989079000, "dur": 728000, "relative_dur": 0.021132075471698115, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "728 us"}}, "gpu3": {"time": {"ts": 1677893022355961000, "dur": 12853000, "relative_dur": 0.3730914368650218, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "13 ms"}}}, "is_backward_op": true, "id": "dLESjz4Lph8K8GUe", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.504.pt.trace.json", "trace_disk_size": "23.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12853000, "runtime_str": "13 ms", "start_timestamp": "01:23:41.989.989079", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 177}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 727, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 727, "resources": {"cpu4": {"time": {"ts": 1677893021989815000, "dur": 139000, "relative_dur": 0.004034833091436865, "relative_gap_to_previous": 0.00023222060957910014, "parent_is_longest": true, "runtime_str": "139 us"}}, "gpu3": {"time": {"ts": 1677893022368816000, "dur": 448000, "relative_dur": 0.013004354136429608, "relative_gap_to_previous": 5.8055152394775034e-05, "parent_is_longest": true, "runtime_str": "448 us"}}}, "is_backward_op": true, "id": "7pRlQ7im86CVWXI2", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.510.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 448000, "runtime_str": "448 us", "start_timestamp": "01:23:41.989.989815", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 776, in _mha_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 776, "resources": {"cpu4": {"time": {"ts": 1677893021989959000, "dur": 77000, "relative_dur": 0.002235123367198839, "relative_gap_to_previous": 0.00014513788098693758, "parent_is_longest": true, "runtime_str": "77 us"}}, "gpu3": {"time": {"ts": 1677893022369265000, "dur": 96000, "relative_dur": 0.002786647314949202, "relative_gap_to_previous": 2.9027576197387517e-05, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "isolDVKgveN1VIi4", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.511.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "01:23:41.989.989959", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Multihead-Attention", "type": "Multihead-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 771, in _mha_block\n    with hotline.annotate('Multihead-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 771, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893021990040000, "dur": 149000, "relative_dur": 0.014539422326307573, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "149 us"}}, "gpu3": {"time": {"ts": 1677893022369362000, "dur": 1731000, "relative_dur": 0.16891100702576112, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "ldDyDj0G0iQsXO3w", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.513.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1731000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.990.990040", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893021990196000, "dur": 164000, "relative_dur": 0.01600312256049961, "relative_gap_to_previous": 0.0006830601092896175, "parent_is_longest": true, "runtime_str": "164 us"}}, "gpu3": {"time": {"ts": 1677893022371095000, "dur": 799000, "relative_dur": 0.0779664324746292, "relative_gap_to_previous": 0.000195160031225605, "parent_is_longest": true, "runtime_str": "799 us"}}}, "is_backward_op": true, "id": "mlNd4zSys1aBo4gS", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.514.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 799000, "runtime_str": "799 us", "start_timestamp": "01:23:41.990.990196", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893021990366000, "dur": 29000, "relative_dur": 0.0028298204527712725, "relative_gap_to_previous": 0.000585480093676815, "parent_is_longest": true, "runtime_str": "29 us"}}, "gpu3": {"time": {"ts": 1677893022371895000, "dur": 359000, "relative_dur": 0.035031225604996094, "relative_gap_to_previous": 9.75800156128025e-05, "parent_is_longest": true, "runtime_str": "359 us"}}}, "is_backward_op": true, "id": "kxb3qO1yfQqrYsIP", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.515.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "01:23:41.990.990366", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893021990399000, "dur": 37000, "relative_dur": 0.0036104605776736925, "relative_gap_to_previous": 0.00039032006245121, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1677893022372255000, "dur": 952000, "relative_dur": 0.09289617486338798, "relative_gap_to_previous": 9.75800156128025e-05, "parent_is_longest": true, "runtime_str": "952 us"}}}, "is_backward_op": true, "id": "yUAHrc2KUScMGRrW", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.516.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "01:23:41.990.990399", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893021990441000, "dur": 71000, "relative_dur": 0.006928181108508977, "relative_gap_to_previous": 0.0004879000780640125, "parent_is_longest": true, "runtime_str": "71 us"}}, "gpu3": {"time": {"ts": 1677893022373208000, "dur": 851000, "relative_dur": 0.08304059328649492, "relative_gap_to_previous": 9.75800156128025e-05, "parent_is_longest": true, "runtime_str": "851 us"}}}, "is_backward_op": true, "id": "ax9BZNXKS4wlI2j1", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.517.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 851000, "runtime_str": "851 us", "start_timestamp": "01:23:41.990.990441", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893021990517000, "dur": 37000, "relative_dur": 0.0036104605776736925, "relative_gap_to_previous": 0.0004879000780640125, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1677893022374061000, "dur": 80000, "relative_dur": 0.0078064012490242, "relative_gap_to_previous": 0.000195160031225605, "parent_is_longest": true, "runtime_str": "80 us"}}}, "is_backward_op": true, "id": "LwviOmOvbhHuSTb8", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.518.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 80000, "runtime_str": "80 us", "start_timestamp": "01:23:41.990.990517", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893021990559000, "dur": 227000, "relative_dur": 0.022150663544106167, "relative_gap_to_previous": 0.0004879000780640125, "parent_is_longest": true, "runtime_str": "227 us"}}, "gpu3": {"time": {"ts": 1677893022374143000, "dur": 1816000, "relative_dur": 0.17720530835284934, "relative_gap_to_previous": 0.000195160031225605, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "MyPxljCIzVwxLugV", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.519.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1816000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.990.990559", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893021990792000, "dur": 211000, "relative_dur": 0.020589383294301326, "relative_gap_to_previous": 0.000585480093676815, "parent_is_longest": true, "runtime_str": "211 us"}}, "gpu3": {"time": {"ts": 1677893022375961000, "dur": 1859000, "relative_dur": 0.18140124902419985, "relative_gap_to_previous": 0.000195160031225605, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "wqjtgCpyyYUOVrc3", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.520.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1859000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.990.990792", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 726, in forward\n    x = x + self._mha_block(x, memory, memory_mask, None)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 772, in _mha_block\n    x = self.multihead_attn(x, mem, mem,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893021991009000, "dur": 179000, "relative_dur": 0.017466822794691646, "relative_gap_to_previous": 0.000585480093676815, "parent_is_longest": true, "runtime_str": "179 us"}}, "gpu3": {"time": {"ts": 1677893022377821000, "dur": 1789000, "relative_dur": 0.17457064793130367, "relative_gap_to_previous": 9.75800156128025e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "Xxxb6vLbxKhDUOYm", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.521.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1789000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.991.991009", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893021990040000, "dur": 1148000, "relative_dur": 0.03332365747460087, "relative_gap_to_previous": 0.00011611030478955007, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022369362000, "dur": 10248000, "relative_dur": 0.2974746008708273, "relative_gap_to_previous": 2.9027576197387517e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "Kr74BPdPId56mez6", "pretty_name": "Multihead-Attention", "trace_file": "/results/Transformer/Transformer.512.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10248000, "runtime_str": "10 ms", "start_timestamp": "01:23:41.990.990040", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 724, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 724, "resources": {"cpu4": {"time": {"ts": 1677893021991194000, "dur": 168000, "relative_dur": 0.004876632801161103, "relative_gap_to_previous": 0.00017416545718432512, "parent_is_longest": true, "runtime_str": "168 us"}}, "gpu3": {"time": {"ts": 1677893022379611000, "dur": 511000, "relative_dur": 0.014833091436865021, "relative_gap_to_previous": 2.9027576197387517e-05, "parent_is_longest": true, "runtime_str": "511 us"}}}, "is_backward_op": true, "id": "R3mDK5hx9usekzCT", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.522.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 511000, "runtime_str": "511 us", "start_timestamp": "01:23:41.991.991194", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 764, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 764, "resources": {"cpu4": {"time": {"ts": 1677893021991368000, "dur": 89000, "relative_dur": 0.002583454281567489, "relative_gap_to_previous": 0.00017416545718432512, "parent_is_longest": true, "runtime_str": "89 us"}}, "gpu3": {"time": {"ts": 1677893022380123000, "dur": 96000, "relative_dur": 0.002786647314949202, "relative_gap_to_previous": 2.9027576197387517e-05, "parent_is_longest": true, "runtime_str": "96 us"}}}, "is_backward_op": true, "id": "foHRlQjB2AZrL2EY", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.523.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 96000, "runtime_str": "96 us", "start_timestamp": "01:23:41.991.991368", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 753, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 753, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893021991461000, "dur": 141000, "relative_dur": 0.0138357374153665, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "141 us"}}, "gpu3": {"time": {"ts": 1677893022380220000, "dur": 1729000, "relative_dur": 0.1696595034834658, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "7CSYKwIomkTgMwEB", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.525.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1729000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.991.991461", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893021991608000, "dur": 143000, "relative_dur": 0.014031989009910706, "relative_gap_to_previous": 0.0005887547836326171, "parent_is_longest": true, "runtime_str": "143 us"}}, "gpu3": {"time": {"ts": 1677893022381951000, "dur": 801000, "relative_dur": 0.07859876361495437, "relative_gap_to_previous": 0.00019625159454420566, "parent_is_longest": true, "runtime_str": "801 us"}}}, "is_backward_op": true, "id": "JkHJxDCsKiU7CkES", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.526.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 801000, "runtime_str": "801 us", "start_timestamp": "01:23:41.991.991608", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893021991756000, "dur": 28000, "relative_dur": 0.0027475223236188796, "relative_gap_to_previous": 0.0004906289863605142, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1677893022382753000, "dur": 359000, "relative_dur": 0.035227161220684915, "relative_gap_to_previous": 9.812579727210283e-05, "parent_is_longest": true, "runtime_str": "359 us"}}}, "is_backward_op": true, "id": "Pg6Nm3ApNCdwQQrI", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.527.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "01:23:41.991.991756", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893021991788000, "dur": 48000, "relative_dur": 0.0047100382690609364, "relative_gap_to_previous": 0.0003925031890884113, "parent_is_longest": true, "runtime_str": "48 us"}}, "gpu3": {"time": {"ts": 1677893022383114000, "dur": 953000, "relative_dur": 0.093513884800314, "relative_gap_to_previous": 0.00019625159454420566, "parent_is_longest": true, "runtime_str": "953 us"}}}, "is_backward_op": true, "id": "IWppgWQfc6ml28ki", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.528.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 953000, "runtime_str": "953 us", "start_timestamp": "01:23:41.991.991788", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893021991841000, "dur": 72000, "relative_dur": 0.007065057403591404, "relative_gap_to_previous": 0.0004906289863605142, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1677893022384068000, "dur": 844000, "relative_dur": 0.0828181728976548, "relative_gap_to_previous": 9.812579727210283e-05, "parent_is_longest": true, "runtime_str": "844 us"}}}, "is_backward_op": true, "id": "IxPjGRVefcGTtJIQ", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.529.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 844000, "runtime_str": "844 us", "start_timestamp": "01:23:41.991.991841", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893021991918000, "dur": 38000, "relative_dur": 0.0037287802963399076, "relative_gap_to_previous": 0.0004906289863605142, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1677893022384914000, "dur": 81000, "relative_dur": 0.00794818957904033, "relative_gap_to_previous": 0.00019625159454420566, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "k1U2JSppjJ2BUUXf", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.530.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "01:23:41.991.991918", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893021991960000, "dur": 226000, "relative_dur": 0.02217643018349524, "relative_gap_to_previous": 0.0003925031890884113, "parent_is_longest": true, "runtime_str": "226 us"}}, "gpu3": {"time": {"ts": 1677893022384996000, "dur": 1802000, "relative_dur": 0.17682268668432932, "relative_gap_to_previous": 9.812579727210283e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "dWlCjMSzYoZYU5VU", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.531.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1802000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.991.991960", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893021992193000, "dur": 185000, "relative_dur": 0.018153272495339025, "relative_gap_to_previous": 0.0006868805809047199, "parent_is_longest": true, "runtime_str": "185 us"}}, "gpu3": {"time": {"ts": 1677893022386799000, "dur": 1839000, "relative_dur": 0.18045334118339712, "relative_gap_to_previous": 9.812579727210283e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "OLbMLA8sxFBzR6Mz", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.532.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1839000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.992.992193", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 315, in forward\n    output = self.decoder(  # ret float32(32, 256, 1024)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 593, in forward\n    output, cache = mod(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 716, in forward\n    sa_out, cache = self._sa_block(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 754, in _sa_block\n    x, _, cache = self.self_attn(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893021992384000, "dur": 187000, "relative_dur": 0.018349524089883232, "relative_gap_to_previous": 0.0005887547836326171, "parent_is_longest": true, "runtime_str": "187 us"}}, "gpu3": {"time": {"ts": 1677893022388639000, "dur": 1772000, "relative_dur": 0.17387891276616624, "relative_gap_to_previous": 9.812579727210283e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "DpZjMtOMYGigtWJq", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.533.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1772000, "runtime_str": "2 ms", "start_timestamp": "01:23:41.992.992384", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893021991461000, "dur": 1110000, "relative_dur": 0.03222060957910015, "relative_gap_to_previous": 0.00011611030478955007, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022380220000, "dur": 10191000, "relative_dur": 0.2958200290275762, "relative_gap_to_previous": 2.9027576197387517e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "TzJx155EgCbRXm90", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.524.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10191000, "runtime_str": "10 ms", "start_timestamp": "01:23:41.991.991461", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}], "resources": {"cpu4": {"time": {"ts": 1677893021989079000, "dur": 3492000, "relative_dur": 0.016868342873704804, "relative_gap_to_previous": 2.898340699949279e-05, "parent_is_longest": true, "runtime_str": "3 ms"}}, "gpu3": {"time": {"ts": 1677893022355961000, "dur": 34450000, "relative_dur": 0.1664130618554211, "relative_gap_to_previous": 9.661135666497597e-06, "parent_is_longest": true, "runtime_str": "34 ms"}}}, "is_backward_op": true, "id": "gvreXbu61eSxu2S6", "pretty_name": "Layer1", "trace_file": "/results/Transformer/Transformer.503.pt.trace.json", "trace_disk_size": "116.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 34450000, "runtime_str": "34 ms", "start_timestamp": "01:23:41.989.989079", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 885}], "resources": {"cpu4": {"time": {"ts": 1677893021972116000, "dur": 20455000, "relative_dur": 0.07410077415475125, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "20 ms"}}, "gpu3": {"time": {"ts": 1677893022183396000, "dur": 207015000, "relative_dur": 0.749937509735802, "relative_gap_to_previous": 3.6226240114764728e-06, "parent_is_longest": true, "runtime_str": "207 ms"}}}, "is_backward_op": true, "id": "6FflDRfCoIawEenv", "pretty_name": "TransformerDecoder", "trace_file": "/results/Transformer/Transformer.346.pt.trace.json", "trace_disk_size": "694.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 207015000, "runtime_str": "207 ms", "start_timestamp": "01:23:41.972.972116", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5282}, {"name": "PositionalEncoding", "type": "PositionalEncoding", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 310, in forward\n    with hotline.annotate('PositionalEncoding'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 310, "ops": [{"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 311, in forward\n    tgt = self.pos_encoder(tgt, targets_positions, decode=decode, cache=cache) # ret float32(32, 256)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 372, in forward\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 372, "resources": {"cpu4": {"time": {"ts": 1677893021992577000, "dur": 265000, "relative_dur": 0.3818443804034582, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "265 us"}}, "gpu3": {"time": {"ts": 1677893022390412000, "dur": 694000, "relative_dur": 1.0, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "694 us"}}}, "is_backward_op": true, "id": "AXMhmqeMZOjbIT3q", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.535.pt.trace.json", "trace_disk_size": "8.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 694000, "runtime_str": "694 us", "start_timestamp": "01:23:41.992.992577", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 60}], "resources": {"cpu4": {"time": {"ts": 1677893021992577000, "dur": 265000, "relative_dur": 0.0009599953630412653, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "265 us"}}, "gpu3": {"time": {"ts": 1677893022390412000, "dur": 694000, "relative_dur": 0.002514101063964672, "relative_gap_to_previous": 3.6226240114764728e-06, "parent_is_longest": true, "runtime_str": "694 us"}}}, "is_backward_op": true, "id": "7vWVnbZEuySl8Pw3", "pretty_name": "PositionalEncoding", "trace_file": "/results/Transformer/Transformer.534.pt.trace.json", "trace_disk_size": "8.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 694000, "runtime_str": "694 us", "start_timestamp": "01:23:41.992.992577", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 60}, {"name": "Embedding", "type": "Embedding", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 213, in forward\n    output = self.decoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 301, in forward\n    with hotline.annotate('Embedding'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 301, "resources": {"cpu4": {"time": {"ts": 1677893021992847000, "dur": 415000, "relative_dur": 0.0015033889647627363, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "415 us"}}, "gpu3": {"time": {"ts": 1677893022391107000, "dur": 1112000, "relative_dur": 0.004028357900761838, "relative_gap_to_previous": 3.6226240114764728e-06, "parent_is_longest": true, "runtime_str": "1 ms"}}}, "is_backward_op": true, "id": "Sh3KftpSsX5bk2c9", "pretty_name": "Embedding", "trace_file": "/results/Transformer/Transformer.536.pt.trace.json", "trace_disk_size": "17.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1112000, "runtime_str": "1 ms", "start_timestamp": "01:23:41.992.992847", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 112}], "resources": {"cpu4": {"time": {"ts": 1677893021971155000, "dur": 22075000, "relative_dur": 0.052724979817618145, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "22 ms"}}, "gpu3": {"time": {"ts": 1677893022116176000, "dur": 276043000, "relative_dur": 0.6593142289374753, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "276 ms"}}}, "is_backward_op": true, "id": "dWAqoSWaQ5ayohmm", "pretty_name": "Decoder", "trace_file": "/results/Transformer/Transformer.344.pt.trace.json", "trace_disk_size": "740.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 276043000, "runtime_str": "276 ms", "start_timestamp": "01:23:41.971.971155", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5589}, {"name": "Encoder", "type": "Encoder", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 205, in forward\n    with hotline.annotate('Encoder'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 205, "ops": [{"name": "TransformerEncoder", "type": "TransformerEncoder", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 260, in forward\n    with hotline.annotate('TransformerEncoder'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 260, "ops": [{"name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu4": {"time": {"ts": 1677893022391570000, "dur": 163000, "relative_dur": 0.012966351125606556, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "163 us"}}, "gpu3": {"time": {"ts": 1677893022392220000, "dur": 433000, "relative_dur": 0.03444435605759287, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "433 us"}}}, "is_backward_op": true, "id": "cZ5fud6OMnV96QzF", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.541.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 433000, "runtime_str": "433 us", "start_timestamp": "01:23:42.391.391570", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 36}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu4": {"time": {"ts": 1677893022391738000, "dur": 151000, "relative_dur": 0.012011773128629385, "relative_gap_to_previous": 0.0003977408320738207, "parent_is_longest": true, "runtime_str": "151 us"}}, "gpu3": {"time": {"ts": 1677893022392654000, "dur": 5515000, "relative_dur": 0.43870813777742423, "relative_gap_to_previous": 7.954816641476414e-05, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "K8KQstsaMQU085oN", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.542.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5515000, "runtime_str": "6 ms", "start_timestamp": "01:23:42.391.391738", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu4": {"time": {"ts": 1677893022391896000, "dur": 122000, "relative_dur": 0.009704876302601225, "relative_gap_to_previous": 0.000556837164903349, "parent_is_longest": true, "runtime_str": "122 us"}}, "gpu3": {"time": {"ts": 1677893022398169000, "dur": 423000, "relative_dur": 0.03364887439344523, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "423 us"}}}, "is_backward_op": true, "id": "dRRx7WPZXkSYHARH", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.543.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 423000, "runtime_str": "423 us", "start_timestamp": "01:23:42.391.391896", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu4": {"time": {"ts": 1677893022392022000, "dur": 24000, "relative_dur": 0.0019091559939543395, "relative_gap_to_previous": 0.00031819266565905654, "parent_is_longest": true, "runtime_str": "24 us"}}, "gpu3": {"time": {"ts": 1677893022398594000, "dur": 476000, "relative_dur": 0.03786492721342773, "relative_gap_to_previous": 0.00015909633282952827, "parent_is_longest": true, "runtime_str": "476 us"}}}, "is_backward_op": true, "id": "5eTxpgDi3XPd6IfY", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.544.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 476000, "runtime_str": "476 us", "start_timestamp": "01:23:42.392.392022", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu4": {"time": {"ts": 1677893022392050000, "dur": 158000, "relative_dur": 0.012568610293532734, "relative_gap_to_previous": 0.00031819266565905654, "parent_is_longest": true, "runtime_str": "158 us"}}, "gpu3": {"time": {"ts": 1677893022399072000, "dur": 5719000, "relative_dur": 0.4549359637260361, "relative_gap_to_previous": 0.00015909633282952827, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "1qwRyLGWeSLXTYtT", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.545.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5719000, "runtime_str": "6 ms", "start_timestamp": "01:23:42.392.392050", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1677893022391570000, "dur": 638000, "relative_dur": 0.026802218114602587, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "638 us"}}, "gpu3": {"time": {"ts": 1677893022392220000, "dur": 12571000, "relative_dur": 0.5281045202486977, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "13 ms"}}}, "is_backward_op": true, "id": "WJQ9PPY2YJVu5zzN", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.540.pt.trace.json", "trace_disk_size": "20.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12571000, "runtime_str": "13 ms", "start_timestamp": "01:23:42.391.391570", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 151}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu4": {"time": {"ts": 1677893022392216000, "dur": 136000, "relative_dur": 0.005713325491514031, "relative_gap_to_previous": 0.0003360779700890607, "parent_is_longest": true, "runtime_str": "136 us"}}, "gpu3": {"time": {"ts": 1677893022404792000, "dur": 440000, "relative_dur": 0.018484288354898338, "relative_gap_to_previous": 4.2009746261132585e-05, "parent_is_longest": true, "runtime_str": "440 us"}}}, "is_backward_op": true, "id": "YcEoZhZFgr5llT65", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.546.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 440000, "runtime_str": "440 us", "start_timestamp": "01:23:42.392.392216", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu4": {"time": {"ts": 1677893022392358000, "dur": 76000, "relative_dur": 0.0031927407158460763, "relative_gap_to_previous": 0.0002520584775667955, "parent_is_longest": true, "runtime_str": "76 us"}}, "gpu3": {"time": {"ts": 1677893022405233000, "dur": 100000, "relative_dur": 0.004200974626113258, "relative_gap_to_previous": 4.2009746261132585e-05, "parent_is_longest": true, "runtime_str": "100 us"}}}, "is_backward_op": true, "id": "wJMcL06gmzjFwVju", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.547.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 100000, "runtime_str": "100 us", "start_timestamp": "01:23:42.392.392358", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893022392438000, "dur": 143000, "relative_dur": 0.0141654284299158, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "143 us"}}, "gpu3": {"time": {"ts": 1677893022405334000, "dur": 1703000, "relative_dur": 0.16869737493808817, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "U8Rp6wdUs2uXijVF", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.549.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1703000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.392.392438", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893022392588000, "dur": 160000, "relative_dur": 0.0158494304110946, "relative_gap_to_previous": 0.0006934125804853888, "parent_is_longest": true, "runtime_str": "160 us"}}, "gpu3": {"time": {"ts": 1677893022407038000, "dur": 785000, "relative_dur": 0.07776126795443289, "relative_gap_to_previous": 9.905894006934126e-05, "parent_is_longest": true, "runtime_str": "785 us"}}}, "is_backward_op": true, "id": "oigQ54HwtnLevyuO", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.550.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 785000, "runtime_str": "785 us", "start_timestamp": "01:23:42.392.392588", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893022392754000, "dur": 28000, "relative_dur": 0.0027736503219415553, "relative_gap_to_previous": 0.0005943536404160475, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1677893022407824000, "dur": 360000, "relative_dur": 0.03566121842496285, "relative_gap_to_previous": 9.905894006934126e-05, "parent_is_longest": true, "runtime_str": "360 us"}}}, "is_backward_op": true, "id": "CxuvVbktZDidj4sk", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.551.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "01:23:42.392.392754", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893022392786000, "dur": 39000, "relative_dur": 0.003863298662704309, "relative_gap_to_previous": 0.00039623576027736503, "parent_is_longest": true, "runtime_str": "39 us"}}, "gpu3": {"time": {"ts": 1677893022408185000, "dur": 950000, "relative_dur": 0.0941059930658742, "relative_gap_to_previous": 9.905894006934126e-05, "parent_is_longest": true, "runtime_str": "950 us"}}}, "is_backward_op": true, "id": "GBawnBtjvlamuyg2", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.552.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 950000, "runtime_str": "950 us", "start_timestamp": "01:23:42.392.392786", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893022392830000, "dur": 71000, "relative_dur": 0.0070331847449232295, "relative_gap_to_previous": 0.0004952947003467063, "parent_is_longest": true, "runtime_str": "71 us"}}, "gpu3": {"time": {"ts": 1677893022409136000, "dur": 837000, "relative_dur": 0.08291233283803863, "relative_gap_to_previous": 9.905894006934126e-05, "parent_is_longest": true, "runtime_str": "837 us"}}}, "is_backward_op": true, "id": "4funA7LeQishi60N", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.553.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 837000, "runtime_str": "837 us", "start_timestamp": "01:23:42.392.392830", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893022392907000, "dur": 36000, "relative_dur": 0.003566121842496285, "relative_gap_to_previous": 0.0005943536404160475, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1677893022409973000, "dur": 81000, "relative_dur": 0.008023774145616641, "relative_gap_to_previous": 0.0, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "URCKiRs4S1ownfeB", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.554.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "01:23:42.392.392907", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893022392948000, "dur": 226000, "relative_dur": 0.022387320455671125, "relative_gap_to_previous": 0.0004952947003467063, "parent_is_longest": true, "runtime_str": "226 us"}}, "gpu3": {"time": {"ts": 1677893022410055000, "dur": 1788000, "relative_dur": 0.17711738484398218, "relative_gap_to_previous": 9.905894006934126e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "9lI75fuyLaU8n89X", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.555.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1788000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.392.392948", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893022393180000, "dur": 180000, "relative_dur": 0.017830609212481426, "relative_gap_to_previous": 0.0005943536404160475, "parent_is_longest": true, "runtime_str": "180 us"}}, "gpu3": {"time": {"ts": 1677893022411845000, "dur": 1824000, "relative_dur": 0.18068350668647845, "relative_gap_to_previous": 0.00019811788013868252, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "nwKXYMboWJs6HC8i", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.556.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1824000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.393.393180", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893022393366000, "dur": 184000, "relative_dur": 0.018226844972758793, "relative_gap_to_previous": 0.0005943536404160475, "parent_is_longest": true, "runtime_str": "184 us"}}, "gpu3": {"time": {"ts": 1677893022413670000, "dur": 1759000, "relative_dur": 0.17424467558197126, "relative_gap_to_previous": 9.905894006934126e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "ajEquviQusJOnBAP", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.557.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1759000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.393.393366", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893022392438000, "dur": 1112000, "relative_dur": 0.04671483784237943, "relative_gap_to_previous": 0.00016803898504453034, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022405334000, "dur": 10095000, "relative_dur": 0.4240883885061334, "relative_gap_to_previous": 4.2009746261132585e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "pnj2pnGSEStTmzmQ", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.548.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10095000, "runtime_str": "10 ms", "start_timestamp": "01:23:42.392.392438", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu4": {"time": {"ts": 1677893022393556000, "dur": 184000, "relative_dur": 0.007729793312048395, "relative_gap_to_previous": 0.0002520584775667955, "parent_is_longest": true, "runtime_str": "184 us"}}, "gpu3": {"time": {"ts": 1677893022415431000, "dur": 593000, "relative_dur": 0.024911779532851622, "relative_gap_to_previous": 8.401949252226517e-05, "parent_is_longest": true, "runtime_str": "593 us"}}}, "is_backward_op": true, "id": "s40lfkALxS7z39GL", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.558.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 593000, "runtime_str": "593 us", "start_timestamp": "01:23:42.393.393556", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 43}], "resources": {"cpu4": {"time": {"ts": 1677893022391570000, "dur": 2170000, "relative_dur": 0.015402524026517895, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893022392220000, "dur": 23804000, "relative_dur": 0.16895930042729582, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "24 ms"}}}, "is_backward_op": true, "id": "Jxti4MyiHNf9mzKS", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.539.pt.trace.json", "trace_disk_size": "72.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 23804000, "runtime_str": "24 ms", "start_timestamp": "01:23:42.391.391570", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 544}, {"name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu4": {"time": {"ts": 1677893022393747000, "dur": 75000, "relative_dur": 0.006183527083848627, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "75 us"}}, "gpu3": {"time": {"ts": 1677893022416025000, "dur": 98000, "relative_dur": 0.00807980872289554, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "98 us"}}}, "is_backward_op": true, "id": "K5fIJuUbMCYybOEZ", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.561.pt.trace.json", "trace_disk_size": "2.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 98000, "runtime_str": "98 us", "start_timestamp": "01:23:42.393.393747", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu4": {"time": {"ts": 1677893022393827000, "dur": 140000, "relative_dur": 0.011542583889850772, "relative_gap_to_previous": 0.00041223513892324183, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1677893022416125000, "dur": 5465000, "relative_dur": 0.4505730068431033, "relative_gap_to_previous": 0.00016489405556929673, "parent_is_longest": true, "runtime_str": "5 ms"}}}, "is_backward_op": true, "id": "YZ3QR62j1vQ96ZOb", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.562.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5465000, "runtime_str": "5 ms", "start_timestamp": "01:23:42.393.393827", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu4": {"time": {"ts": 1677893022393975000, "dur": 98000, "relative_dur": 0.00807980872289554, "relative_gap_to_previous": 0.0006595762222771869, "parent_is_longest": true, "runtime_str": "98 us"}}, "gpu3": {"time": {"ts": 1677893022421592000, "dur": 422000, "relative_dur": 0.03479264572512161, "relative_gap_to_previous": 0.00016489405556929673, "parent_is_longest": true, "runtime_str": "422 us"}}}, "is_backward_op": true, "id": "FPSxaGmi6bRpvLwf", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.563.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 422000, "runtime_str": "422 us", "start_timestamp": "01:23:42.393.393975", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu4": {"time": {"ts": 1677893022394078000, "dur": 24000, "relative_dur": 0.001978728666831561, "relative_gap_to_previous": 0.00041223513892324183, "parent_is_longest": true, "runtime_str": "24 us"}}, "gpu3": {"time": {"ts": 1677893022422016000, "dur": 475000, "relative_dur": 0.03916233819770797, "relative_gap_to_previous": 0.00016489405556929673, "parent_is_longest": true, "runtime_str": "475 us"}}}, "is_backward_op": true, "id": "PbxEz7uH1S0ma5T2", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.564.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 475000, "runtime_str": "475 us", "start_timestamp": "01:23:42.394.394078", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu4": {"time": {"ts": 1677893022394106000, "dur": 156000, "relative_dur": 0.012861736334405145, "relative_gap_to_previous": 0.00032978811113859347, "parent_is_longest": true, "runtime_str": "156 us"}}, "gpu3": {"time": {"ts": 1677893022422492000, "dur": 5662000, "relative_dur": 0.46681507131667904, "relative_gap_to_previous": 8.244702778464837e-05, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "FXtR3NjBDMklkexy", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.565.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5662000, "runtime_str": "6 ms", "start_timestamp": "01:23:42.394.394106", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1677893022393747000, "dur": 515000, "relative_dur": 0.022044345518363154, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "515 us"}}, "gpu3": {"time": {"ts": 1677893022416025000, "dur": 12129000, "relative_dur": 0.5191764403732557, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12 ms"}}}, "is_backward_op": true, "id": "ZXtHUKSa0nifkcC8", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.560.pt.trace.json", "trace_disk_size": "17.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12129000, "runtime_str": "12 ms", "start_timestamp": "01:23:42.393.393747", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 134}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu4": {"time": {"ts": 1677893022394270000, "dur": 137000, "relative_dur": 0.005864223953428645, "relative_gap_to_previous": 0.0003424364352367092, "parent_is_longest": true, "runtime_str": "137 us"}}, "gpu3": {"time": {"ts": 1677893022428155000, "dur": 438000, "relative_dur": 0.018748394829209827, "relative_gap_to_previous": 4.280455440458865e-05, "parent_is_longest": true, "runtime_str": "438 us"}}}, "is_backward_op": true, "id": "nrVUPnYdHZpTMD6h", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.566.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 438000, "runtime_str": "438 us", "start_timestamp": "01:23:42.394.394270", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu4": {"time": {"ts": 1677893022394413000, "dur": 76000, "relative_dur": 0.003253146134748737, "relative_gap_to_previous": 0.00025682732642753187, "parent_is_longest": true, "runtime_str": "76 us"}}, "gpu3": {"time": {"ts": 1677893022428594000, "dur": 98000, "relative_dur": 0.004194846331649687, "relative_gap_to_previous": 4.280455440458865e-05, "parent_is_longest": true, "runtime_str": "98 us"}}}, "is_backward_op": true, "id": "Mjisaajz0dTJMCWN", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.567.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 98000, "runtime_str": "98 us", "start_timestamp": "01:23:42.394.394413", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893022394493000, "dur": 143000, "relative_dur": 0.014157014157014158, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "143 us"}}, "gpu3": {"time": {"ts": 1677893022428694000, "dur": 1702000, "relative_dur": 0.1684981684981685, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "BVmSa9PExUw7fmdK", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.569.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1702000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.394.394493", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893022394642000, "dur": 153000, "relative_dur": 0.015147015147015146, "relative_gap_to_previous": 0.000594000594000594, "parent_is_longest": true, "runtime_str": "153 us"}}, "gpu3": {"time": {"ts": 1677893022430397000, "dur": 787000, "relative_dur": 0.07791307791307792, "relative_gap_to_previous": 9.9000099000099e-05, "parent_is_longest": true, "runtime_str": "787 us"}}}, "is_backward_op": true, "id": "6GTduwqMnPeJKmNq", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.570.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 787000, "runtime_str": "787 us", "start_timestamp": "01:23:42.394.394642", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893022394800000, "dur": 29000, "relative_dur": 0.002871002871002871, "relative_gap_to_previous": 0.000495000495000495, "parent_is_longest": true, "runtime_str": "29 us"}}, "gpu3": {"time": {"ts": 1677893022431185000, "dur": 361000, "relative_dur": 0.03573903573903574, "relative_gap_to_previous": 9.9000099000099e-05, "parent_is_longest": true, "runtime_str": "361 us"}}}, "is_backward_op": true, "id": "Hd0POAWzmrzUD8FA", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.571.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 361000, "runtime_str": "361 us", "start_timestamp": "01:23:42.394.394800", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893022394834000, "dur": 37000, "relative_dur": 0.003663003663003663, "relative_gap_to_previous": 0.000495000495000495, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1677893022431547000, "dur": 952000, "relative_dur": 0.09424809424809424, "relative_gap_to_previous": 9.9000099000099e-05, "parent_is_longest": true, "runtime_str": "952 us"}}}, "is_backward_op": true, "id": "GWtXmZZGTwHBQrug", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.572.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "01:23:42.394.394834", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893022394875000, "dur": 69000, "relative_dur": 0.006831006831006831, "relative_gap_to_previous": 0.000396000396000396, "parent_is_longest": true, "runtime_str": "69 us"}}, "gpu3": {"time": {"ts": 1677893022432500000, "dur": 837000, "relative_dur": 0.08286308286308286, "relative_gap_to_previous": 9.9000099000099e-05, "parent_is_longest": true, "runtime_str": "837 us"}}}, "is_backward_op": true, "id": "SW19hwhixoSId6cG", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.573.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 837000, "runtime_str": "837 us", "start_timestamp": "01:23:42.394.394875", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893022394949000, "dur": 36000, "relative_dur": 0.003564003564003564, "relative_gap_to_previous": 0.000495000495000495, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1677893022433338000, "dur": 81000, "relative_dur": 0.008019008019008018, "relative_gap_to_previous": 9.9000099000099e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "TKHjK9C7nwy2dVA0", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.574.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "01:23:42.394.394949", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893022394989000, "dur": 242000, "relative_dur": 0.023958023958023957, "relative_gap_to_previous": 0.000396000396000396, "parent_is_longest": true, "runtime_str": "242 us"}}, "gpu3": {"time": {"ts": 1677893022433421000, "dur": 1788000, "relative_dur": 0.177012177012177, "relative_gap_to_previous": 0.000198000198000198, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "t8BPuTvnRz688QVh", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.575.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1788000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.394.394989", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893022395237000, "dur": 190000, "relative_dur": 0.01881001881001881, "relative_gap_to_previous": 0.000594000594000594, "parent_is_longest": true, "runtime_str": "190 us"}}, "gpu3": {"time": {"ts": 1677893022435210000, "dur": 1825000, "relative_dur": 0.18067518067518068, "relative_gap_to_previous": 9.9000099000099e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "iDjIaPcdg3J2GfpK", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.576.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1825000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.395.395237", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893022395433000, "dur": 174000, "relative_dur": 0.017226017226017225, "relative_gap_to_previous": 0.000594000594000594, "parent_is_longest": true, "runtime_str": "174 us"}}, "gpu3": {"time": {"ts": 1677893022437037000, "dur": 1758000, "relative_dur": 0.17404217404217404, "relative_gap_to_previous": 0.000198000198000198, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "YgwmfTwXKCQKKs9N", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.577.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1758000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.395.395433", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893022394493000, "dur": 1114000, "relative_dur": 0.047684273606711756, "relative_gap_to_previous": 0.0001712182176183546, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022428694000, "dur": 10101000, "relative_dur": 0.43236880404074995, "relative_gap_to_previous": 8.56091088091773e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "fnr4OeWN8PcC49Ey", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.568.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10101000, "runtime_str": "10 ms", "start_timestamp": "01:23:42.394.394493", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu4": {"time": {"ts": 1677893022395613000, "dur": 185000, "relative_dur": 0.0079188425648489, "relative_gap_to_previous": 0.00025682732642753187, "parent_is_longest": true, "runtime_str": "185 us"}}, "gpu3": {"time": {"ts": 1677893022438796000, "dur": 591000, "relative_dur": 0.025297491653111893, "relative_gap_to_previous": 4.280455440458865e-05, "parent_is_longest": true, "runtime_str": "591 us"}}}, "is_backward_op": true, "id": "iV0ZxUIjlS8zpxf4", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.578.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 591000, "runtime_str": "591 us", "start_timestamp": "01:23:42.395.395613", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 43}], "resources": {"cpu4": {"time": {"ts": 1677893022393747000, "dur": 2051000, "relative_dur": 0.014557869483128203, "relative_gap_to_previous": 4.968556137586417e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893022416025000, "dur": 23362000, "relative_dur": 0.16582201212327696, "relative_gap_to_previous": 7.097937339409168e-06, "parent_is_longest": true, "runtime_str": "23 ms"}}}, "is_backward_op": true, "id": "2xsf3Yhp8mdgODUK", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.559.pt.trace.json", "trace_disk_size": "69.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 23362000, "runtime_str": "23 ms", "start_timestamp": "01:23:42.393.393747", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 527}, {"name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu4": {"time": {"ts": 1677893022395805000, "dur": 74000, "relative_dur": 0.00609605404069528, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "74 us"}}, "gpu3": {"time": {"ts": 1677893022439388000, "dur": 100000, "relative_dur": 0.008237910865804433, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "100 us"}}}, "is_backward_op": true, "id": "UloO6k3Ppgvf9Voa", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.581.pt.trace.json", "trace_disk_size": "2.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 100000, "runtime_str": "100 us", "start_timestamp": "01:23:42.395.395805", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu4": {"time": {"ts": 1677893022395884000, "dur": 164000, "relative_dur": 0.013510173819919268, "relative_gap_to_previous": 0.0004118955432902216, "parent_is_longest": true, "runtime_str": "164 us"}}, "gpu3": {"time": {"ts": 1677893022439490000, "dur": 5461000, "relative_dur": 0.44987231238158004, "relative_gap_to_previous": 0.00016475821731608864, "parent_is_longest": true, "runtime_str": "5 ms"}}}, "is_backward_op": true, "id": "TEdvfiGm3ETeqhix", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.582.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5461000, "runtime_str": "5 ms", "start_timestamp": "01:23:42.395.395884", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu4": {"time": {"ts": 1677893022396057000, "dur": 111000, "relative_dur": 0.009144081061042919, "relative_gap_to_previous": 0.0007414119779223989, "parent_is_longest": true, "runtime_str": "111 us"}}, "gpu3": {"time": {"ts": 1677893022444953000, "dur": 423000, "relative_dur": 0.034846362962352745, "relative_gap_to_previous": 0.00016475821731608864, "parent_is_longest": true, "runtime_str": "423 us"}}}, "is_backward_op": true, "id": "YTojk4Jo4kqBakTn", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.583.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 423000, "runtime_str": "423 us", "start_timestamp": "01:23:42.396.396057", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu4": {"time": {"ts": 1677893022396172000, "dur": 24000, "relative_dur": 0.0019770986077930638, "relative_gap_to_previous": 0.0003295164346321773, "parent_is_longest": true, "runtime_str": "24 us"}}, "gpu3": {"time": {"ts": 1677893022445378000, "dur": 475000, "relative_dur": 0.03913007661257105, "relative_gap_to_previous": 0.00016475821731608864, "parent_is_longest": true, "runtime_str": "475 us"}}}, "is_backward_op": true, "id": "YoWEsopWQRhTwuIr", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.584.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 475000, "runtime_str": "475 us", "start_timestamp": "01:23:42.396.396172", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu4": {"time": {"ts": 1677893022396200000, "dur": 165000, "relative_dur": 0.013592552928577313, "relative_gap_to_previous": 0.0003295164346321773, "parent_is_longest": true, "runtime_str": "165 us"}}, "gpu3": {"time": {"ts": 1677893022445855000, "dur": 5672000, "relative_dur": 0.4672543043084274, "relative_gap_to_previous": 0.00016475821731608864, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "312wVXdPXj6ILS6L", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.585.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5672000, "runtime_str": "6 ms", "start_timestamp": "01:23:42.396.396200", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1677893022395805000, "dur": 560000, "relative_dur": 0.023990061260335004, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "560 us"}}, "gpu3": {"time": {"ts": 1677893022439388000, "dur": 12139000, "relative_dur": 0.5200274172128689, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12 ms"}}}, "is_backward_op": true, "id": "Vx7MErJ4LPT5dPbK", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.580.pt.trace.json", "trace_disk_size": "17.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12139000, "runtime_str": "12 ms", "start_timestamp": "01:23:42.395.395805", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 134}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu4": {"time": {"ts": 1677893022396373000, "dur": 137000, "relative_dur": 0.005868997129760528, "relative_gap_to_previous": 0.00034271516086192865, "parent_is_longest": true, "runtime_str": "137 us"}}, "gpu3": {"time": {"ts": 1677893022451528000, "dur": 432000, "relative_dur": 0.018506618686544147, "relative_gap_to_previous": 4.283939510774108e-05, "parent_is_longest": true, "runtime_str": "432 us"}}}, "is_backward_op": true, "id": "wAK39SYq5x2g7L1C", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.586.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 432000, "runtime_str": "432 us", "start_timestamp": "01:23:42.396.396373", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu4": {"time": {"ts": 1677893022396516000, "dur": 84000, "relative_dur": 0.0035985091890502505, "relative_gap_to_previous": 0.00025703637064644646, "parent_is_longest": true, "runtime_str": "84 us"}}, "gpu3": {"time": {"ts": 1677893022451961000, "dur": 99000, "relative_dur": 0.0042411001156663665, "relative_gap_to_previous": 4.283939510774108e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "is_backward_op": true, "id": "tNGVjKKzLUNDUsMo", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.587.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "01:23:42.396.396516", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893022396604000, "dur": 153000, "relative_dur": 0.015181583647549117, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "153 us"}}, "gpu3": {"time": {"ts": 1677893022452061000, "dur": 1696000, "relative_dur": 0.1682873586028974, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "9BqZXEi3dVhJRAmZ", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.589.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1696000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.396.396604", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893022396763000, "dur": 144000, "relative_dur": 0.014288549315340345, "relative_gap_to_previous": 0.0005953562214725144, "parent_is_longest": true, "runtime_str": "144 us"}}, "gpu3": {"time": {"ts": 1677893022453758000, "dur": 790000, "relative_dur": 0.07838856916054773, "relative_gap_to_previous": 9.922603691208574e-05, "parent_is_longest": true, "runtime_str": "790 us"}}}, "is_backward_op": true, "id": "torSUYFhN2ZbQZ2q", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.590.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 790000, "runtime_str": "790 us", "start_timestamp": "01:23:42.396.396763", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893022396912000, "dur": 29000, "relative_dur": 0.002877555070450486, "relative_gap_to_previous": 0.0004961301845604287, "parent_is_longest": true, "runtime_str": "29 us"}}, "gpu3": {"time": {"ts": 1677893022454549000, "dur": 359000, "relative_dur": 0.03562214725143878, "relative_gap_to_previous": 9.922603691208574e-05, "parent_is_longest": true, "runtime_str": "359 us"}}}, "is_backward_op": true, "id": "eanhusvos2WyX2Jo", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.591.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 359000, "runtime_str": "359 us", "start_timestamp": "01:23:42.396.396912", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893022396945000, "dur": 38000, "relative_dur": 0.003770589402659258, "relative_gap_to_previous": 0.00039690414764834294, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1677893022454909000, "dur": 955000, "relative_dur": 0.09476086525104187, "relative_gap_to_previous": 9.922603691208574e-05, "parent_is_longest": true, "runtime_str": "955 us"}}}, "is_backward_op": true, "id": "x9T2pou7xdy66hQE", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.592.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 955000, "runtime_str": "955 us", "start_timestamp": "01:23:42.396.396945", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893022396987000, "dur": 70000, "relative_dur": 0.006945822583846001, "relative_gap_to_previous": 0.00039690414764834294, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1677893022455865000, "dur": 836000, "relative_dur": 0.08295296685850367, "relative_gap_to_previous": 9.922603691208574e-05, "parent_is_longest": true, "runtime_str": "836 us"}}}, "is_backward_op": true, "id": "IbObJHNf436B34bu", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.593.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 836000, "runtime_str": "836 us", "start_timestamp": "01:23:42.396.396987", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893022397063000, "dur": 36000, "relative_dur": 0.0035721373288350863, "relative_gap_to_previous": 0.0005953562214725144, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1677893022456702000, "dur": 81000, "relative_dur": 0.008037308989878944, "relative_gap_to_previous": 9.922603691208574e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "PL4dOW7mtYdvaSMK", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.594.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "01:23:42.397.397063", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893022397104000, "dur": 224000, "relative_dur": 0.022226632268307202, "relative_gap_to_previous": 0.0004961301845604287, "parent_is_longest": true, "runtime_str": "224 us"}}, "gpu3": {"time": {"ts": 1677893022456785000, "dur": 1782000, "relative_dur": 0.17682079777733678, "relative_gap_to_previous": 0.00019845207382417147, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "LnoenZIMrOg0sMaJ", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.595.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1782000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.397.397104", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893022397334000, "dur": 190000, "relative_dur": 0.01885294701329629, "relative_gap_to_previous": 0.0005953562214725144, "parent_is_longest": true, "runtime_str": "190 us"}}, "gpu3": {"time": {"ts": 1677893022458568000, "dur": 1817000, "relative_dur": 0.18029370906925976, "relative_gap_to_previous": 9.922603691208574e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "cRlrfKeUZwEA2cXQ", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.596.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1817000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.397.397334", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893022397530000, "dur": 175000, "relative_dur": 0.017364556459615002, "relative_gap_to_previous": 0.0005953562214725144, "parent_is_longest": true, "runtime_str": "175 us"}}, "gpu3": {"time": {"ts": 1677893022460386000, "dur": 1753000, "relative_dur": 0.1739432427068863, "relative_gap_to_previous": 9.922603691208574e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "GF057clsRuwIMTrc", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.597.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1753000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.397.397530", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893022396604000, "dur": 1101000, "relative_dur": 0.04716617401362293, "relative_gap_to_previous": 0.00017135758043096433, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022452061000, "dur": 10078000, "relative_dur": 0.4317354238958146, "relative_gap_to_previous": 4.283939510774108e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "dCY3GCQtLseUABVO", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.588.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10078000, "runtime_str": "10 ms", "start_timestamp": "01:23:42.396.396604", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu4": {"time": {"ts": 1677893022397711000, "dur": 186000, "relative_dur": 0.00796812749003984, "relative_gap_to_previous": 0.00025703637064644646, "parent_is_longest": true, "runtime_str": "186 us"}}, "gpu3": {"time": {"ts": 1677893022462141000, "dur": 590000, "relative_dur": 0.025275243113567235, "relative_gap_to_previous": 8.567879021548216e-05, "parent_is_longest": true, "runtime_str": "590 us"}}}, "is_backward_op": true, "id": "wiQsntHmwGeeBGg0", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.598.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 590000, "runtime_str": "590 us", "start_timestamp": "01:23:42.397.397711", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 43}], "resources": {"cpu4": {"time": {"ts": 1677893022395805000, "dur": 2092000, "relative_dur": 0.014848884914043979, "relative_gap_to_previous": 4.968556137586417e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893022439388000, "dur": 23343000, "relative_dur": 0.1656871513138282, "relative_gap_to_previous": 7.097937339409168e-06, "parent_is_longest": true, "runtime_str": "23 ms"}}}, "is_backward_op": true, "id": "NvQpd0Di6NkkxJtV", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.579.pt.trace.json", "trace_disk_size": "69.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 23343000, "runtime_str": "23 ms", "start_timestamp": "01:23:42.395.395805", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 527}, {"name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu4": {"time": {"ts": 1677893022397903000, "dur": 75000, "relative_dur": 0.006186077202243484, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "75 us"}}, "gpu3": {"time": {"ts": 1677893022462733000, "dur": 99000, "relative_dur": 0.0081656219069614, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "99 us"}}}, "is_backward_op": true, "id": "QZugpu1Fotq27S9T", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.601.pt.trace.json", "trace_disk_size": "2.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "01:23:42.397.397903", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu4": {"time": {"ts": 1677893022397983000, "dur": 140000, "relative_dur": 0.011547344110854504, "relative_gap_to_previous": 0.00041240514681623226, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1677893022462833000, "dur": 5461000, "relative_dur": 0.45042890135268887, "relative_gap_to_previous": 8.248102936324645e-05, "parent_is_longest": true, "runtime_str": "5 ms"}}}, "is_backward_op": true, "id": "luoQi3GZUzZYygDP", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.602.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5461000, "runtime_str": "5 ms", "start_timestamp": "01:23:42.397.397983", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu4": {"time": {"ts": 1677893022398130000, "dur": 108000, "relative_dur": 0.008907951171230617, "relative_gap_to_previous": 0.0005773672055427252, "parent_is_longest": true, "runtime_str": "108 us"}}, "gpu3": {"time": {"ts": 1677893022468295000, "dur": 423000, "relative_dur": 0.03488947542065325, "relative_gap_to_previous": 8.248102936324645e-05, "parent_is_longest": true, "runtime_str": "423 us"}}}, "is_backward_op": true, "id": "inUMlFTdTkDIjROj", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.603.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 423000, "runtime_str": "423 us", "start_timestamp": "01:23:42.398.398130", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu4": {"time": {"ts": 1677893022398243000, "dur": 23000, "relative_dur": 0.0018970636753546685, "relative_gap_to_previous": 0.00041240514681623226, "parent_is_longest": true, "runtime_str": "23 us"}}, "gpu3": {"time": {"ts": 1677893022468720000, "dur": 475000, "relative_dur": 0.03917848894754206, "relative_gap_to_previous": 0.0001649620587264929, "parent_is_longest": true, "runtime_str": "475 us"}}}, "is_backward_op": true, "id": "6XnfzONknyiukDWM", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.604.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 475000, "runtime_str": "475 us", "start_timestamp": "01:23:42.398.398243", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu4": {"time": {"ts": 1677893022398270000, "dur": 171000, "relative_dur": 0.014104256021115143, "relative_gap_to_previous": 0.0003299241174529858, "parent_is_longest": true, "runtime_str": "171 us"}}, "gpu3": {"time": {"ts": 1677893022469196000, "dur": 5661000, "relative_dur": 0.4669251072253382, "relative_gap_to_previous": 8.248102936324645e-05, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "ejnyVi1CBwwhTILv", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.605.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5661000, "runtime_str": "6 ms", "start_timestamp": "01:23:42.398.398270", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1677893022397903000, "dur": 538000, "relative_dur": 0.023042658900119924, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "538 us"}}, "gpu3": {"time": {"ts": 1677893022462733000, "dur": 12124000, "relative_dur": 0.5192735994517732, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12 ms"}}}, "is_backward_op": true, "id": "KM9fq23uHTerDSeP", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.600.pt.trace.json", "trace_disk_size": "17.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12124000, "runtime_str": "12 ms", "start_timestamp": "01:23:42.397.397903", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 134}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu4": {"time": {"ts": 1677893022398449000, "dur": 140000, "relative_dur": 0.005996230940551653, "relative_gap_to_previous": 0.00034264176803152307, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1677893022474858000, "dur": 431000, "relative_dur": 0.018459825252698302, "relative_gap_to_previous": 4.283022100394038e-05, "parent_is_longest": true, "runtime_str": "431 us"}}}, "is_backward_op": true, "id": "gE99eFSSpMFYZx5z", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.606.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 431000, "runtime_str": "431 us", "start_timestamp": "01:23:42.398.398449", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu4": {"time": {"ts": 1677893022398595000, "dur": 85000, "relative_dur": 0.0036405687853349323, "relative_gap_to_previous": 0.00025698132602364226, "parent_is_longest": true, "runtime_str": "85 us"}}, "gpu3": {"time": {"ts": 1677893022475290000, "dur": 100000, "relative_dur": 0.004283022100394038, "relative_gap_to_previous": 4.283022100394038e-05, "parent_is_longest": true, "runtime_str": "100 us"}}}, "is_backward_op": true, "id": "Caq1KCihw4GUwy6y", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.607.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 100000, "runtime_str": "100 us", "start_timestamp": "01:23:42.398.398595", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893022398685000, "dur": 141000, "relative_dur": 0.013978388024189551, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "141 us"}}, "gpu3": {"time": {"ts": 1677893022475391000, "dur": 1696000, "relative_dur": 0.16813720630514523, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "uwNWqL10GRouPdvF", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.609.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1696000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.398.398685", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893022398832000, "dur": 140000, "relative_dur": 0.013879250520471894, "relative_gap_to_previous": 0.0005948250223059383, "parent_is_longest": true, "runtime_str": "140 us"}}, "gpu3": {"time": {"ts": 1677893022477088000, "dur": 787000, "relative_dur": 0.07802121542579558, "relative_gap_to_previous": 9.91375037176564e-05, "parent_is_longest": true, "runtime_str": "787 us"}}}, "is_backward_op": true, "id": "RJLXaJaVsQBSgzuH", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.610.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 787000, "runtime_str": "787 us", "start_timestamp": "01:23:42.398.398832", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893022398978000, "dur": 28000, "relative_dur": 0.002775850104094379, "relative_gap_to_previous": 0.0005948250223059383, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1677893022477876000, "dur": 361000, "relative_dur": 0.03578863884207396, "relative_gap_to_previous": 9.91375037176564e-05, "parent_is_longest": true, "runtime_str": "361 us"}}}, "is_backward_op": true, "id": "wQiiIEh7nP8FRfL4", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.611.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 361000, "runtime_str": "361 us", "start_timestamp": "01:23:42.398.398978", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893022399010000, "dur": 38000, "relative_dur": 0.0037672251412709428, "relative_gap_to_previous": 0.0003965500148706256, "parent_is_longest": true, "runtime_str": "38 us"}}, "gpu3": {"time": {"ts": 1677893022478239000, "dur": 952000, "relative_dur": 0.09437890353920889, "relative_gap_to_previous": 0.0001982750074353128, "parent_is_longest": true, "runtime_str": "952 us"}}}, "is_backward_op": true, "id": "gIemxrLvThHfmk8H", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.612.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "01:23:42.399.399010", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893022399053000, "dur": 80000, "relative_dur": 0.007931000297412512, "relative_gap_to_previous": 0.000495687518588282, "parent_is_longest": true, "runtime_str": "80 us"}}, "gpu3": {"time": {"ts": 1677893022479192000, "dur": 835000, "relative_dur": 0.08277981560424308, "relative_gap_to_previous": 9.91375037176564e-05, "parent_is_longest": true, "runtime_str": "835 us"}}}, "is_backward_op": true, "id": "h4mxvWb2a83TFuoA", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.613.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 835000, "runtime_str": "835 us", "start_timestamp": "01:23:42.399.399053", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893022399139000, "dur": 37000, "relative_dur": 0.003668087637553286, "relative_gap_to_previous": 0.0005948250223059383, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1677893022480028000, "dur": 80000, "relative_dur": 0.007931000297412512, "relative_gap_to_previous": 9.91375037176564e-05, "parent_is_longest": true, "runtime_str": "80 us"}}}, "is_backward_op": true, "id": "yWrGeqaHXXA3fcvH", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.614.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 80000, "runtime_str": "80 us", "start_timestamp": "01:23:42.399.399139", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893022399180000, "dur": 240000, "relative_dur": 0.023793000892237532, "relative_gap_to_previous": 0.0003965500148706256, "parent_is_longest": true, "runtime_str": "240 us"}}, "gpu3": {"time": {"ts": 1677893022480110000, "dur": 1783000, "relative_dur": 0.17676216912858134, "relative_gap_to_previous": 0.0001982750074353128, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "CrBnshHomD8uw62Q", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.615.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1783000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.399.399180", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893022399426000, "dur": 191000, "relative_dur": 0.01893526321007237, "relative_gap_to_previous": 0.0005948250223059383, "parent_is_longest": true, "runtime_str": "191 us"}}, "gpu3": {"time": {"ts": 1677893022481894000, "dur": 1818000, "relative_dur": 0.1802319817586993, "relative_gap_to_previous": 9.91375037176564e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "zZddX256qZvVMqR1", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.616.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1818000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.399.399426", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893022399622000, "dur": 173000, "relative_dur": 0.017150788143154554, "relative_gap_to_previous": 0.000495687518588282, "parent_is_longest": true, "runtime_str": "173 us"}}, "gpu3": {"time": {"ts": 1677893022483713000, "dur": 1765000, "relative_dur": 0.17497769406166352, "relative_gap_to_previous": 9.91375037176564e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "RjwaxesL1bfprKT0", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.617.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1765000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.399.399622", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893022398685000, "dur": 1110000, "relative_dur": 0.04754154531437382, "relative_gap_to_previous": 0.0002141511050197019, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022475391000, "dur": 10087000, "relative_dur": 0.4320284392667466, "relative_gap_to_previous": 4.283022100394038e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "VwFH9qAFNdn7dooe", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.608.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10087000, "runtime_str": "10 ms", "start_timestamp": "01:23:42.398.398685", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu4": {"time": {"ts": 1677893022399809000, "dur": 178000, "relative_dur": 0.007623779338701388, "relative_gap_to_previous": 0.0005996230940551653, "parent_is_longest": true, "runtime_str": "178 us"}}, "gpu3": {"time": {"ts": 1677893022485479000, "dur": 602000, "relative_dur": 0.02578379304437211, "relative_gap_to_previous": 4.283022100394038e-05, "parent_is_longest": true, "runtime_str": "602 us"}}}, "is_backward_op": true, "id": "aIfJ27XIKLvgc3gh", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.618.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 602000, "runtime_str": "602 us", "start_timestamp": "01:23:42.399.399809", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 43}], "resources": {"cpu4": {"time": {"ts": 1677893022397903000, "dur": 2084000, "relative_dur": 0.014792101415328706, "relative_gap_to_previous": 4.258762403645501e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893022462733000, "dur": 23348000, "relative_dur": 0.16572264100052525, "relative_gap_to_previous": 1.4195874678818336e-05, "parent_is_longest": true, "runtime_str": "23 ms"}}}, "is_backward_op": true, "id": "wmnQfP17dwNhUrKR", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.599.pt.trace.json", "trace_disk_size": "69.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 23348000, "runtime_str": "23 ms", "start_timestamp": "01:23:42.397.397903", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 527}, {"name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu4": {"time": {"ts": 1677893022399993000, "dur": 75000, "relative_dur": 0.006134467528218551, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "75 us"}}, "gpu3": {"time": {"ts": 1677893022486083000, "dur": 99000, "relative_dur": 0.008097497137248487, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "99 us"}}}, "is_backward_op": true, "id": "4gzGOtiXkUZDE8jG", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.621.pt.trace.json", "trace_disk_size": "2.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "01:23:42.399.399993", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu4": {"time": {"ts": 1677893022400072000, "dur": 149000, "relative_dur": 0.012187142156060853, "relative_gap_to_previous": 0.0003271716015049894, "parent_is_longest": true, "runtime_str": "149 us"}}, "gpu3": {"time": {"ts": 1677893022486183000, "dur": 5506000, "relative_dur": 0.45035170947161784, "relative_gap_to_previous": 8.179290037624735e-05, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "vEU1iQGwsr3dNWus", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.622.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5506000, "runtime_str": "6 ms", "start_timestamp": "01:23:42.400.400072", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu4": {"time": {"ts": 1677893022400229000, "dur": 101000, "relative_dur": 0.008261082938000982, "relative_gap_to_previous": 0.0006543432030099788, "parent_is_longest": true, "runtime_str": "101 us"}}, "gpu3": {"time": {"ts": 1677893022491691000, "dur": 422000, "relative_dur": 0.03451660395877638, "relative_gap_to_previous": 0.0001635858007524947, "parent_is_longest": true, "runtime_str": "422 us"}}}, "is_backward_op": true, "id": "nae5zeOjWW94M23d", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.623.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 422000, "runtime_str": "422 us", "start_timestamp": "01:23:42.400.400229", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu4": {"time": {"ts": 1677893022400334000, "dur": 24000, "relative_dur": 0.001963029609029936, "relative_gap_to_previous": 0.0003271716015049894, "parent_is_longest": true, "runtime_str": "24 us"}}, "gpu3": {"time": {"ts": 1677893022492115000, "dur": 474000, "relative_dur": 0.03876983477834124, "relative_gap_to_previous": 0.0001635858007524947, "parent_is_longest": true, "runtime_str": "474 us"}}}, "is_backward_op": true, "id": "DNn1eV0gl05w5Usr", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.624.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 474000, "runtime_str": "474 us", "start_timestamp": "01:23:42.400.400334", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu4": {"time": {"ts": 1677893022400362000, "dur": 146000, "relative_dur": 0.011941763454932112, "relative_gap_to_previous": 0.0003271716015049894, "parent_is_longest": true, "runtime_str": "146 us"}}, "gpu3": {"time": {"ts": 1677893022492591000, "dur": 5718000, "relative_dur": 0.4676918043513823, "relative_gap_to_previous": 0.0001635858007524947, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "AtEYGbqesUI76uzw", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.625.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5718000, "runtime_str": "6 ms", "start_timestamp": "01:23:42.400.400362", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1677893022399993000, "dur": 515000, "relative_dur": 0.021920490337958627, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "515 us"}}, "gpu3": {"time": {"ts": 1677893022486083000, "dur": 12226000, "relative_dur": 0.520388184217247, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12 ms"}}}, "is_backward_op": true, "id": "GvU1nfh1bJ39LkmA", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.620.pt.trace.json", "trace_disk_size": "17.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12226000, "runtime_str": "12 ms", "start_timestamp": "01:23:42.399.399993", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 134}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu4": {"time": {"ts": 1677893022400515000, "dur": 135000, "relative_dur": 0.005746147952668767, "relative_gap_to_previous": 0.0002979484123606027, "parent_is_longest": true, "runtime_str": "135 us"}}, "gpu3": {"time": {"ts": 1677893022498311000, "dur": 428000, "relative_dur": 0.018217417212905422, "relative_gap_to_previous": 8.512811781731506e-05, "parent_is_longest": true, "runtime_str": "428 us"}}}, "is_backward_op": true, "id": "M9k6dFl5DeLUjp8P", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.626.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 428000, "runtime_str": "428 us", "start_timestamp": "01:23:42.400.400515", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu4": {"time": {"ts": 1677893022400665000, "dur": 79000, "relative_dur": 0.0033625606537839446, "relative_gap_to_previous": 0.000638460883629863, "parent_is_longest": true, "runtime_str": "79 us"}}, "gpu3": {"time": {"ts": 1677893022498741000, "dur": 100000, "relative_dur": 0.004256405890865753, "relative_gap_to_previous": 8.512811781731506e-05, "parent_is_longest": true, "runtime_str": "100 us"}}}, "is_backward_op": true, "id": "18UlmWNFJYlNsI8x", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.627.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 100000, "runtime_str": "100 us", "start_timestamp": "01:23:42.400.400665", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893022400748000, "dur": 142000, "relative_dur": 0.013999802819678596, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "142 us"}}, "gpu3": {"time": {"ts": 1677893022498843000, "dur": 1710000, "relative_dur": 0.16858917480035493, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "ut8q7mFIESSIpvfs", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.629.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1710000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.400.400748", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893022400896000, "dur": 144000, "relative_dur": 0.01419698314108252, "relative_gap_to_previous": 0.0005915409642117716, "parent_is_longest": true, "runtime_str": "144 us"}}, "gpu3": {"time": {"ts": 1677893022500554000, "dur": 795000, "relative_dur": 0.07837917775805975, "relative_gap_to_previous": 9.859016070196194e-05, "parent_is_longest": true, "runtime_str": "795 us"}}}, "is_backward_op": true, "id": "oJRCZgXd7WWDYyRT", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.630.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 795000, "runtime_str": "795 us", "start_timestamp": "01:23:42.400.400896", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893022401045000, "dur": 28000, "relative_dur": 0.0027605244996549345, "relative_gap_to_previous": 0.0004929508035098097, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1677893022501350000, "dur": 361000, "relative_dur": 0.03559104801340826, "relative_gap_to_previous": 9.859016070196194e-05, "parent_is_longest": true, "runtime_str": "361 us"}}}, "is_backward_op": true, "id": "61sDjJItCIdaYlEJ", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.631.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 361000, "runtime_str": "361 us", "start_timestamp": "01:23:42.401.401045", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893022401083000, "dur": 43000, "relative_dur": 0.004239376910184363, "relative_gap_to_previous": 0.0009859016070196194, "parent_is_longest": true, "runtime_str": "43 us"}}, "gpu3": {"time": {"ts": 1677893022501712000, "dur": 951000, "relative_dur": 0.09375924282756581, "relative_gap_to_previous": 9.859016070196194e-05, "parent_is_longest": true, "runtime_str": "951 us"}}}, "is_backward_op": true, "id": "Bp1Fb2J9MJrE96xI", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.632.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 951000, "runtime_str": "951 us", "start_timestamp": "01:23:42.401.401083", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893022401131000, "dur": 71000, "relative_dur": 0.006999901409839298, "relative_gap_to_previous": 0.0004929508035098097, "parent_is_longest": true, "runtime_str": "71 us"}}, "gpu3": {"time": {"ts": 1677893022502664000, "dur": 842000, "relative_dur": 0.08301291531105195, "relative_gap_to_previous": 9.859016070196194e-05, "parent_is_longest": true, "runtime_str": "842 us"}}}, "is_backward_op": true, "id": "gjGvAr75LarPKpEl", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.633.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 842000, "runtime_str": "842 us", "start_timestamp": "01:23:42.401.401131", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893022401207000, "dur": 37000, "relative_dur": 0.003647835945972592, "relative_gap_to_previous": 0.0004929508035098097, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1677893022503507000, "dur": 81000, "relative_dur": 0.007985803016858917, "relative_gap_to_previous": 9.859016070196194e-05, "parent_is_longest": true, "runtime_str": "81 us"}}}, "is_backward_op": true, "id": "SKLK3Ung6e1BSj3B", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.634.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 81000, "runtime_str": "81 us", "start_timestamp": "01:23:42.401.401207", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893022401248000, "dur": 226000, "relative_dur": 0.0222813763186434, "relative_gap_to_previous": 0.00039436064280784776, "parent_is_longest": true, "runtime_str": "226 us"}}, "gpu3": {"time": {"ts": 1677893022503590000, "dur": 1795000, "relative_dur": 0.1769693384600217, "relative_gap_to_previous": 0.00019718032140392388, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "9U793nByNkqtrSPu", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.635.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1795000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.401.401248", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893022401481000, "dur": 205000, "relative_dur": 0.0202109829439022, "relative_gap_to_previous": 0.0006901311249137336, "parent_is_longest": true, "runtime_str": "205 us"}}, "gpu3": {"time": {"ts": 1677893022505387000, "dur": 1831000, "relative_dur": 0.18051858424529232, "relative_gap_to_previous": 0.00019718032140392388, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "eND1r7jCee9MccCh", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.636.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1831000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.401.401481", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893022401692000, "dur": 187000, "relative_dur": 0.018436360051266882, "relative_gap_to_previous": 0.0005915409642117716, "parent_is_longest": true, "runtime_str": "187 us"}}, "gpu3": {"time": {"ts": 1677893022507219000, "dur": 1767000, "relative_dur": 0.17420881396036675, "relative_gap_to_previous": 9.859016070196194e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "5ux91Wo73gv6jTVd", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.637.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1767000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.401.401692", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893022400748000, "dur": 1131000, "relative_dur": 0.048139950625691666, "relative_gap_to_previous": 0.0001702562356346301, "parent_is_longest": true, "runtime_str": "1 ms"}}, "gpu3": {"time": {"ts": 1677893022498843000, "dur": 10143000, "relative_dur": 0.43172724951051333, "relative_gap_to_previous": 8.512811781731506e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "NHCtHaq93X9H7Zy6", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.628.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10143000, "runtime_str": "10 ms", "start_timestamp": "01:23:42.400.400748", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu4": {"time": {"ts": 1677893022401885000, "dur": 177000, "relative_dur": 0.007533838426832383, "relative_gap_to_previous": 0.00025538435345194517, "parent_is_longest": true, "runtime_str": "177 us"}}, "gpu3": {"time": {"ts": 1677893022508987000, "dur": 590000, "relative_dur": 0.02511279475610794, "relative_gap_to_previous": 4.256405890865753e-05, "parent_is_longest": true, "runtime_str": "590 us"}}}, "is_backward_op": true, "id": "IwzcZ4qXXsBuQxZh", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.638.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 590000, "runtime_str": "590 us", "start_timestamp": "01:23:42.401.401885", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 43}], "resources": {"cpu4": {"time": {"ts": 1677893022399993000, "dur": 2069000, "relative_dur": 0.014685632355237567, "relative_gap_to_previous": 4.258762403645501e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893022486083000, "dur": 23494000, "relative_dur": 0.16675893985207899, "relative_gap_to_previous": 1.4195874678818336e-05, "parent_is_longest": true, "runtime_str": "23 ms"}}}, "is_backward_op": true, "id": "Xbjx9QUvaXtfpzz2", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.619.pt.trace.json", "trace_disk_size": "69.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 23494000, "runtime_str": "23 ms", "start_timestamp": "01:23:42.399.399993", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 527}, {"name": "TransformerEncoderLayer", "type": "TransformerEncoderLayer", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 482, in forward\n    with hotline.annotate('TransformerEncoderLayer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 482, "ops": [{"name": "Feed-Forward", "type": "Feed-Forward", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 514, in _ff_block\n    with hotline.annotate('Feed-Forward'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 514, "ops": [{"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 523, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 523, "resources": {"cpu4": {"time": {"ts": 1677893022402068000, "dur": 76000, "relative_dur": 0.006213719238001798, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "76 us"}}, "gpu3": {"time": {"ts": 1677893022509579000, "dur": 99000, "relative_dur": 0.008094186902133923, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "99 us"}}}, "is_backward_op": true, "id": "k9554KZe8qgKpJRT", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.641.pt.trace.json", "trace_disk_size": "2.9 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "01:23:42.402.402068", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 521, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 521, "resources": {"cpu4": {"time": {"ts": 1677893022402148000, "dur": 153000, "relative_dur": 0.012509197939661517, "relative_gap_to_previous": 0.0003270378546316736, "parent_is_longest": true, "runtime_str": "153 us"}}, "gpu3": {"time": {"ts": 1677893022509679000, "dur": 5510000, "relative_dur": 0.4504946447551304, "relative_gap_to_previous": 8.17594636579184e-05, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "zm74MJqLgQARm8cY", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.642.pt.trace.json", "trace_disk_size": "4.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5510000, "runtime_str": "6 ms", "start_timestamp": "01:23:42.402.402148", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 39}, {"name": "dropout", "type": "dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 519, in _ff_block\n    with hotline.annotate('dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 519, "resources": {"cpu4": {"time": {"ts": 1677893022402309000, "dur": 99000, "relative_dur": 0.008094186902133923, "relative_gap_to_previous": 0.0006540757092633472, "parent_is_longest": true, "runtime_str": "99 us"}}, "gpu3": {"time": {"ts": 1677893022515191000, "dur": 425000, "relative_dur": 0.03474777205461532, "relative_gap_to_previous": 0.0001635189273158368, "parent_is_longest": true, "runtime_str": "425 us"}}}, "is_backward_op": true, "id": "dtKOermhaSVIEoML", "pretty_name": "dropout", "trace_file": "/results/Transformer/Transformer.643.pt.trace.json", "trace_disk_size": "3.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 425000, "runtime_str": "425 us", "start_timestamp": "01:23:42.402.402309", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 26}, {"name": "relu", "type": "relu", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 517, in _ff_block\n    with hotline.annotate('relu'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 517, "resources": {"cpu4": {"time": {"ts": 1677893022402412000, "dur": 24000, "relative_dur": 0.0019622271277900416, "relative_gap_to_previous": 0.0003270378546316736, "parent_is_longest": true, "runtime_str": "24 us"}}, "gpu3": {"time": {"ts": 1677893022515617000, "dur": 475000, "relative_dur": 0.038835745237511245, "relative_gap_to_previous": 8.17594636579184e-05, "parent_is_longest": true, "runtime_str": "475 us"}}}, "is_backward_op": true, "id": "7vnwkbLGeh318wKb", "pretty_name": "relu", "trace_file": "/results/Transformer/Transformer.644.pt.trace.json", "trace_disk_size": "1.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 475000, "runtime_str": "475 us", "start_timestamp": "01:23:42.402.402412", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 5}, {"name": "linear", "type": "linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 490, in forward\n    x = x + self._ff_block(xx)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 515, in _ff_block\n    with hotline.annotate('linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 515, "resources": {"cpu4": {"time": {"ts": 1677893022402440000, "dur": 147000, "relative_dur": 0.012018641157714005, "relative_gap_to_previous": 0.0003270378546316736, "parent_is_longest": true, "runtime_str": "147 us"}}, "gpu3": {"time": {"ts": 1677893022516094000, "dur": 5716000, "relative_dur": 0.4673370942686616, "relative_gap_to_previous": 0.0001635189273158368, "parent_is_longest": true, "runtime_str": "6 ms"}}}, "is_backward_op": true, "id": "1C0YVJt6Yn38ZEKP", "pretty_name": "linear", "trace_file": "/results/Transformer/Transformer.645.pt.trace.json", "trace_disk_size": "5.5 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 5716000, "runtime_str": "6 ms", "start_timestamp": "01:23:42.402.402440", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 45}], "resources": {"cpu4": {"time": {"ts": 1677893022402068000, "dur": 519000, "relative_dur": 0.022059761125515366, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "519 us"}}, "gpu3": {"time": {"ts": 1677893022509579000, "dur": 12231000, "relative_dur": 0.5198707867556425, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "12 ms"}}}, "is_backward_op": true, "id": "yCFBKCt3NbRcHf1u", "pretty_name": "Feed-Forward", "trace_file": "/results/Transformer/Transformer.640.pt.trace.json", "trace_disk_size": "17.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 12231000, "runtime_str": "12 ms", "start_timestamp": "01:23:42.402.402068", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 134}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 488, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 488, "resources": {"cpu4": {"time": {"ts": 1677893022402594000, "dur": 157000, "relative_dur": 0.00667318400136014, "relative_gap_to_previous": 0.0002975304968759298, "parent_is_longest": true, "runtime_str": "157 us"}}, "gpu3": {"time": {"ts": 1677893022521811000, "dur": 443000, "relative_dur": 0.0188294300165767, "relative_gap_to_previous": 4.25043566965614e-05, "parent_is_longest": true, "runtime_str": "443 us"}}}, "is_backward_op": true, "id": "gOWUp4gJnczuXzG1", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.646.pt.trace.json", "trace_disk_size": "4.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 443000, "runtime_str": "443 us", "start_timestamp": "01:23:42.402.402594", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 32}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 509, in _sa_block\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 509, "resources": {"cpu4": {"time": {"ts": 1677893022402757000, "dur": 77000, "relative_dur": 0.0032728354656352274, "relative_gap_to_previous": 0.0002550261401793684, "parent_is_longest": true, "runtime_str": "77 us"}}, "gpu3": {"time": {"ts": 1677893022522255000, "dur": 100000, "relative_dur": 0.00425043566965614, "relative_gap_to_previous": 4.25043566965614e-05, "parent_is_longest": true, "runtime_str": "100 us"}}}, "is_backward_op": true, "id": "9lphSmZeOggsEfR7", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.647.pt.trace.json", "trace_disk_size": "3.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 100000, "runtime_str": "100 us", "start_timestamp": "01:23:42.402.402757", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 19}, {"name": "Self-Attention", "type": "Self-Attention", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 504, in _sa_block\n    with hotline.annotate('Self-Attention'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 504, "ops": [{"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1211, in multi_head_attention_forward\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1211, "resources": {"cpu4": {"time": {"ts": 1677893022402838000, "dur": 143000, "relative_dur": 0.014091446590461175, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "143 us"}}, "gpu3": {"time": {"ts": 1677893022522356000, "dur": 1710000, "relative_dur": 0.16850610957824202, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "hEZcol9RpYt3lXNN", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.649.pt.trace.json", "trace_disk_size": "5.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1710000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.402.402838", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 40}, {"name": "BatchMatMul", "type": "BatchMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1281, in _scaled_dot_product_attention\n    with hotline.annotate('BatchMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1281, "resources": {"cpu4": {"time": {"ts": 1677893022402987000, "dur": 152000, "relative_dur": 0.014978320851399291, "relative_gap_to_previous": 0.0005912495072920772, "parent_is_longest": true, "runtime_str": "152 us"}}, "gpu3": {"time": {"ts": 1677893022524068000, "dur": 791000, "relative_dur": 0.07794639337800552, "relative_gap_to_previous": 0.00019708316909735908, "parent_is_longest": true, "runtime_str": "791 us"}}}, "is_backward_op": true, "id": "MOSFuigtszyn4F5D", "pretty_name": "BatchMatMul", "trace_file": "/results/Transformer/Transformer.650.pt.trace.json", "trace_disk_size": "5.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 791000, "runtime_str": "791 us", "start_timestamp": "01:23:42.402.402987", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 44}, {"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1277, in _scaled_dot_product_attention\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1277, "resources": {"cpu4": {"time": {"ts": 1677893022403145000, "dur": 28000, "relative_dur": 0.0027591643673630273, "relative_gap_to_previous": 0.0005912495072920772, "parent_is_longest": true, "runtime_str": "28 us"}}, "gpu3": {"time": {"ts": 1677893022524860000, "dur": 360000, "relative_dur": 0.035474970437524636, "relative_gap_to_previous": 9.854158454867954e-05, "parent_is_longest": true, "runtime_str": "360 us"}}}, "is_backward_op": true, "id": "e5P596GEx1jvheEk", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.651.pt.trace.json", "trace_disk_size": "1.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 360000, "runtime_str": "360 us", "start_timestamp": "01:23:42.403.403145", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 7}, {"name": "SoftMax", "type": "SoftMax", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1275, in _scaled_dot_product_attention\n    with hotline.annotate('SoftMax'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1275, "resources": {"cpu4": {"time": {"ts": 1677893022403177000, "dur": 37000, "relative_dur": 0.003646038628301143, "relative_gap_to_previous": 0.00039416633819471815, "parent_is_longest": true, "runtime_str": "37 us"}}, "gpu3": {"time": {"ts": 1677893022525221000, "dur": 952000, "relative_dur": 0.09381158849034292, "relative_gap_to_previous": 9.854158454867954e-05, "parent_is_longest": true, "runtime_str": "952 us"}}}, "is_backward_op": true, "id": "jkYtgt4Y8dRm8X5C", "pretty_name": "SoftMax", "trace_file": "/results/Transformer/Transformer.652.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 952000, "runtime_str": "952 us", "start_timestamp": "01:23:42.403.403177", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8}, {"name": "BatchedAddMatMul", "type": "BatchedAddMatMul", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1269, in _scaled_dot_product_attention\n    with hotline.annotate('BatchedAddMatMul'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1269, "resources": {"cpu4": {"time": {"ts": 1677893022403219000, "dur": 70000, "relative_dur": 0.006897910918407568, "relative_gap_to_previous": 0.0004927079227433977, "parent_is_longest": true, "runtime_str": "70 us"}}, "gpu3": {"time": {"ts": 1677893022526174000, "dur": 842000, "relative_dur": 0.08297201418998817, "relative_gap_to_previous": 9.854158454867954e-05, "parent_is_longest": true, "runtime_str": "842 us"}}}, "is_backward_op": true, "id": "GHg4hRrJO7T0Rr1S", "pretty_name": "BatchedAddMatMul", "trace_file": "/results/Transformer/Transformer.653.pt.trace.json", "trace_disk_size": "3.0 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 842000, "runtime_str": "842 us", "start_timestamp": "01:23:42.403.403219", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 24}, {"name": "Div", "type": "Div", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1208, in multi_head_attention_forward\n    attn_output, attn_output_weights = _scaled_dot_product_attention(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1265, in _scaled_dot_product_attention\n    with hotline.annotate('Div'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1265, "resources": {"cpu4": {"time": {"ts": 1677893022403295000, "dur": 36000, "relative_dur": 0.0035474970437524636, "relative_gap_to_previous": 0.0005912495072920772, "parent_is_longest": true, "runtime_str": "36 us"}}, "gpu3": {"time": {"ts": 1677893022527017000, "dur": 80000, "relative_dur": 0.007883326763894364, "relative_gap_to_previous": 9.854158454867954e-05, "parent_is_longest": true, "runtime_str": "80 us"}}}, "is_backward_op": true, "id": "7eRfycjy6xb2LBzm", "pretty_name": "Div", "trace_file": "/results/Transformer/Transformer.654.pt.trace.json", "trace_disk_size": "1.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 80000, "runtime_str": "80 us", "start_timestamp": "01:23:42.403.403295", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 9}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1340, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1340, "resources": {"cpu4": {"time": {"ts": 1677893022403335000, "dur": 226000, "relative_dur": 0.022270398108001577, "relative_gap_to_previous": 0.00039416633819471815, "parent_is_longest": true, "runtime_str": "226 us"}}, "gpu3": {"time": {"ts": 1677893022527099000, "dur": 1797000, "relative_dur": 0.17707922743397714, "relative_gap_to_previous": 0.00019708316909735908, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "Sw8OhWZkrIdg4UIK", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.655.pt.trace.json", "trace_disk_size": "8.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1797000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.403.403335", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 66}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1338, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1338, "resources": {"cpu4": {"time": {"ts": 1677893022403566000, "dur": 183000, "relative_dur": 0.018033109972408357, "relative_gap_to_previous": 0.0004927079227433977, "parent_is_longest": true, "runtime_str": "183 us"}}, "gpu3": {"time": {"ts": 1677893022528897000, "dur": 1838000, "relative_dur": 0.181119432400473, "relative_gap_to_previous": 9.854158454867954e-05, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "SNfLBdPMWjk5uEWd", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.656.pt.trace.json", "trace_disk_size": "6.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1838000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.403.403566", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 52}, {"name": "Linear", "type": "Linear", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 487, in forward\n    x = x + self._sa_block(xx, src_mask, src_key_padding_mask)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 505, in _sa_block\n    x = self.self_attn(x, x, x,\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 969, in forward\n    attn_output, attn_output_weights, loc_cache = multi_head_attention_forward(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1098, in multi_head_attention_forward\n    q, k, v = _in_projection(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 1336, in _in_projection\n    with hotline.annotate('Linear'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 1336, "resources": {"cpu4": {"time": {"ts": 1677893022403755000, "dur": 832000, "relative_dur": 0.08198659834450138, "relative_gap_to_previous": 0.0005912495072920772, "parent_is_longest": true, "runtime_str": "832 us"}}, "gpu3": {"time": {"ts": 1677893022530737000, "dur": 1767000, "relative_dur": 0.17412297989751674, "relative_gap_to_previous": 0.00019708316909735908, "parent_is_longest": true, "runtime_str": "2 ms"}}}, "is_backward_op": true, "id": "RtoxJV0sETzIdeIc", "pretty_name": "Linear", "trace_file": "/results/Transformer/Transformer.657.pt.trace.json", "trace_disk_size": "5.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1767000, "runtime_str": "2 ms", "start_timestamp": "01:23:42.403.403755", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 49}], "resources": {"cpu4": {"time": {"ts": 1677893022402838000, "dur": 1749000, "relative_dur": 0.07434011986228588, "relative_gap_to_previous": 0.0001700174267862456, "parent_is_longest": true, "runtime_str": "2 ms"}}, "gpu3": {"time": {"ts": 1677893022522356000, "dur": 10148000, "relative_dur": 0.43133421175670505, "relative_gap_to_previous": 4.25043566965614e-05, "parent_is_longest": true, "runtime_str": "10 ms"}}}, "is_backward_op": true, "id": "9c2k1DkKtqVq86YB", "pretty_name": "Self-Attention", "trace_file": "/results/Transformer/Transformer.648.pt.trace.json", "trace_disk_size": "38.3 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 10148000, "runtime_str": "10 ms", "start_timestamp": "01:23:42.402.402838", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 299}, {"name": "LayerNorm", "type": "LayerNorm", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 261, in forward\n    memory = self.encoder(src, mask=src_mask)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 280, in forward\n    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask_for_layers)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 485, in forward\n    with hotline.annotate('LayerNorm'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 485, "resources": {"cpu4": {"time": {"ts": 1677893022404593000, "dur": 184000, "relative_dur": 0.007820801632167296, "relative_gap_to_previous": 0.0002550261401793684, "parent_is_longest": true, "runtime_str": "184 us"}}, "gpu3": {"time": {"ts": 1677893022532505000, "dur": 601000, "relative_dur": 0.0255451183746334, "relative_gap_to_previous": 4.25043566965614e-05, "parent_is_longest": true, "runtime_str": "601 us"}}}, "is_backward_op": true, "id": "ZBC2uQFdGGCg1aHx", "pretty_name": "LayerNorm", "trace_file": "/results/Transformer/Transformer.658.pt.trace.json", "trace_disk_size": "6.2 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 601000, "runtime_str": "601 us", "start_timestamp": "01:23:42.404.404593", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 43}], "resources": {"cpu4": {"time": {"ts": 1677893022402068000, "dur": 2709000, "relative_dur": 0.019228312252459436, "relative_gap_to_previous": 4.258762403645501e-05, "parent_is_longest": true, "runtime_str": "3 ms"}}, "gpu3": {"time": {"ts": 1677893022509579000, "dur": 23527000, "relative_dur": 0.1669931717842795, "relative_gap_to_previous": 1.4195874678818336e-05, "parent_is_longest": true, "runtime_str": "24 ms"}}}, "is_backward_op": true, "id": "7D9b0jVrDvWXSDtL", "pretty_name": "TransformerEncoderLayer", "trace_file": "/results/Transformer/Transformer.639.pt.trace.json", "trace_disk_size": "69.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 23527000, "runtime_str": "24 ms", "start_timestamp": "01:23:42.402.402068", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 527}], "resources": {"cpu4": {"time": {"ts": 1677893022391570000, "dur": 13207000, "relative_dur": 0.09289386873738333, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "13 ms"}}, "gpu3": {"time": {"ts": 1677893022392220000, "dur": 140886000, "relative_dur": 0.9909476482876496, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "141 ms"}}}, "is_backward_op": true, "id": "ls3ynUxcQNadtFDG", "pretty_name": "TransformerEncoder", "trace_file": "/results/Transformer/Transformer.538.pt.trace.json", "trace_disk_size": "421.1 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 140886000, "runtime_str": "141 ms", "start_timestamp": "01:23:42.391.391570", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 3179}, {"name": "PositionalEncoding", "type": "PositionalEncoding", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 258, in forward\n    with hotline.annotate('PositionalEncoding'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 258, "ops": [{"name": "Dropout", "type": "Dropout", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 259, in forward\n    src = self.pos_encoder(src, inputs_positions)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 372, in forward\n    with hotline.annotate('Dropout'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 372, "resources": {"cpu4": {"time": {"ts": 1677893022404792000, "dur": 72000, "relative_dur": 0.7272727272727273, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1677893022533108000, "dur": 99000, "relative_dur": 1.0, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "99 us"}}}, "is_backward_op": true, "id": "2aO5bw5v2MeCBTXu", "pretty_name": "Dropout", "trace_file": "/results/Transformer/Transformer.660.pt.trace.json", "trace_disk_size": "2.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "01:23:42.404.404792", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 17}], "resources": {"cpu4": {"time": {"ts": 1677893022404792000, "dur": 72000, "relative_dur": 0.000506425270621004, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "72 us"}}, "gpu3": {"time": {"ts": 1677893022533108000, "dur": 99000, "relative_dur": 0.0006963347471038804, "relative_gap_to_previous": 1.4067368628361222e-05, "parent_is_longest": true, "runtime_str": "99 us"}}}, "is_backward_op": true, "id": "0dJsQ01qBVg5O1mI", "pretty_name": "PositionalEncoding", "trace_file": "/results/Transformer/Transformer.659.pt.trace.json", "trace_disk_size": "2.7 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 99000, "runtime_str": "99 us", "start_timestamp": "01:23:42.404.404792", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 17}, {"name": "Embedding", "type": "Embedding", "is_model_op": true, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 138, in update_params\n    logits, _ = workload.model_fn(\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/workload.py\", line 191, in model_fn\n    logits_batch = model(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n    return self.module(*inputs[0], **kwargs[0])\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 208, in forward\n    memory = self.encoder(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py\", line 254, in forward\n    with hotline.annotate('Embedding'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "source_file_num": 254, "resources": {"cpu4": {"time": {"ts": 1677893022404868000, "dur": 463000, "relative_dur": 0.003256595837465623, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "463 us"}}, "gpu3": {"time": {"ts": 1677893022533209000, "dur": 1184000, "relative_dur": 0.008327882227989844, "relative_gap_to_previous": 1.4067368628361222e-05, "parent_is_longest": true, "runtime_str": "1 ms"}}}, "is_backward_op": true, "id": "3LXiBlSpN7yDTxld", "pretty_name": "Embedding", "trace_file": "/results/Transformer/Transformer.661.pt.trace.json", "trace_disk_size": "15.8 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 1184000, "runtime_str": "1 ms", "start_timestamp": "01:23:42.404.404868", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 105}], "resources": {"cpu4": {"time": {"ts": 1677893022391570000, "dur": 13761000, "relative_dur": 0.0328674268299091, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "14 ms"}}, "gpu3": {"time": {"ts": 1677893022392220000, "dur": 142173000, "relative_dur": 0.3395727545010294, "relative_gap_to_previous": 2.388447556856994e-06, "parent_is_longest": true, "runtime_str": "142 ms"}}}, "is_backward_op": true, "id": "4Vxbv4MnxgXaGwNV", "pretty_name": "Encoder", "trace_file": "/results/Transformer/Transformer.537.pt.trace.json", "trace_disk_size": "439.6 kB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/algorithmic_efficiency/workloads/wmt/wmt_pytorch/models.py", "runtime": 142173000, "runtime_str": "142 ms", "start_timestamp": "01:23:42.391.391570", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 3301}], "is_model_pass": "Backward", "idx": 324, "id": "eojftgdjjenAO3lC", "pretty_name": "Backward", "trace_file": "/results/Transformer/Transformer.343.pt.trace.json", "trace_disk_size": "1.2 MB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "runtime": 418682000, "runtime_str": "419 ms", "start_timestamp": "01:23:41.971.971155", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3", "trace_event_count": 8902}, {"name": "Optimizer", "type": "training loop", "instances": 1, "resources": {"cpu2": {"time": {"ts": 1677893022534443000, "dur": 28656000, "relative_dur": 0.03930389747191011, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "29 ms"}}, "gpu3": {"time": {"ts": 1677893022535165000, "dur": 27915000, "relative_dur": 0.038287559252106744, "relative_gap_to_previous": 0.00042107399929775283, "parent_is_longest": true, "runtime_str": "28 ms"}}}, "source_stack_trace": "  File \"submission_runner.py\", line 592, in <module>\n    app.run(main)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\n    _run_main(main, args)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\n    sys.exit(main(argv))\n  File \"submission_runner.py\", line 566, in main\n    score = score_submission_on_workload(workload,\n  File \"submission_runner.py\", line 502, in score_submission_on_workload\n    timing, metrics = train_once(workload, global_batch_size,\n  File \"submission_runner.py\", line 356, in train_once\n    optimizer_state, model_params, model_state = update_params(\n  File \"/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py\", line 160, in update_params\n    with hotline.annotate('Optimizer'):\n", "source_file_name": "/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "source_file_num": 160, "ops": [{"name": "aten::sqrt(13%) and 9 others\u2026", "type": "generated", "generated_depth": 1, "instances": 1906, "resources": {"cpu2": {"time": {"ts": 1677893022534445000, "dur": 28633000, "relative_dur": 0.9991973757677275, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "29 ms"}, "res_name": "aten::sqrt(13%) and 9 others\u2026"}, "gpu3": {"time": {"ts": 1677893022535165000, "dur": 27915000, "relative_dur": 0.974141541038526, "relative_gap_to_previous": 0, "parent_is_longest": true, "runtime_str": "28 ms"}, "res_name": "vectorized_elementwise_kernel<4 addcdiv_cuda_kernelTensorIteratorBase& c10::Scalar const&::{lambda#2}::operator const::{lambda#14}::operator const::{lambda  #1} Array<char 4> addcdiv_cuda_kernelTensorIteratorBase& c10::Scalar const&::{lambda#2}::operator const::{lambda#14}::operator const::{lambda  #1} Array<char 4>(20%) and 6 others\u2026"}}, "id": "aT6aXydB5LoMQwiU", "pretty_name": "aten::sqrt(13%) and 9 others\u2026", "trace_file": "/results/Transformer/Transformer.663.pt.trace.json", "trace_disk_size": "1.0 MB", "runtime": 28633000, "runtime_str": "29 ms", "start_timestamp": "01:23:42.534.534445", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 5737}], "idx": 325, "id": "DT0bIXXyeC4sAQub", "pretty_name": "Optimizer", "trace_file": "/results/Transformer/Transformer.662.pt.trace.json", "trace_disk_size": "1.0 MB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/reference_algorithms/development_algorithms/wmt/wmt_pytorch/submission.py", "runtime": 28656000, "runtime_str": "29 ms", "start_timestamp": "01:23:42.534.534443", "recommendations": "None", "bound_by": "CPU-Bound", "longest_res": "cpu2", "trace_event_count": 5738}], "resources": {"cpu2": {"time": {"ts": 1677893021833875000, "dur": 674250000, "parent_is_longest": true, "runtime_str": "674 ms"}}, "cpu4": {"time": {"ts": 1677893021971155000, "dur": 35876000, "parent_is_longest": false, "runtime_str": "36 ms"}}, "gpu3": {"time": {"ts": 1677893021833992000, "dur": 729088000, "parent_is_longest": true, "runtime_str": "729 ms"}}}, "source_file_name": "/home/ubuntu/algorithmic-efficiency/submission_runner.py", "source_file_num": 324, "id": "lCTkLzk85GAa4OrW", "pretty_name": "Transformer Training Iteration", "total_accuracy_str": "98.57%", "trace_file": "/results/Transformer/Transformer.1.pt.trace.json", "trace_disk_size": "6.7 MB", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer/home/ubuntu/algorithmic-efficiency/submission_runner.py", "config": {"trace_filepath": "/home/ubuntu/cpath/results/Transformer.worker0.pt.trace.json", "output_dir": "/home/ubuntu/cpath/results", "ui_traces_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/Transformer", "ui_model_path": "/home/ubuntu/cpath/results/ui/src/results/Transformer.js", "results_summary_csv_filepath": "/home/ubuntu/cpath/results/results_summary.csv", "ui_source_code_path": "/home/ubuntu/cpath/results/ui/dist/traces/results/code/Transformer", "run_name": "Transformer", "model_name": "Transformer", "backend": "torch", "test": false, "is_test_accuracy": true, "view_manual_annotations": false, "source_file_name": "/home/ubuntu/algorithmic-efficiency/submission_runner.py", "source_file_num": 324, "num_gpus": 1, "slice_idx": 0, "drop_flow_view": "", "last_found_was_fused": false, "count_sum_greater_than_1": 0, "count_sum_less_than_1": 0, "tiny_op_threshold": 0.05, "max_generated_depth": 2, "remove_slice_args": true, "write_model_ops_to_file": true, "op_idx": 1}, "metadata": {}, "metadata.model": "Transformer", "metadata.dataset": "WMT17", "metadata.batch_size": 32, "metadata.optimizer": "Adam", "trace_event_count": 21293, "pytorch_version": "1.13.1+cu117", "gpu_model": "NVIDIA GeForce RTX 3090", "gpu_cuda_version": "11.7", "hotline_traces_trace_disk_size": "17.0 MB", "hotline_annotation_count": "663", "processed_datetime": "04/03/2023 01:23:58", "runtime_without_profiling": "720 ms \u00b10.0%", "runtime_with_profiling": "726 ms \u00b10.1%", "runtime_profiling_overhead_factor": "0.01\u00d7 slower", "hotline_analysis_time": "4 s, 341 ms", "runtime": 729088000, "runtime_str": "729 ms", "start_timestamp": "01:23:41.833.833875", "recommendations": "None", "bound_by": "GPU-Bound", "longest_res": "gpu3"}]